================================================================================
ูุญุชูู ุงูุฃุฑุดูู (ZIP): Text_snippets-main.zip
ุชุงุฑูุฎ ุงูุฅูุดุงุก: 2026-02-17 01:26:28
ุงุณู ููู ุงูุฅุฎุฑุงุฌ: Text_snippets-main_zip_contents.txt
================================================================================

ุงุณู ุงูููู: Text_snippets-main/LICENSE
----------------------------------------
Creative Commons Attribution-ShareAlike 4.0 International License

Copyright (c) 2024 DrAbdulmalek

This work is licensed under the Creative Commons Attribution-ShareAlike 4.0
International License. To view a copy of this license, visit
https://creativecommons.org/licenses/by-sa/4.0/ or send a letter to
Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

===============================================================================

CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE LEGAL
SERVICES. DISTRIBUTION OF THIS LICENSE DOES NOT CREATE AN ATTORNEY-CLIENT
RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS INFORMATION ON AN "AS-IS" BASIS.
CREATIVE COMMONS MAKES NO WARRANTIES REGARDING THE USE OF THIS DOCUMENT OR THE
INFORMATION OR WORKS PROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES
RESULTING FROM THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED
HEREUNDER.

===============================================================================

Creative Commons Attribution-ShareAlike 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be
bound by the terms and conditions of this Creative Commons Attribution-ShareAlike
4.0 International Public License ("Public License"). To the extent this Public
License may be interpreted as a contract, You are granted the Licensed Rights
in consideration of Your acceptance of these terms and conditions, and the
Licensor grants You such rights in consideration of benefits the Licensor
receives from making the Licensed Material available under these terms and
conditions.

Section 1 โ Definitions.

a. Adapted Material means material subject to Copyright and Similar Rights that
   is derived from or based upon the Licensed Material and in which the Licensed
   Material is translated, altered, arranged, transformed, or otherwise modified
   in a manner requiring permission under the Copyright and Similar Rights held
   by the Licensor.

b. Adapter's License means the license You apply to Your Copyright and Similar
   Rights in Your contributions to Adapted Material in accordance with the terms
   and conditions of this Public License.

c. BY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses,
   approved by Creative Commons as essentially the equivalent of this Public License.

d. Copyright and Similar Rights means copyright and/or similar rights closely
   related to copyright including, without limitation, performance, broadcast,
   sound recording, and Sui Generis Database Rights, without regard to how the
   rights are labeled or categorized.

e. Effective Technological Measures means those measures that, in the absence
   of proper authority, may not be circumvented under laws fulfilling obligations
   under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996,
   and/or similar international agreements.

f. Exceptions and Limitations means fair use, fair dealing, and/or any other
   exception or limitation to Copyright and Similar Rights that applies to Your
   use of the Licensed Material.

g. License Elements means the license attributes listed in the name of a Creative
   Commons Public License. The License Elements of this Public License are
   Attribution and ShareAlike.

h. Licensed Material means the artistic or literary work, database, or other
   material to which the Licensor applied this Public License.

i. Licensed Rights means the rights granted to You subject to the terms and
   conditions of this Public License, which are limited to all Copyright and
   Similar Rights that apply to Your use of the Licensed Material and that the
   Licensor has authority to license.

j. Licensor means the individual(s) or entity(ies) granting rights under this
   Public License.

k. Share means to provide material to the public by any means or process that
   requires permission under the Licensed Rights, such as reproduction, public
   display, public performance, distribution, dissemination, communication, or
   importation, and to make material available to the public including in ways
   that members of the public may access the material from a place and at a time
   individually chosen by them.

l. Sui Generis Database Rights means rights other than copyright resulting from
   Directive 96/9/EC of the European Parliament and of the Council of 11 March
   1996 on the legal protection of databases, as amended and/or succeeded, as
   well as other essentially equivalent rights anywhere in the world.

m. You means the individual or entity exercising the Licensed Rights under this
   Public License. Your has a corresponding meaning.

Section 2 โ Scope.

a. License grant.
   1. Subject to the terms and conditions of this Public License, the Licensor
      hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive,
      irrevocable license to exercise the Licensed Rights in the Licensed Material to:
      A. reproduce and Share the Licensed Material, in whole or in part; and
      B. produce, reproduce, and Share Adapted Material.

b. Exceptions and Limitations.
   1. Where Exceptions and Limitations apply to Your use, this Public License does
      not apply, and You do not need to comply with its terms and conditions.

c. Term.
   1. The term of this Public License is specified in Section 6(a).

d. Media and formats; technical modifications allowed.
   1. The Licensor authorizes You to exercise the Licensed Rights in all media and
      formats whether now known or hereafter created, and to make technical modifications
      necessary to do so.

e. Downstream recipients.
   1. Offer from the Licensor โ Licensed Material. Every recipient of the Licensed
      Material automatically receives an offer from the Licensor to exercise the
      Licensed Rights under the terms and conditions of this Public License.

   2. Additional offer from the Licensor โ Adapted Material. Every recipient of
      Adapted Material from You automatically receives an offer from the Licensor to
      exercise the Licensed Rights in the Adapted Material under the conditions of
      the Adapter's License You apply.

   3. No downstream restrictions. You may not offer or impose any additional or
      different terms or conditions on, or apply any Effective Technological Measures
      to, the Licensed Material if doing so restricts exercise of the Licensed Rights
      by any recipient of the Licensed Material.

f. No endorsement. Nothing in this Public License constitutes or may be construed
   as permission to assert or imply that You are, or that Your use of the Licensed
   Material is, connected with, or sponsored, endorsed, or granted official status
   by, the Licensor or others designated to receive attribution as provided in
   Section 3(a)(1)(A)(i).

Section 3 โ License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following
conditions.

a. Attribution.
   1. If You Share the Licensed Material (including in modified form), You must:
      A. retain the following if it is supplied by the Licensor with the Licensed Material:
         i. identification of the creator(s) of the Licensed Material and any others
            designated to receive attribution, in any reasonable manner requested by
            the Licensor (including by pseudonym if designated);
        ii. a copyright notice;
       iii. a notice that refers to this Public License;
        iv. a notice that refers to the disclaimer of warranties;
         v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable;
      B. indicate if You modified the Licensed Material and retain an indication of any
         previous modifications; and
      C. indicate the Licensed Material is licensed under this Public License, and include
         the text of, or the URI or hyperlink to, this Public License.

   2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on
      the medium, means, and context in which You Share the Licensed Material.

   3. If requested by the Licensor, You must remove any of the information required by
      Section 3(a)(1)(A) to the extent reasonably practicable.

b. ShareAlike. In addition to the conditions in Section 3(a), if You Share Adapted
   Material You produce, the following conditions also apply.
   1. The Adapter's License You apply must be a Creative Commons license with the same
      License Elements, this version or later, or a BY-SA Compatible License.
   2. You must include the text of, or the URI or hyperlink to, the Adapter's License
      You apply.
   3. You may satisfy the condition in Section 3(b)(2) in any reasonable manner based
      on the medium, means, and context in which You Share Adapted Material.

Section 4 โ Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use
of the Licensed Material:
a. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse,
   reproduce, and Share all or a substantial portion of the contents of the database;
b. if You include all or a substantial portion of the database contents in a database
   in which You have Sui Generis Database Rights, then the database in which You have
   Sui Generis Database Rights (but not its individual contents) is Adapted Material,
   including for purposes of Section 3(b); and
c. You must comply with the conditions in Section 3(a) if You Share all or a substantial
   portion of the contents of the database.

Section 5 โ Disclaimer of Warranties and Limitation of Liability.

a. Unless otherwise separately undertaken by the Licensor, to the extent possible, the
   Licensor offers the Licensed Material as-is and as-available, and makes no
   representations or warranties of any kind concerning the Licensed Material, whether
   express, implied, statutory, or other.

b. TO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL
   THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT,
   SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES,
   COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED
   MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES,
   COSTS, EXPENSES, OR DAMAGES.

c. The disclaimer of warranties and limitation of liability provided above shall be
   interpreted in a manner that, to the extent possible, most closely approximates an
   absolute disclaimer and waiver of all liability.

Section 6 โ Term and Termination.

a. This Public License applies for the term of the Copyright and Similar Rights licensed
   here. However, if You fail to comply with this Public License, then Your rights under
   this Public License terminate automatically.

b. Where Your right to use the Licensed Material has terminated under Section 6(a), it
   reinstates:
   1. automatically as of the date the violation is cured, provided it is cured within
      30 days of Your discovery of the violation; or
   2. upon express reinstatement by the Licensor.

Section 7 โ Other Terms and Conditions.

a. The Licensor shall not be bound by any additional or different terms or conditions
   communicated by You unless expressly agreed.

b. Any arrangements, understandings, or agreements regarding the Licensed Material not
   stated herein are separate from and independent of the terms and conditions of this
   Public License.

===============================================================================

CREATIVE COMMONS IS NOT A PARTY TO ITS PUBLIC LICENSES. NOTWITHSTANDING, CREATIVE
COMMONS MAY ELECT TO APPLY ONE OF ITS PUBLIC LICENSES TO MATERIAL IT PUBLISHES AND
IN THOSE INSTANCES WILL BE CONSIDERED THE "LICENSOR." THE TEXT OF THE CREATIVE
COMMONS PUBLIC LICENSES IS DEDICATED TO THE PUBLIC DOMAIN UNDER THE CREATIVE COMMONS
CCO PUBLIC DOMAIN DEDICATION. EXCEPT FOR THE LIMITED PURPOSE OF INDICATING THAT
MATERIAL IS SHARED UNDER A CREATIVE COMMONS PUBLIC LICENSE AND AS OTHERWISE
PERMITTED BY THE CREATIVE COMMONS POLICIES PUBLISHED AT CREATIVECOMMONS.ORG/POLICIES,
CREATIVE COMMONS DOES NOT AUTHORIZE THE USE OF THE TRADEMARK "CREATIVE COMMONS" OR
ANY OTHER TRADEMARK OR LOGO OF CREATIVE COMMONS WITHOUT ITS PRIOR WRITTEN CONSENT
INCLUDING, WITHOUT LIMITATION, IN CONNECTION WITH ANY UNAUTHORIZED MODIFICATION OF
ANY OF ITS PUBLIC LICENSES OR ANY OTHER ARRANGEMENTS, UNDERSTANDINGS, OR AGREEMENTS
CONCERNING USE OF LICENSED MATERIAL. FOR THE AVOIDANCE OF DOUBT, THIS PARAGRAPH DOES
NOT FORM PART OF THE PUBLIC LICENSES.

===============================================================================

================================================================================

ุงุณู ุงูููู: Text_snippets-main/PROCESSING_REPORT.md
----------------------------------------
# ุชูุฑูุฑ ูุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ

**ุงูุชุงุฑูุฎ:** 2026-02-14 21:27:26

## ๐ ุฅุญุตุงุฆูุงุช

| ุงูุจูุฏ | ุงูุนุฏุฏ |
|-------|-------|
| ุงููููุงุช ุงููุนุงูุฌุฉ | 9 |
| ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | 372 |

## ๐ ุงูุชุตููู

### ๐ป technical (8)

- Aaa2_zip_contents (1).txt
- Aaa_zip_contents (1).txt
- Archive_zip_contents.txt
- Garuda-Ultimate-Enhanced-v5.0-FINAL_folder_contents.txt
- Garuda-Ultimate-Enhanced-v5.0-MODIFIED_zip_contents (2).txt
- New Folder_zip_contents.txt
- packages_backup_20260202.txt
- packages_backup_selective_20260202.txt

### ๐ misc (1)

- garuda-ultimate-zorin-edition-v2.0.tar-1_file_contents.txt


================================================================================

ุงุณู ุงูููู: Text_snippets-main/README.md
----------------------------------------
# ๐ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุชูููุฉ ูุงูุทุจูุฉ | Technical & Medical Knowledge Base

[![License: CC BY-SA 4.0](https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-sa/4.0/)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![GitHub Stars](https://img.shields.io/github/stars/DrAbdulmalek/Text_snippets?style=social)](https://github.com/DrAbdulmalek/Text_snippets)

> ูุฌููุนุฉ ุดุงููุฉ ูู ุงููุญุงุฏุซุงุช ูุงููุฑุงุฌุน ุงูุชูููุฉ ูุงูุทุจูุฉ ููุธูุฉ ููุจุงุญุซูู ูุงููุทูุฑูู ูููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู.

> A comprehensive collection of technical and medical conversations and references, organized for researchers, developers, and AI models.

---

## ๐ ููุฑุณ ุงููุญุชููุงุช | Table of Contents

- [ูุธุฑุฉ ุนุงูุฉ | Overview](#-ูุธุฑุฉ-ุนุงูุฉ--overview)
- [ูููู ุงููุณุชูุฏุน | Repository Structure](#-ูููู-ุงููุณุชูุฏุน--repository-structure)
- [ุงูุชุซุจูุช ูุงูุงุณุชุฎุฏุงู | Installation & Usage](#-ุงูุชุซุจูุช-ูุงูุงุณุชุฎุฏุงู--installation--usage)
- [ุฃุฏูุงุช ุงููุนุงูุฌุฉ | Processing Tools](#-ุฃุฏูุงุช-ุงููุนุงูุฌุฉ--processing-tools)
- [ุงููุฆุงุช ุงูุฑุฆูุณูุฉ | Main Categories](#-ุงููุฆุงุช-ุงูุฑุฆูุณูุฉ--main-categories)
- [ุงุณุชุฎุฏุงู ุงูุจูุงูุงุช ูุชุฏุฑูุจ AI | Using Data for AI Training](#-ุงุณุชุฎุฏุงู-ุงูุจูุงูุงุช-ูุชุฏุฑูุจ-ai--using-data-for-ai-training)
- [ุฅุญุตุงุฆูุงุช | Statistics](#-ุฅุญุตุงุฆูุงุช--statistics)
- [ุงููุณุงููุฉ | Contributing](#-ุงููุณุงููุฉ--contributing)
- [ุงูุฑุฎุตุฉ | License](#-ุงูุฑุฎุตุฉ--license)

---

## ๐ ูุธุฑุฉ ุนุงูุฉ | Overview

ูุฐุง ุงููุณุชูุฏุน ูุญุชูู ุนูู ูุฌููุนุฉ ููุธูุฉ ูู ุงููุญุงุฏุซุงุช ูุงููุฑุงุฌุน ุงูุชูููุฉ ูุงูุทุจูุฉ ุงูุชู ุชู ุฌูุนูุง ูุชุตููููุง ุจุนูุงูุฉ. ููุฏู ุฅูู ุชูููุฑ ุจูุงูุงุช ุนุงููุฉ ุงูุฌูุฏุฉ ูุชุฏุฑูุจ ููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุงูุจุญุซ ุงูุนููู.

This repository contains an organized collection of technical and medical conversations and references carefully collected and classified. It aims to provide high-quality data for AI model training and scientific research.

### ุงููููุฒุงุช ุงูุฑุฆูุณูุฉ | Key Features

- โ **ุซูุงุฆู ุงููุบุฉ**: ูุญุชูู ุจุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
- โ **ูุตูู ุขููุงู**: ุชุตููู ุฐูู ูููุญุชูู (ุทุจูุ ุชูููุ ุชุฑุฌูุฉ)
- โ **ุชุฑุฌูุงุช ูุณุชุฎุฑุฌุฉ**: ุฃุฒูุงุฌ ุชุฑุฌูุฉ ุฌุงูุฒุฉ ููุชุฏุฑูุจ
- โ **ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช**: ุจูุงูุงุช ูุธููุฉ ุจุฏูู ุชูุฑุงุฑ
- โ **ุณูุฑูุจุชุงุช ูุชูุงููุฉ**: ุฃุฏูุงุช ูุนุงูุฌุฉ ุฌุงูุฒุฉ ููุงุณุชุฎุฏุงู

---

## ๐ ูููู ุงููุณุชูุฏุน | Repository Structure

```
Text_snippets/
โโโ ๐ README.md                    # ูุฐุง ุงูููู | This file
โโโ ๐ LICENSE                      # ุฑุฎุตุฉ CC BY-SA 4.0
โโโ ๐ .gitignore                   # ูููุงุช ูุณุชุซูุงุฉ
โโโ ๐ PROCESSING_REPORT.md         # ุชูุฑูุฑ ุงููุนุงูุฌุฉ
โ
โโโ ๐ technical/                   # ุงููุญุชูู ุงูุชููู
โ   โโโ ๐ linux/
โ   โ   โโโ garuda-packages-list.md
โ   โ   โโโ vpn-tools.md
โ   โ   โโโ wine-comparison.md
โ   โโโ package-lists.md
โ   โโโ garuda-alternatives.md
โ
โโโ ๐ medical/                     # ุงููุญุชูู ุงูุทุจู
โ   โโโ ๐ orthopedics/
โ   โ   โโโ supplies-prices.md
โ   โโโ orthopedics-supplies.md
โ
โโโ ๐ translation/                 # ุงูุชุฑุฌูุงุช
โ   โโโ ocr-bilingual.md
โ   โโโ translations.csv
โ   โโโ translations_corrected.xlsx
โ
โโโ ๐ scripts/                     # ุณูุฑูุจุชุงุช ุงููุนุงูุฌุฉ
โ   โโโ corpus_processor_offline.py     # ูุนุงูุฌ ุงููุตูุต ุงูุฑุฆูุณู
โ   โโโ corpus_dedup_ai.py              # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู
โ   โโโ knowledge_processor.py          # ูุนุงูุฌ ูุงุนุฏุฉ ุงููุนุฑูุฉ
โ   โโโ knowledge_base_processor.py     # ูุนุงูุฌ ูุชูุงูู
โ   โโโ zip_rar_folder2txt.py           # ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ุงูุฃุฑุดููุงุช
โ   โโโ combined_text_splitter.py       # ุชูุณูู ุงููููุงุช ุงููุจูุฑุฉ
โ   โโโ ๐ guides/
โ       โโโ CORPUS_PROCESSOR_GUIDE.md
โ       โโโ DEDUP_AI_GUIDE.md
โ       โโโ WORKFLOW_GUIDE.md
โ
โโโ ๐ digests/                     # ุงูููุฎุตุงุช ูุงูููุงุฑุณ
โ   โโโ conversation-index.md
โ   โโโ organize_corpus.md
โ
โโโ ๐ references/                  # ุงููุฑุงุฌุน ูุงูููุงุฑุณ
    โโโ analysis-plan.md
    โโโ file_index.json
    โโโ topics-index.md
```

---

## ๐ ุงูุชุซุจูุช ูุงูุงุณุชุฎุฏุงู | Installation & Usage

### ุงููุชุทูุจุงุช | Requirements

```bash
# Python 3.8 ุฃู ุฃุญุฏุซ
python --version

# ุชุซุจูุช ุงููุชุทูุจุงุช
pip install -r requirements.txt
```

### requirements.txt

```txt
# ูุนุงูุฌุฉ ุงููุตูุต
python-docx>=0.8.11
openpyxl>=3.1.2
chardet>=5.2.0
tqdm>=4.66.1

# ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
datasketch>=1.6.0  # MinHash LSH

# ุงูุฐูุงุก ุงูุงุตุทูุงุนู (ุงุฎุชูุงุฑู)
# openai>=1.0.0
# anthropic>=0.18.0

# ุชุตููู ูุชุฏุฑูุจ
scikit-learn>=1.3.0
numpy>=1.24.0

# ูุงุฌูุฉ ุงูููุจ (ุงุฎุชูุงุฑู)
streamlit>=1.30.0

# ุชุตุฏูุฑ HuggingFace (ุงุฎุชูุงุฑู)
datasets>=2.16.0
huggingface-hub>=0.20.0
```

### ุงูุงุณุชุฎุฏุงู ุงูุณุฑูุน | Quick Start

```bash
# 1. ุงุณุชูุณุงุฎ ุงููุณุชูุฏุน
git clone https://github.com/DrAbdulmalek/Text_snippets.git
cd Text_snippets

# 2. ูุนุงูุฌุฉ ุงููููุงุช
python scripts/corpus_processor_offline.py --input ./data --output ./processed

# 3. ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
python scripts/knowledge_processor.py --mode translate --input ./data

# 4. ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
python scripts/corpus_dedup_ai.py --input ./processed --threshold 0.85
```

---

## ๐๏ธ ุฃุฏูุงุช ุงููุนุงูุฌุฉ | Processing Tools

### 1. corpus_processor_offline.py

ุงูุฃุฏุงุฉ ุงูุฑุฆูุณูุฉ ููุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ ุงููุจูุฑุฉ (5GB+):

```bash
python corpus_processor_offline.py --input ./raw_data --output ./processed --mode all
```

**ุงูุฃูุถุงุน ุงููุชุงุญุฉ:**
| ุงููุถุน | ุงููุตู |
|-------|-------|
| `extract` | ุงุณุชุฎุฑุงุฌ ุงููุญุชูู |
| `dedup` | ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช |
| `classify` | ุชุตููู ุงููุญุชูู |
| `translate` | ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช |
| `all` | ุชุดุบูู ุฌููุน ุงูุนูููุงุช |

### 2. corpus_dedup_ai.py

ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ุจุงุณุชุฎุฏุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ู MinHash LSH:

```bash
python corpus_dedup_ai.py --input ./data --output ./deduped --ai --threshold 0.85
```

### 3. knowledge_processor.py

ูุนุงูุฌ ูุชุนุฏุฏ ุงููุธุงุฆู ููุงุนุฏุฉ ุงููุนุฑูุฉ:

```bash
# ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
python knowledge_processor.py --mode translate

# ุชุตููู ุงููุญุชูู
python knowledge_processor.py --mode classify

# ุชุตุญูุญ ุงูุฅููุงุก
python knowledge_processor.py --mode spellcheck
```

### 4. zip_rar_folder2txt.py

ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ุงูุฃุฑุดููุงุช:

```bash
python zip_rar_folder2txt.py --input ./archives --output ./extracted
```

---

## ๐ ุงููุฆุงุช ุงูุฑุฆูุณูุฉ | Main Categories

### ๐ง ููููุณ ูุชูููุงุชู | Linux Technologies

| ุงูููุถูุน | ุงููุตู |
|---------|-------|
| Garuda Linux | ุชุฎุตูุต ูุชุซุจูุช ูุธุงู ุบุงุฑูุฏุง ููููุณ |
| Wine/Bottles/CrossOver | ููุงุฑูุฉ ุฃุฏูุงุช ุชุดุบูู ุชุทุจููุงุช ูููุฏูุฒ |
| ุจุฏุงุฆู ุงูุจุฑุงูุฌ | ุงูุจุฑุงูุฌ ููุชูุญุฉ ุงููุตุฏุฑ ูุจุฏูู ููููุฏูุฒ |
| ุฃุฏูุงุช VPN | Psiphon, Outline, Tor ุนูู ููููุณ |

### ๐ช ูููุฏูุฒ ูุงูุงูุชูุงู ููู | Windows & Migration

| ุงูููุถูุน | ุงููุตู |
|---------|-------|
| ููุงุฆู ุงูุจุฑุงูุฌ | ุฌูุจ ููุงุฆู ุงูุจุฑุงูุฌ ูู ูููุฏูุฒ 11 |
| ุงูุจุฏุงุฆู ุงูููุชูุญุฉ | ุจุฏุงุฆู ููุชูุญุฉ ุงููุตุฏุฑ ููุจุฑุงูุฌ ุงูุชุฌุงุฑูุฉ |
| ุงูุฃุฌูุฒุฉ ุงูุงูุชุฑุงุถูุฉ | ุชุดุบูู ูููุฏูุฒ ุนูู ููููุณ |

### ๐ฅ ุทุจูุฉ ูุนุธููุฉ | Medical & Orthopedics

| ุงูููุถูุน | ุงููุตู |
|---------|-------|
| ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ | ุฃุณุนุงุฑ ููุนูููุงุช ุงููุนุฏุงุช ุงูุทุจูุฉ |
| ุฃุฌูุฒุฉ DICOM | ุฃุฏูุงุช ุงูุชุตููุฑ ุงูุทุจู |
| ุงูุชุฑุฌูุฉ ุงูุทุจูุฉ | ูุตุทูุญุงุช ุทุจูุฉ ุซูุงุฆูุฉ ุงููุบุฉ |

### ๐ ุชุฑุฌูุฉ ู OCR | Translation & OCR

| ุงูููุถูุน | ุงููุตู |
|---------|-------|
| OCR ุซูุงุฆู ุงููุบุฉ | ุฅุนุฏุงุฏ ุจูุฆุฉ ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู |
| ุชุฏุฑูุจ ุงูููุงุฐุฌ | ุชุฏุฑูุจ ููุงุฐุฌ ุงูุชุฑุฌูุฉ ุงูุทุจูุฉ |
| ูุนุงูุฌุฉ ุงูุนุฑุจูุฉ | ุชุญุณูู ุฌูุฏุฉ ุงููุตูุต ุงูุนุฑุจูุฉ |

---

## ๐ค ุงุณุชุฎุฏุงู ุงูุจูุงูุงุช ูุชุฏุฑูุจ AI | Using Data for AI Training

### ุชุญููู ุงูุจูุงูุงุช | Loading Data

```python
import os
import json
from pathlib import Path

def load_knowledge_base(base_path):
    """ุชุญููู ุฌููุน ูููุงุช Markdown ูู ุงููุณุชูุฏุน"""
    docs = []
    for md_file in Path(base_path).rglob("*.md"):
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
            docs.append({
                'path': str(md_file.relative_to(base_path)),
                'content': content,
                'category': md_file.parent.name,
                'language': 'ar' if any('\u0600' <= c <= '\u06FF' for c in content) else 'en'
            })
    return docs

# ุงุณุชุฎุฏุงู ุงูุฏุงูุฉ
docs = load_knowledge_base('./Text_snippets')
print(f"ุชู ุชุญููู {len(docs)} ูุณุชูุฏ")
```

### ุชุญููู ุงูุชุฑุฌูุงุช | Loading Translations

```python
import pandas as pd

# ุชุญููู ููู ุงูุชุฑุฌูุงุช
translations = pd.read_csv('translation/translations.csv', sep='\t')
print(f"ุนุฏุฏ ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ: {len(translations)}")

# ุชูุณูู ููุชุฏุฑูุจ
for _, row in translations.iterrows():
    training_example = {
        'source': row['english'],
        'target': row['arabic'],
        'category': row.get('category', 'general')
    }
```

### ุชุตุฏูุฑ ูู HuggingFace | Export to HuggingFace

```python
from datasets import Dataset

# ุฅูุดุงุก Dataset
dataset = Dataset.from_pandas(translations)

# ุฑูุน ุฅูู HuggingFace
dataset.push_to_hub(
    "your-username/medical-translations-ar-en",
    private=False
)
```

---

## ๐ ุฅุญุตุงุฆูุงุช | Statistics

### ุงููุญุชูู ุญุณุจ ุงููุฆุฉ | Content by Category

| ุงููุฆุฉ | ุนุฏุฏ ุงููููุงุช | ุงููุบุฉ ุงูุฃุณุงุณูุฉ |
|-------|-------------|----------------|
| ๐ง ุชูููุฉ - ููููุณ | ~40 ูุญุงุฏุซุฉ | ุนุฑุจู/ุฅูุฌููุฒู |
| ๐ช ุชูููุฉ - ูููุฏูุฒ | ~15 ูุญุงุฏุซุฉ | ุนุฑุจู/ุฅูุฌููุฒู |
| ๐ฅ ุทุจูุฉ | ~10 ูุญุงุฏุซุงุช | ุนุฑุจู/ุฅูุฌููุฒู |
| ๐ ุชุฑุฌูุฉ ู OCR | ~10 ูุญุงุฏุซุงุช | ุนุฑุจู/ุฅูุฌููุฒู |
| ๐ ุณูุฑูุจุชุงุช | ~12 ููู | Python/Shell |

### ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | Extracted Translations

| ุงูููุน | ุงูุนุฏุฏ | ุงูุฌูุฏุฉ |
|-------|-------|--------|
| ุฃุฒูุงุฌ ุชุฑุฌูุฉ | 319+ | ุชู ุงูุชุญูู |
| ุชุฑุฌูุงุช ุนุงููุฉ ุงูุฌูุฏุฉ | 5+ | ูุตุญุญุฉ ูุฏููุงู |
| ูุตุทูุญุงุช ุทุจูุฉ | 50+ | ูุชุฎุตุตุฉ |

---

## ๐ค ุงููุณุงููุฉ | Contributing

ูุฑุญุจ ุจูุณุงููุงุชูู! ูุฅุถุงูุฉ ูุญุชูู ุฃู ุชุญุณูู:

### ุทุฑู ุงููุณุงููุฉ

1. **ุงูุฅุจูุงุบ ุนู ุฎุทุฃ**: ุงูุชุญ [Issue](https://github.com/DrAbdulmalek/Text_snippets/issues)
2. **ุฅุถุงูุฉ ูุญุชูู**: ุงูุชุญ Pull Request ูุน ูุตู ุงูุชุบููุฑุงุช
3. **ุงูุชุฑุงุญ ุชุญุณููุงุช**: ุงุณุชุฎุฏู Discussions ููููุงุด

### ุฅุฑุดุงุฏุงุช ุงููุณุงููุฉ

```bash
# 1. Fork ุงููุณุชูุฏุน
git clone https://github.com/your-username/Text_snippets.git

# 2. ุฅูุดุงุก ูุฑุน ุฌุฏูุฏ
git checkout -b feature/your-feature

# 3. ุฅุฌุฑุงุก ุงูุชุบููุฑุงุช ูุงูุงูุชุฒุงู
git add .
git commit -m "ูุตู ุงูุชุบููุฑุงุช"

# 4. ุฑูุน ุงูุชุบููุฑุงุช
git push origin feature/your-feature

# 5. ูุชุญ Pull Request
```

---

## ๐ ุงูุฑุฎุตุฉ | License

ูุฐุง ุงูุนูู ูุฑุฎุต ุจููุฌุจ **[Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/)**.

### ูุง ูุณูุญ ุจู ุงูุฑุฎุตุฉ:
- โ ุงููุดุงุฑูุฉ: ูุณุฎ ูุชูุฒูุน ุงูุนูู
- โ ุงูุชูููู: ุงูุชุนุฏูู ูุงูุชุญููู
- โ ุงูุงุณุชุฎุฏุงู ุงูุชุฌุงุฑู

### ุงูุดุฑูุท:
- ๐ ูุณุจ ุงูุนูู ูููุคูู ุงูุฃุตูู
- ๐ ุชูุฒูุน ุงูุนูู ุงููุนุฏู ุจููุณ ุงูุฑุฎุตุฉ

---

## ๐ ุงูุชูุงุตู | Contact

- **GitHub**: [@DrAbdulmalek](https://github.com/DrAbdulmalek)
- **Repository**: [Text_snippets](https://github.com/DrAbdulmalek/Text_snippets)

---

## ๐ ุดูุฑ ูุชูุฏูุฑ | Acknowledgments

ุดูุฑ ูุฌููุน ุงููุณุงูููู ูู ุชุทููุฑ ูุฐุง ุงููุดุฑูุน ูุงูุฃุฏูุงุช ุงููุณุชุฎุฏูุฉ:
- ูุฌุชูุน Python ุงูุนุฑุจู
- ูุทูุฑู ููุชุจุงุช ุงููุนุงูุฌุฉ ุงููุตูุฉ
- ูุฌุชูุน ุงููุตุฏุฑ ุงูููุชูุญ

---

> โ๏ธ **ููุงุญุธุฉ ุทุจูุฉ**: ุงููุญุชูู ุงูุทุจู ููุฃุบุฑุงุถ ุงููุฑุฌุนูุฉ ููุท ููุง ูุบูู ุนู ุงูุงุณุชุดุงุฑุฉ ุงูุทุจูุฉ ุงููุชุฎุตุตุฉ.

> โ๏ธ **Medical Disclaimer**: Medical content is for reference purposes only and does not replace professional medical consultation.

---

<p align="center">
  <b>ุตููุน ุจู โค๏ธ ููุจุงุญุซูู ูุงููุทูุฑูู</b>
</p>

================================================================================

ุงุณู ุงูููู: Text_snippets-main/SESSION_TEMPLATE.md
----------------------------------------
# ==============================================================================
# ๐ ูุงูุจ ุงูุจุฏุงูุฉ ููู ุฌูุณุฉ - ุงูุณุฎ ูุฐุง ุจุงููุงูู ูุงูุตูู ูู ุจุฏุงูุฉ ูู ุฌูุณุฉ
# ==============================================================================

## ๐ ุจูุงูุงุช GitHub (ุถุฑูุฑูุฉ ููุฑูุน ุงูุชููุงุฆู)

- **ุงุณู ุงููุณุชุฎุฏู:** DrAbdulmalek
- **ุงููุณุชูุฏุน:** https://github.com/DrAbdulmalek/Text_snippets
- **ุงูุชูููู:** YOUR_GITHUB_TOKEN_HERE

---

## ๐ฏ ุงููููุฉ ุงููุทููุจุฉ

ุฃุฑูุฏู ุฃู ุชููู ุจูุง ููู ุชููุงุฆูุงู:

### 1. ูุนุงูุฌุฉ ุงููููุงุช ุงููุฑููุนุฉ
- ุงุณุชุฎุฑุฌ ุงููุนูููุงุช ุงููููุฏุฉ ููุท
- ุฃุฒู ุงูุชูุฑุงุฑุงุช ูุงูููุงูุฉ (ูุงุดุ ุณุฌูุงุชุ ูููุงุช ูุคูุชุฉ)
- ุตูู ุงููุญุชูู ุฅูู ูุฆุงุช:
  - ๐ฅ ุทุจู (ุนุธุงูุ ุฃุณุนุงุฑุ ุฃุฏูุงุช ุทุจูุฉ)
  - ๐ป ุชููู (Linuxุ ุจุฑูุฌุฉุ ุฃุฏูุงุช)
  - ๐ ุชุฑุฌูุฉ (ุซูุงุฆู ุงููุบุฉุ OCR)
  - ๐ ูุฑุงุฌุน (ููุงุฑุณุ ููุงุฆู)

### 2. ุฅูุดุงุก ูุณุชูุฏุน ููุธู
```
knowledge-base/
โโโ README.md
โโโ medical/
โ   โโโ [ูููุงุช ุทุจูุฉ]
โโโ technical/
โ   โโโ [ูููุงุช ุชูููุฉ]
โโโ translation/
โ   โโโ [ูููุงุช ุชุฑุฌูุฉ]
โโโ references/
    โโโ [ููุงุฑุณ ููุฑุงุฌุน]
```

### 3. ุฑูุน ุฅูู GitHub
- ุงุณุชุฎุฏู ุงูุชููู ุฃุนูุงู ููุฑูุน ุงูุชููุงุฆู
- ุฃุถู ุงููุญุชูู ุงูุฌุฏูุฏ ูููุณุชูุฏุน ุงูููุฌูุฏ
- ูุง ุชุญุฐู ุงููุญุชูู ุงูุณุงุจู

### 4. ุชูุธูู ุงููููุงุช ุงููุนุงูุฌุฉ
- ุงุญุฐู ุงููููุงุช ุงููุฑููุนุฉ ุจุนุฏ ูุนุงูุฌุชูุง
- ุฃุฎุจุฑูู ุจุงููููุงุช ุงููุชุจููุฉ

---

## ๐ ุชูุณูู ุงูุฅุฎุฑุงุฌ ุงููุทููุจ

### ููุฎุต ูู ุงูููุงูุฉ:
```
โ ุชู ูุนุงูุฌุฉ X ููู
๐ X ููู ุชู ุฅูุดุงุคู
๐ค ุชู ุงูุฑูุน ุฅูู: https://github.com/DrAbdulmalek/Text_snippets
๐๏ธ ุชู ุญุฐู X ููู ูุนุงูุฌ
```

---

## โ๏ธ ุชุนูููุงุช ุฅุถุงููุฉ

1. **ุงููุบุฉ:** ุงุณุชุฎุฏู ุงูุนุฑุจูุฉ ูู ุงูุฑุฏูุฏ ูุงููููุงุช
2. **ุงูุชุฑููุฒ:** UTF-8 ุฏุงุฆูุงู
3. **ุงูุชุฑุฎูุต:** CC BY-SA 4.0 ูููููุงุช
4. **ุฅุฐุง ูุงุฌูุช ุฎุทุฃ:** ุงุดุฑุญ ุงูุฎุทุฃ ูุงูุชุฑุญ ุงูุญู

---

## ๐ ุงูุขู ุงุจุฏุฃ ุจูุนุงูุฌุฉ ุงููููุงุช ุงููุฑููุนุฉ ูู ูุฐู ุงูุฌูุณุฉ


================================================================================

ุงุณู ุงูููู: Text_snippets-main/digests/conversation-index.md
----------------------------------------
# ููุฑุณ ุงููุญุงุฏุซุงุช ุงููุงูู | Complete Conversation Index

## ููุฎุต
ููุฑุณ ุจุฌููุน ุนูุงููู ุงููุญุงุฏุซุงุช ุงูุชูููุฉ ูุงูุทุจูุฉ ุงููุชููุฑุฉ ูู ูุฐู ุงููุงุนุฏุฉ ุงููุนุฑููุฉ.

---

## ๐ง Linux & System Administration

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 1 | ุชุญููู ูููุงุช ุงููุณุฎ ุงูุงุญุชูุงุทู ููุงูุฌุงุฑู | Manjaro Backup File Analysis |
| 2 | ุชุซุจูุช Telegram ุนูู Manjaro | Install Telegram on Manjaro |
| 3 | Fix pacman database lock error | Fix pacman database lock error |
| 4 | Linux user frustration and advice | Linux user frustration and advice |
| 5 | ุงุณุชูุดุงู ูุฅุตูุงุญ ูุดููุฉ ูุธุงู ุงููููุงุช | Filesystem troubleshooting |
| 6 | Reinstalling Plasma Desktop and SDDM | Reinstalling Plasma Desktop and SDDM |
| 7 | ุงุณุชูุดุงู ูุฅุตูุงุญ ุฎุทุฃ ุชุฑููุจ ูุณู ุฌุฏูุฏ | Partition mount error troubleshooting |
| 8 | ุชุญููู ูุดุงูู ูููููุณ ุบุงุฑูุฏุง ูุญููููุง | Garuda Linux issues analysis |
| 9 | ุชูุตูุจ Ollama ูLM Studio ุนูู Garuda | Install Ollama and LM Studio on Garuda |
| 10 | Running Psiphon on Linux Guide | Running Psiphon on Linux Guide |
| 11 | ุชุทุจูู ุฎุทูุท Windows 11 ูู ูููููุณ | Windows 11 fonts on Linux |
| 12 | ุงุณุชุฎุฏุงู ูููุงุช ุชุซุจูุช ุบุงุฑูุฏุง ูู ููููุณ | Using Garuda setup files on Linux |
| 13 | ุญู ูุดููุฉ CrossOver ูู ุชุดุบูู ุชุทุจููุงุช ูููุฏูุฒ | CrossOver Windows apps issue |
| 14 | ุจุฏุงุฆู ACDSee ูุชุดุบูู ุงูุตูุฑ ูู ููููุณ | ACDSee alternatives for Linux |
| 15 | ุฌุนู ููู ุงูุชูููุฐ ูุฅูุดุงุก ุงุฎุชุตุงุฑ ุณุทุญ ุงูููุชุจ | Make executable & create desktop shortcut |
| 16 | ุจูุงุก Perl 5.18 ุจูุงูุชุธุงุฑ ุทููู | Building Perl 5.18 long wait |
| 17 | ุญู ูุดููุฉ Crossover ุนูู Garuda Linux | Crossover issue on Garuda Linux |
| 18 | ุฅุตูุงุญ ุฎุทุฃ ูุธุงู ุงููููุงุช ูู ููููุณ | Fix filesystem error in Linux |
| 19 | ุฅุตูุงุญ ูุดููุฉ Conda ูู Fish | Fix Conda issue in Fish shell |
| 20 | Installing Outline Client on Arch Linux | Installing Outline Client on Arch |
| 21 | ุชุซุจูุช ุบูุบู ูุฑูู ุนูู ุฌุงุฑูุฏุง | Install Google Chrome on Garuda |
| 22 | ุฏุฎูู ูููููุณ ุบุงุฑูุฏุง ูู ูููุฏูุฒ | Access Garuda Linux from Windows |
| 23 | ุญู ูุดููุฉ ุงููุตูู ุฅูู ูุญุฑู ุงูุฃูุฑุงุต | Fix drive access issue |
| 24 | ุชุทููุฑ ููุญุฉ ุชุญูู ููุฑุงูุจุฉ Garuda Linux | Develop monitoring panel for Garuda |
| 25 | ุฅุตูุงุญ ูุดุงูู ุงูุฎุทูุท ูู Wine | Fix Wine font issues |
| 26 | ุชุดุบูู ุณูุฑูุจุช ุงุณุชุฎุฑุงุฌ ูุต ูู Garuda | Run text extraction script on Garuda |
| 27 | ุชูููู GNOME ููุธูุฑ Windows 11 | Configure GNOME for Windows 11 look |
| 28 | ุญู ูุดุงูู Zorin OS ุนูู Wayland | Fix Zorin OS Wayland issues |
| 29 | ุญููู ูุชุซุจูุช ูุชูููู ุฅุนุฏุงุฏุงุช Zorin OS | Zorin OS setup solutions |
| 30 | Garuda Fusion v4.0 Error Registration | Garuda Fusion v4.0 Registration Error |
| 31 | ุชุซุจูุช ูุญู ูุดุงูู Latte Dock ูู Garuda | Install and fix Latte Dock on Garuda |
| 32 | ุชููู ุชุญููู ููู ูู ุฅุนุฏุงุฏ ููููุณ | File download stalled during setup |
| 33 | ุชุซุจูุช ุณูุฑูุจุช ุงุณุชุฎุฑุงุฌ ุงูุฃุฑุดููุงุช | Install archive extraction script |
| 34 | ุชุญุฏูุซุงุช ูููุฒุงุช ุฌุฏูุฏุฉ ูู ุฅุตุฏุงุฑ v4.0 | Updates and new features in v4.0 |
| 35 | ุชุซุจูุช ูุชุดุบูู Garuda Ultimate Enhanced v5.0 | Install Garuda Ultimate Enhanced v5.0 |
| 36 | ุชุซุจูุช ุฃุฏูุงุช DICOM ููุดุงูุฏุฉ ุงูุณุฌูุงุช | Install DICOM tools and view logs |
| 37 | ุชุซุจูุช ุฃุฏูุงุช DICOM ุงูุทุจูุฉ ุนูู Linux | Install medical DICOM tools on Linux |
| 38 | ุจุฑูุงูุฌ ูุชุญููู ุงููููุงุช ุงููุถุบูุทุฉ ุฅูู ูุต | Convert archives to text |
| 39 | ูุดุฑูุน Garuda Ultimate Enhanced v5.0 ุดุงูู | Garuda Ultimate Enhanced v5.0 Complete |
| 40 | ุชูุฒูู ุญุฒูุฉ ุฎุท ุงููุตูุต Amiri | Download Amiri font package |
| 41 | ุฅูุดุงุก ุดุฑูุท ููุงู ูุดุจู ูููุฏูุฒ ูู Garuda | Create Windows-like taskbar in Garuda |
| 42 | ููุฎุต ูุดุฑูุน Garuda Ultimate Fusion | Garuda Ultimate Fusion Summary |
| 43 | ุชุทููุฑ ุณูุฑูุจุช ูุฏุนู ุฃูุชุฏุงุฏุงุช ุงูุถุบุท | Develop compression extension script |
| 44 | ูุดุฑูุน Garuda Linux ุจุชุฎุตูุต ุนุฑุจู ูุชูุงูู | Garuda Linux Arabic customization project |
| 45 | ุฅุตุฏุงุฑ ุซุงูู ููุดุฑูุน Garuda Ultimate Zorin | Garuda Ultimate Zorin v2 |
| 46 | ุชุตุญูุญ ุณูุฑูุจุช ุชุซุจูุช ุจุฑุงูุฌ Garuda Linux | Fix Garuda Linux software install script |
| 47 | ุชุทููุฑ Garuda Linux ูุชุดุจู Zorin OS | Develop Garuda to look like Zorin OS |
| 48 | ูุดุฑูุน ุชููู ุจุฑูุญ ุฅูุณุงููุฉ ูู ุญูุต | Technical project from Homs |
| 49 | ุฏูุฌ ูููุงุช ูุดุฑูุน Garuda Linux ุนูู ุบูุช ูุจ | Merge Garuda Linux files on GitHub |
| 50 | ุฅุตูุงุญ ูุดุงูู ุชุญุฏูุซ ูุฑุงูุง Garuda Linux | Fix Garuda Linux mirror update issues |
| 51 | ุฏููู ูุฌูู ูุฅุฏุงุฑุฉ Garuda Linux | Garuda Linux management guide |
| 52 | ุฅุฏุงุฑุฉ ุจูุฆุงุช ุณุทุญ ุงูููุชุจ ูู Garuda Linux | Manage desktop environments in Garuda |
| 53 | ุชุซุจูุช ูุชูููู ุจุฑุงูุฌ ุนูู Arch Linux | Install and configure software on Arch |
| 54 | ุฅุนุฏุงุฏุงุช ูุฅุถุงูุงุช WPS Office ุนูู Arch | WPS Office settings and addons on Arch |
| 55 | ุญู ูุดููุฉ Outline VPN ูุง ูุบูุฑ ุงููููุน | Fix Outline VPN not changing location |
| 56 | ุฅุตูุงุญ ูุดุงูู Garuda Linux ูุญููููุง | Fix Garuda Linux issues |
| 57 | ุงุณุชุฎุฑุงุฌ ุชุทุจููุงุช Zorin ุนูู Garuda | Extract Zorin apps on Garuda |
| 58 | ุฌุนู Garuda ูุดุจู Zorin OS 18 Pro | Make Garuda look like Zorin OS 18 Pro |
| 59 | ุชุญููู ุจุฑุงูุฌ ูุซุจุชุฉ ุนูู ูุธุงููู | Analyze installed software on two systems |
| 60 | ุชุซุจูุช ุงูุจุฑุงูุฌ ุงูููููุฏุฉ ูู ูููููุณ | Install missing software on Linux |
| 61 | ุชุซุจูุช ุจูุฆุฉ ุชุทููุฑ ุจุนุฏ ุฅุตูุงุญ ุงูุฃุฎุทุงุก | Install dev environment after fixing errors |
| 62 | ุชุซุจูุช ุจูุฆุฉ ุชุทููุฑ ูุงููุฉ ุนูู ููููุณ | Install complete dev environment on Linux |
| 63 | ุชุดุบูู ูููุฏูุฒ ูู VM ุนูู ุงููุงุฑุฏ ุฏูุณู | Run Windows as VM from hard disk |
| 64 | ุชุซุจูุช Outline VPN ุนูู ูููููุณ ุฌุงุฑูุฏุง | Install Outline VPN on Garuda Linux |
| 65 | ุชุญูู ูู Zorin OS ุฅูู Garuda Linux | Switch from Zorin OS to Garuda Linux |

---

## ๐ช Windows & Migration

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 66 | ุทุฑู ุฌูุจ ุงูุจุฑุงูุฌ ูู ูููุฏูุฒ 11 | Methods to get software list in Windows 11 |
| 67 | ููุงุฑูุฉ ูุฑูู ููุงูุฑูููุณ 2026 | Chrome vs Firefox 2026 comparison |
| 68 | ุจุฏุงุฆู Rufus ูู ููููุณ ุฃุฑุด | Rufus alternatives on Arch Linux |

---

## ๐ฅ Medical Topics

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 69 | ูุตู ููุตู ุงููุฑู ูู ุงูุชุฏุฎูุงุช ุงูุทุจูุฉ | Hip joint in medical interventions |
| 70 | ุงุฎุชูุงุฑ ุถูุงุฏ ูุฌุฑูุญ ุงูุนุธู ุงูููุดููุฉ | Choosing dressing for exposed bone wounds |
| 71 | ุทุจูุนุฉ ุงูุฑุนุงูุฉ ุงูุฃูุซู ูุฅุตุงุจุงุช ูุชุนุฏุฏุฉ | Optimal care for multiple symptoms |
| 72 | ุชุญููู ุงุณุชุซูุงุฑ ูุดุฑูุน ุฌูุงุฒ CT Scan | CT Scan investment analysis |
| 73 | ุฃุณุนุงุฑ ูุงูููุงุช ุงูุชุตููุฑ ุงูููุชุจุงุช | Library imaging machine prices |

---

## ๐ Translation & OCR

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 74 | ุฅุนุฏุงุฏ ุจูุฆุฉ OCR ุซูุงุฆู ุงููุบุฉ | Setup bilingual OCR environment |
| 75 | ุฑุจุท ุงูุจูุฆุฉ ุจููุตุฉ Hugging Face | Connect environment to Hugging Face |
| 76 | ุฏูุฌ Mistral AI ูู ูุนุงูุฌุฉ ุงููุตูุต ุงูุนุฑุจูุฉ | Integrate Mistral AI for Arabic text |
| 77 | ุฅุถุงูุฉ ุงูุนุฑุจูุฉ ูุชุทุจูู DeepL | Add Arabic to DeepL app |

---

## ๐ค AI & Development

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 78 | ุฅุนุฏุงุฏ Ollama ูุงุณุชุฎุฏุงู API Key | Setup Ollama and use API Key |
| 79 | ุฅุฏุงุฑุฉ ููุงุชูุญ API ุจุดูู ุขูู | Manage API keys securely |
| 80 | ุงุณุชูุดุงู ูุดููุฉ API OpenRouter | Troubleshoot OpenRouter API issue |
| 81 | ุชุทุจูู Continue ูุน ููุงุฐุฌ DeepSeek | Continue app with DeepSeek models |
| 82 | ุชุทููุฑ ุงูุฐูุงุก ุงูุงุฌุชูุงุนู ุนุจุฑ ุงูููุงุทุน ุงูุตูุชูุฉ | Develop social intelligence via audio |
| 83 | ุญู ูุดููุฉ OpenRouter ูุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงููุญููุฉ | Fix OpenRouter and use local models |
| 84 | Troubleshooting API Key Authentication | API Key authentication troubleshooting |
| 85 | ููุงุฐุฌ ุฐูุงุก ุงุตุทูุงุนู ูุฌุงููุฉ ุบูุฑ ูุญุฏูุฏุฉ | Free unlimited AI models |
| 86 | ููุงูุน ูุฌุงููุฉ ููููุชุงุฌ ููุฏูู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู | Free AI video editing sites |
| 87 | ุจุฏุงุฆู ูุฌุงููุฉ ููุนุงูุฌุฉ ุงูููุฏ ูู ุจูุชุดุงุฑู | Free alternatives for PyCharm code processing |
| 88 | ุชุญููู ููุงุฉ ููุชููุจ ุนุฑุจูุฉ ููุจุฑูุฌุฉ ุฅูู ูุต | Convert Arabic programming YouTube to text |
| 89 | ุฅุถุงูุงุช ูุฌุงููุฉ ูู VSCode ู PyCharm | Free addons for VSCode and PyCharm |

---

## ๐ป Programming Projects

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 90 | SmartTextETL v8 ุชูุตูุงุช ููุฎุตุฉ | SmartTextETL v8 summary recommendations |
| 91 | ุชุญุณููุงุช ูุฅุตูุงุญุงุช ูุดุฑูุน SmartTextETL v8 | SmartTextETL v8 improvements and fixes |
| 92 | SmartTextETL v6 JWT Authentication | SmartTextETL v6 JWT Auth Guide |
| 93 | SmartTextETL v6 Project Overview | SmartTextETL v6 Overview |
| 94 | SmartTextETL v6 Data Processing | SmartTextETL v6 Data Processing |
| 95 | ุญู ูุดููุฉ PyTorch ูู Windows | Fix PyTorch on Windows |
| 96 | Python Package Installation Issues | Python package installation fixes |
| 97 | Fixing Python Syntax Error | Fix Python syntax error |
| 98 | SmartTextETL v4.3 bug fixes | SmartTextETL v4.3 fixes |
| 99 | ุชุตุฏูุฑ ูุงุณุชูุฑุงุฏ ุงูุตูุฑ ูุน ุฅูุณู | Export and import images with Excel |
| 100 | ุชูุฒูู ุญุฒูุฉ SmartTextETL ูุงููุฉ | Download complete SmartTextETL package |

---

## ๐ Miscellaneous

| # | ุงูุนููุงู ุงูุนุฑุจู | English Title |
|---|----------------|---------------|
| 101 | ูุตู ุขุซุงุฑ ุชุฏูุฑ ุงูุณูุฑูุฉ | Description of Palmyra ruins |
| 102 | ุณุจุจ ูููุงู ูุฃุณุงุฉ ุงูุฎุงุดูุฌู | Khashoggi tragedy cause and location |
| 103 | ุชูุฏูุฏุงุช ููุบุฉ ุชุญุฑูุถูุฉ ุชุฌุงู ุงูุณูุฑููู | Threats and incitement against Syrians |
| 104 | ุชุนุฑูู ุชุทุจูู Discord ูุงุณุชุฎุฏุงูุงุชู | Discord app definition and uses |
| 105 | ุจุญุซ ุณุนุฑ ููุชุฌุงุช ุนุธููุฉ ููุฏูุฉ | Indian orthopedic products price search |
| 106 | ููุงุฑูุฉ ุฃุณุนุงุฑ ูููุงุฏ ุนุฑูุถ ุฃูุณู | Excel offers price and material comparison |

---

## ๐ ุฅุญุตุงุฆูุงุช ุงูููุฑุณ

| ุงููุฆุฉ | ุงูุนุฏุฏ |
|-------|-------|
| Linux & System | 65 |
| Windows & Migration | 3 |
| Medical | 5 |
| Translation & OCR | 4 |
| AI & Development | 10 |
| Programming Projects | 11 |
| Miscellaneous | 8 |
| **ุงููุฌููุน** | **106** |

---

## ๐ ููููุฉ ุงูุจุญุซ

1. ุงุณุชุฎุฏู Ctrl+F ููุจุญุซ ุนู ูููุฉ ููุชุงุญูุฉ
2. ุฑุงุฌุน ุงููุฆุงุช ุฃุนูุงู ููุชุตูุญ ุญุณุจ ุงูููุถูุน
3. ูู ุนููุงู ูุฑุชุจุท ุจุงูููู ุงูููุงุจู ูู ุงููุณุชูุฏุน

================================================================================

ุงุณู ุงูููู: Text_snippets-main/digests/organize_corpus.md
----------------------------------------
# ุฏููู ุชูุธูู ุงููุตูุต ูุชุฏุฑูุจ ููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู

## ููุฎุต
ุฏููู ุดุงูู ูุชูุธูู ูุชูุญูุฏ ูููุงุช ูุตูุฉ ูุชููุนุฉ (ุฃุฏุจูุฉุ ุชูููุฉุ ุทุจูุฉุ ุชุฑุฌููุฉ) ูุฅุนุฏุงุฏูุง ููุฌููุนุฉ ุจูุงูุงุช ุนุงููุฉ ุงูุฌูุฏุฉ.

---

## ๐ ูููู ุชูุธูู ุงููููุงุช ุงูููุชุฑุญ

```
your-repo/
โโโ README.md                 # ูุตู ุงููุฌููุนุฉ ุงูุจูุงูุงุช
โโโ LICENSE                   # ุฑุฎุตุฉ ุงูุงุณุชุฎุฏุงู (MIT ุฃู CC-BY-SA)
โโโ scripts/
โ   โโโ organize_files.py     # ุชุตููู ุงููููุงุช ุชููุงุฆูุงู
โ   โโโ standardize_bilingual.py  # ุชูุญูุฏ ุงูุชูุณูู ุซูุงุฆู ุงููุบุฉ
โ   โโโ remove_duplicates.py  # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
โโโ data/
โ   โโโ medical/              # ูุตูุต ุทุจูุฉ (ุฅูุฌููุฒู โ ุนุฑุจู)
โ   โโโ technical/            # ูุตูุต ุชูููุฉ ูุจุฑูุฌูุฉ
โ   โโโ literary/             # ูุตูุต ุฃุฏุจูุฉ ูุดุนุฑูุฉ
โ   โโโ translations/         # ุชุฑุฌูุงุช ุนุงูุฉ
โโโ metadata/
    โโโ file_inventory.json   # ุฌุฑุฏ ุงููููุงุช ุงููุตุฏุฑูุฉ
    โโโ stats.csv             # ุฅุญุตุงุฆูุงุช (ุนุฏุฏ ุงูุฌููุ ุงููุบุงุชุ ุฅูุฎ)
```

---

## ๐๏ธ ุณูุฑูุจุช ุชุตููู ุงููููุงุช ุชููุงุฆูุงู

```python
#!/usr/bin/env python3
import os, re, shutil
from pathlib import Path

CATEGORIES = {
    'medical': [r'ุนุธู', r'ุฌุฑุงุญุฉ', r'ุฃุดุนุฉ', r'dicom', r'orthopedic', 
                r'surgery', r'radiology', r'fracture', r'bone'],
    'technical': [r'ููููุณ', r'ุจุงูุซูู', r'ููุฏ', r'git', r'linux', 
                  r'python', r'code', r'docker', r'ocr'],
    'literary': [r'ุดุนุฑ', r'ูุตูุฏุฉ', r'ุฑูุงูุฉ', r'poem', r'poetry', r'novel'],
    'translations': [r'ุชุฑุฌูุฉ', r'translat', r'vocabulary', r'ูุงููุณ']
}

def classify_content(text):
    text_lower = text.lower()
    for cat, keywords in CATEGORIES.items():
        if any(re.search(kw, text_lower, re.IGNORECASE) for kw in keywords):
            return cat
    return 'uncategorized'

def organize_files(source_dir, output_dir):
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    for root, _, files in os.walk(source_dir):
        for fname in files:
            if fname.lower().endswith(('.txt', '.md')):
                fpath = Path(root) / fname
                try:
                    content = fpath.read_text(encoding='utf-8', errors='ignore')
                    category = classify_content(content[:1000])
                    
                    dest_dir = Path(output_dir) / 'data' / category
                    dest_dir.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(fpath, dest_dir / fname)
                    print(f"โ {fname} โ {category}")
                except Exception as e:
                    print(f"โ ุฎุทุฃ ูู {fname}: {e}")
```

---

## ๐ ุณูุฑูุจุช ุชูุญูุฏ ุงูุชูุณูู ุซูุงุฆู ุงููุบุฉ

```python
#!/usr/bin/env python3
import re
from pathlib import Path

def is_arabic(text):
    return bool(re.search(r'[\u0600-\u06FF]', text))

def standardize_pair(eng, ara):
    """ุชูุญูุฏ ุงูุฌููุฉ ุงูุฅูุฌููุฒูุฉ โ ุงูุนุฑุจูุฉ ุจุชูุณูู TSV"""
    eng = re.sub(r'\s+', ' ', eng.strip())
    ara = re.sub(r'\s+', ' ', ara.strip())
    return f"{eng}\t{ara}"

def process_file(fpath, output_dir):
    content = fpath.read_text(encoding='utf-8', errors='ignore')
    lines = [l.strip() for l in content.splitlines() if l.strip()]
    
    pairs = []
    i = 0
    while i < len(lines) - 1:
        if is_arabic(lines[i]) != is_arabic(lines[i+1]):
            eng = lines[i] if not is_arabic(lines[i]) else lines[i+1]
            ara = lines[i+1] if is_arabic(lines[i+1]) else lines[i]
            pairs.append(standardize_pair(eng, ara))
            i += 2
        else:
            i += 1
    
    if pairs:
        out_path = Path(output_dir) / f"{fpath.stem}.tsv"
        out_path.write_text('\n'.join(pairs), encoding='utf-8')
        print(f"โ ุญูู {fpath.name} โ {len(pairs)} ุฒูุฌ ูุบูู")
```

---

## ๐งน ุณูุฑูุจุช ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช

```python
#!/usr/bin/env python3
import hashlib

def hash_pair(pair):
    return hashlib.md5(pair.lower().encode('utf-8')).hexdigest()

def deduplicate_tsv(input_path, output_path):
    seen = set()
    unique = []
    
    with open(input_path, 'r', encoding='utf-8') as f:
        for line in f:
            h = hash_pair(line)
            if h not in seen:
                seen.add(h)
                unique.append(line)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.writelines(unique)
    
    print(f"โ {input_path}: {len(unique)} ูุฑูุฏ ูู ุฃุตู {len(seen)}")
```

---

## ๐ ุฎุทูุงุช ุงูุชูููุฐ ุงูุนูููุฉ

```bash
# 1. ุงูุชุตููู
python scripts/organize_files.py /path/to/your/texts

# 2. ุงูุชูุญูุฏ (ูููุตูุต ุงูุซูุงุฆูุฉ ุงููุบุฉ)
python scripts/standardize_bilingual.py ./organized_output/data/translations

# 3. ุงูุชูุธูู
python scripts/remove_duplicates.py data/translations/general.tsv
```

---

## ๐ ุฑูุน ุงููุฌููุนุฉ ุฅูู GitHub

```bash
mkdir my-text-corpus && cd my-text-corpus
git init

cat > README.md << 'EOF'
# Arabic-English Medical & Technical Corpus

ูุฌููุนุฉ ุจูุงูุงุช ุซูุงุฆูุฉ ุงููุบุฉ ุนุงููุฉ ุงูุฌูุฏุฉ ูููุตูุต ุงูุทุจูุฉ ูุงูุชูููุฉ ูุงูุฃุฏุจูุฉ.

## ุงููุญุชููุงุช
- โ 15,000+ ุฒูุฌ ูุบูู (ุฅูุฌููุฒู โ ุนุฑุจู)
- โ ุชุตููู ุฏููู: ุทุจูุ ุชูููุ ุฃุฏุจู
- โ ุชูุณูู TSV ุฌุงูุฒ ูุชุฏุฑูุจ ุงูููุงุฐุฌ
- โ ุฎุงูู ูู ุงูุชูุฑุงุฑุงุช

## ุงูุงุณุชุฎุฏุงู
```python
import pandas as pd
df = pd.read_csv('data/medical/orthopedics.tsv', sep='\t', 
                  header=None, names=['en', 'ar'])
```

## ุงูุชุฑุฎูุต
CC-BY-SA 4.0 (ูููู ุงูุงุณุชุฎุฏุงู ุงูุชุฌุงุฑู ูุน ุงูุฅุณูุงุฏ)
EOF

git add .
git commit -m "Initial commit: organized bilingual corpus"
git remote add origin https://github.com/yourusername/your-repo.git
git push -u origin main
```

---

## ๐ก ูุตุงุฆุญ ูุชุฏุฑูุจ ุงูููุงุฐุฌ

### ูู Hugging Face
ุญูู ุงููููุงุช ุฅูู ุชูุณูู JSONL:
```json
{"translation": {"en": "Fracture of femur", "ar": "ูุณุฑ ูู ุนุธูุฉ ุงููุฎุฐ"}}
```

### ูุชุญุณูู ุงูุฌูุฏุฉ
ุฃุถู ุนููุฏ "ุซูุฉ" (confidence) ุจุนุฏ ูุนุงูุฌุฉ ุงููุตูุต ุจู OCR

### ูููุดุงุฑูุน ุงูุทุจูุฉ
ุฑูุฒ ุนูู `data/medical/` - ูุฐู ุงูุฃูุซุฑ ูููุฉ ููุชุฏุฑูุจ ุงููุชุฎุตุต

---

## ๐ค ุชุญููู ูุชูุณููุงุช ุงูุชุฏุฑูุจ

### JSONL ูู Hugging Face

```python
import json

def tsv_to_jsonl(tsv_path, jsonl_path):
    with open(tsv_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for line in lines:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                obj = {"translation": {"en": parts[0], "ar": parts[1]}}
                f.write(json.dumps(obj, ensure_ascii=False) + '\n')
```

---

## โ ุงููุฒุงูุง ุงูุฑุฆูุณูุฉ

| ุงูููุฒุฉ | ุงูุชูุงุตูู |
|--------|----------|
| **ุชุตููู ุชููุงุฆู** | 6 ุชุตูููุงุช ุฐููุฉ |
| **ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช** | ุฎูุงุฑุฒููุฉ MD5 |
| **ุชูุณูู ููุญุฏ** | TSV ุฌุงูุฒ ููุชุฏุฑูุจ |
| **ุซูุงุฆู ุงููุบุฉ** | ูุญุงูุธ ุนูู ุงูุฃุฒูุงุฌ ุงูุฅูุฌููุฒูโุงูุนุฑุจู |
| **ูุงุจู ููุชูุณูุน** | ุณูููุฉ ุฅุถุงูุฉ ูููุงุช ุฌุฏูุฏุฉ |

================================================================================

ุงุณู ุงูููู: Text_snippets-main/medical/orthopedics-supplies.md
----------------------------------------
# ุฃุณุนุงุฑ ุงููุณุชูุฒูุงุช ุงูุทุจูุฉ ุงูุนุธููุฉ

## ุฌุฏูู ุงูุฃุณุนุงุฑ ุงูุดุงูู (ุณูุฑูุง - 2026)

| # | ุงููุงุฏุฉ | ุงูููุงุตูุงุช | ุงููุตุฏุฑ | ุงููุนุฏู | ุงูููุงุฏ ุงููุทููุจุฉ ($) | ุงูุงุฎููู ($) | ุฌูุฏูู&ุณุจุงุนู&ุดุนุงุฑ ($) | ุงูุฎููู ($) | ุงุจู ุงููููุณ ($) | ุฃุฏูู ุณุนุฑ ($) | ุงูุดุฑูุฉ ุงูุฃุฑุฎุต |
|---|--------|----------|---------|--------|---------------------|-------------|----------------------|------------|----------------|--------------|---------------|
| 1 | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ุชุฑูู | ุชูุชุงูููู | 180 | 180 | 180 | - | - | 180 | ุงูุงุฎููู |
|   | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ุชุฑูู | ุณุชุงููุณ | 160 | 160 | 160 | - | - | 160 | ุงูุงุฎููู |
|   | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ููุฏู | ุชูุชุงูููู | 120 | 120 | 120 | 120 | - | 120 | ุงูุฎููู |
|   | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ููุฏู | ุณุชุงููุณ | 90 | 90 | 90 | 90 | 90 | 90 | ุงุจู ุงููููุณ |
| 2 | ุตูุงุฆุญ ูุณุชูููุฉ/ุณุงุนุฏ | ููููุฉ/ุบูุฑ ููููุฉ | ุชุฑูู | ุชูุชุงูููู | 150 | 150 | 150 | - | - | 150 | ุงูุงุฎููู |
|   | ุตูุงุฆุญ ูุณุชูููุฉ/ุณุงุนุฏ | ููููุฉ/ุบูุฑ ููููุฉ | ุชุฑูู | ุณุชุงููุณ | 150 | 150 | 150 | - | - | 150 | ุงูุงุฎููู |
|   | ุตูุงุฆุญ ูุณุชูููุฉ/ุณุงุนุฏ | ููููุฉ/ุบูุฑ ููููุฉ | ููุฏู | ุชูุชุงูููู | 100 | 100 | 100 | 100 | - | 100 | ุงูุฎููู |
|   | ุตูุงุฆุญ ูุณุชูููุฉ/ุณุงุนุฏ | ููููุฉ/ุบูุฑ ููููุฉ | ููุฏู | ุณุชุงููุณ | 80 | 80 | 80 | 80 | 80 | 80 | ุงุจู ุงููููุณ |
| 3 | ุจุฑูุดุงุช/ูุฑุดูุฑ | ุบูุฑ ููููุฉ | ููุฏู | ุณุชุงููุณ | 4 | 4 | 4 | 4 | 4 | 4 | ุงุจู ุงููููุณ |
| 4 | ุงุณูุงุฎ ูุฑูุฉ | - | ุชุฑูู | ุชูุชุงูููู | 70 | 70 | 70 | - | - | 70 | ุงูุงุฎููู |
|   | ุงุณูุงุฎ ูุฑูุฉ | - | ููุฏู | ุชูุชุงูููู | 20 | 20 | 20 | 20 | - | 20 | ุงูุฎููู |
|   | ุงุณูุงุฎ ูุฑูุฉ | - | ููุฏู | ุณุชุงููุณ | 15 | 15 | 15 | 15 | 15 | 15 | ุงุจู ุงููููุณ |
| 5 | ุณููุฏ/ูุญุฐ | - | ุชุฑูู | ุชูุชุงูููู | 250 | - | 250 | - | - | 250 | ุงูููุงุฏ ุงููุทููุจุฉ |
|   | ุณููุฏ/ูุญุฐ | - | ููุฏู | ุชูุชุงูููู | 170 | 170 | 170 | 170 | - | 170 | ุงูุฎููู |
|   | ุณููุฏ/ูุญุฐ | - | ููุฏู | ุณุชุงููุณ | 100 | 100 | 100 | 100 | 100 | 100 | ุงุจู ุงููููุณ |
| 6 | ุณููุฏ/ุณุงู | - | ุชุฑูู | ุชูุชุงูููู | 250 | - | 250 | - | - | 250 | ุงูููุงุฏ ุงููุทููุจุฉ |
|   | ุณููุฏ/ุณุงู | - | ููุฏู | ุชูุชุงูููู | 150 | 150 | 150 | 150 | - | 150 | ุงูุฎููู |
|   | ุณููุฏ/ุณุงู | - | ููุฏู | ุณุชุงููุณ | 100 | 100 | 100 | 100 | 100 | 100 | ุงุจู ุงููููุณ |
| 7 | ุณููุฏ/ุนุถุฏ | - | ููุฏู | ุณุชุงููุณ | 100 | 100 | 100 | 100 | 100 | 100 | ุงุจู ุงููููุณ |
| 8 | ุณููุฏ ุฑูุชุฑู | - | ููุฏู | ุณุชุงููุณ | 130 | 130 | 130 | 130 | 130 | 130 | ุงุจู ุงููููุณ |
| 9 | ุตูุงุฆุญ DHS | - | ุชุฑูู | ุชูุชุงูููู | 150 | 150 | 150 | - | - | 150 | ุงูุงุฎููู |
|   | ุตูุงุฆุญ DHS | - | ููุฏู | ุชูุชุงูููู | 120 | 120 | 120 | 120 | - | 120 | ุงูุฎููู |
|   | ุตูุงุฆุญ DHS | - | ููุฏู | ุณุชุงููุณ | 70 | 70 | 70 | 70 | 70 | 70 | ุงุจู ุงููููุณ |

## ุงูููุงุตู ุงูุงุตุทูุงุนูุฉ

| # | ุงููุงุฏุฉ | ุงูููุน | ุงููุตุฏุฑ | ุงูุณุนุฑ ($) | ุงูููุฑุฏ ุงูุฃุฑุฎุต |
|---|--------|-------|--------|-----------|---------------|
| 10 | ููุตู ูุฎุฐู/ุชูุชุงู (ุงุณููุชู) | ุงุณููุชู | ุงูุฑููู | 1300 | ุงูุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ุงุณููุชู) | ุงุณููุชู | ุงูุฑุจู | 750 | ุงูุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ุงุณููุชู) | ุงุณููุชู | ุชุฑูู ุงูุชูุงุฒ ุงูุฑุจู | 650 | ุงูุงุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ุงุณููุชู) | ุงุณููุชู | ุชุฑูู | 450 | ุงูุงุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ุงุณููุชู) | ุงุณููุชู | ููุฏู | 400 | ุงุจู ุงููููุณ |
| 11 | ููุตู ูุฎุฐู/ุชูุชุงู (ูุง ุงุณููุชู) | ูุง ุงุณููุชู | ุงูุฑููู | 1900 | ุงูุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ูุง ุงุณููุชู) | ูุง ุงุณููุชู | ุงูุฑุจู | 1500 | ุงุจู ุงููููุณ |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ูุง ุงุณููุชู) | ูุง ุงุณููุชู | ุชุฑูู ุงูุชูุงุฒ ุงูุฑุจู | 1100 | ุงูุงุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ูุง ุงุณููุชู) | ูุง ุงุณููุชู | ุชุฑูู | 800 | ุงูุงุฎููู |
|   | ููุตู ูุฎุฐู/ุชูุชุงู (ูุง ุงุณููุชู) | ูุง ุงุณููุชู | ููุฏู | 800 | ุงุจู ุงููููุณ |
| 12 | ููุตู ูุฎุฐู/ุจุงุจููููุฑ (ุงุณููุชู) | ุงุณููุชู | ุงูุฑููู | 1000 | ุงูุฎููู |
|   | ููุตู ูุฎุฐู/ุจุงุจููููุฑ (ุงุณููุชู) | ุงุณููุชู | ุงูุฑุจู | 700 | ุงูุฎููู |
|   | ููุตู ูุฎุฐู/ุจุงุจููููุฑ (ุงุณููุชู) | ุงุณููุชู | ุชุฑูู | 400 | ุงูุฎููู |
|   | ููุตู ูุฎุฐู/ุจุงุจููููุฑ (ุงุณููุชู) | ุงุณููุชู | ุชุฑูู ุงูุชูุงุฒ ุงูุฑุจู | 550 | ุงูุงุฎููู |
|   | ููุตู ูุฎุฐู/ุจุงุจููููุฑ (ุงุณููุชู) | ุงุณููุชู | ููุฏู | 400 | ุงุจู ุงููููุณ |
| 14 | ููุตู ุฑูุจุฉ | - | ุงูุฑููู | 1900 | ุงูุฎููู |
|   | ููุตู ุฑูุจุฉ | - | ุงูุฑุจู | 1500 | ุงุจู ุงููููุณ |
|   | ููุตู ุฑูุจุฉ | - | ุชุฑูู ุงูุชูุงุฒ ุงูุฑุจู | 1100 | ุงูุงุฎููู |
|   | ููุตู ุฑูุจุฉ | - | ููุฏู | 1000 | ุงุจู ุงููููุณ |
| 15 | ููุตู ูุฑูู | - | ุงูุฑุจู | 1600 | ุงูุฎููู |
|   | ููุตู ูุฑูู | - | ุชุฑูู | 1500 | ุงูุฎููู |
|   | ููุตู ูุฑูู | - | ููุฏู | 1000 | ุงุจู ุงููููุณ |

## ุงูุจุฑุงุบู ูุงูุชุซุจูุช

| # | ุงููุงุฏุฉ | ุงููุตุฏุฑ | ุงููุนุฏู | ุงูุณุนุฑ ($) | ุงูููุฑุฏ ุงูุฃุฑุฎุต |
|---|--------|--------|--------|-----------|---------------|
| 16 | ุจุฑุงุบู ูุนุจู | ุชุฑูู | ุชูุชุงูููู | 45 | ุงูุงุฎููู |
|   | ุจุฑุงุบู ูุนุจู | ููุฏู | ุชูุชุงูููู | 20 | ุงูุฎููู |
|   | ุจุฑุงุบู ูุนุจู | ููุฏู | ุณุชุงููุณ | 15 | ุงุจู ุงููููุณ |
| 17 | ุจุฑุงุบู ูุงููููุชูุฏ | ุชุฑูู | ุชูุชุงูููู | 75 | ุงูุงุฎููู |
|   | ุจุฑุงุบู ูุงููููุชูุฏ | ููุฏู | ุชูุชุงูููู | 25 | ุงูุฎููู |
|   | ุจุฑุงุบู ูุงููููุชูุฏ | ููุฏู | ุณุชุงููุณ | 20 | ุงุจู ุงููููุณ |
| 18 | ุจุฑุงุบู ุงุณููุฌูุฉ | ุชุฑูู | ุชูุชุงูููู | 9 | ุงูุงุฎููู |
|   | ุจุฑุงุบู ุงุณููุฌูุฉ | ููุฏู | ุชูุชุงูููู | 8 | ุงุจู ุงููููุณ |
| 19 | ุบุงูุงููู | ููุฏู | ุชูุชุงูููู | 200 | ุงูุฎููู |
|   | ุบุงูุงููู | ููุฏู | ุณุชุงููุณ | 175 | ุงุจู ุงููููุณ |
| 20 | ุบุงูุงููู PFN | ุชุฑูู | ุณุชุงููุณ | 150 | ุงุจู ุงููููุณ |
| 21 | ุณููุฏ PFN | ุชุฑููุง | ุชูุชุงูููู | 425 | ุงุจู ุงููููุณ |

## ุฃุฌูุฒุฉ ุงูุชุซุจูุช ุงูุฎุงุฑุฌู

| # | ุงููุงุฏุฉ | ุงููุตุฏุฑ | ุงููุงุฏุฉ | ุงูุณุนุฑ ($) | ุงูููุฑุฏ ุงูุฃุฑุฎุต |
|---|--------|--------|--------|-----------|---------------|
| 22 | ุงุฌูุฒุฉ ุชุซุจูุช ุฎุงุฑุฌู (ุงุฑุซููููุณ) | ุชุฑูู | ูุฑุจูู | 170 | ุงูุงุฎููู |
|   | ุงุฌูุฒุฉ ุชุซุจูุช ุฎุงุฑุฌู (ุงุฑุซููููุณ) | ุชุฑูู | ุงูููููู | 100 | ุงุจู ุงููููุณ |
|   | ุงุฌูุฒุฉ ุชุซุจูุช ุฎุงุฑุฌู (ุงุฑุซููููุณ) | ููุฏู | ุณุชุงููุณ | 200 | ุงูุฎููู |
| 23 | ุงุฌูุฒุฉ ุชุซุจูุช ุฎุงุฑุฌู (AO) | ููุฏู | ุณุชุงููุณ | 90 | ุงุจู ุงููููุณ |
| 24 | ุฌูุงุฒ ุชุทููู ุงุตูุตูุฑ | ุชุฑูู | ูุฑุจูู | 300 | ุงุจู ุงููููุณ |
|   | ุฌูุงุฒ ุชุทููู ุงุตูุตูุฑ | ููุฏู | ุณุชุงููุณ | 200 | ุงุจู ุงููููุณ |
| 28 | ุฌูุงุฒ ุงููุฒุงุฑูู (ุญููุฉ) | ุชุฑูู | ูุฑุจูู | 75 | ุงูุฎููู |
|   | ุฌูุงุฒ ุงููุฒุงุฑูู (ุญููุฉ) | ููุฏู | ุณุชุงููุณ | 25 | ุงุจู ุงููููุณ |

## ุจุฑุงุบู ูุชุฎุตุตุฉ

| # | ุงููุงุฏุฉ | ุงููุตุฏุฑ | ุงููุนุฏู | ุงูุณุนุฑ ($) | ุงูููุฑุฏ ุงูุฃุฑุฎุต |
|---|--------|--------|--------|-----------|---------------|
| 25 | ุจุฑุงุบู ููุฑุจุฑุช | ุชุฑูู | ุชูุชุงูููู | 85 | ุงูุงุฎููู |
|   | ุจุฑุงุบู ููุฑุจุฑุช | ููุฏู | ุชูุชุงูููู | 70 | ุงุจู ุงููููุณ |
| 26 | ุจุฑุงุบู ููุฏููุณ | ุชุฑูู | ุชูุชุงูููู | 85 | ุงูุงุฎููู |
|   | ุจุฑุงุบู ููุฏููุณ | ุชุฑูู ููุชุตุฉ | - | 150 | ุงุจู ุงููููุณ |
|   | ุจุฑุงุบู ููุฏููุณ | ููุฏู | ุชูุชุงูููู | 60 | ุงูุฎููู |
| 27 | ุณูุฑูููุงุฌ | ููุฏู | - | 15 | ุงุจู ุงููููุณ |
| 29 | ุจุฑุบู ููู | ุชุฑูู | ุชูุชุงูููู | 12 | ุงูุฎููู |
|   | ุจุฑุบู ููู | ุชุฑูู | ุณุชุงููุณ | 9 | ุงูุฎููู |
|   | ุจุฑุบู ููู | ููุฏู | ุณุชุงููุณ | 7 | ุงุจู ุงููููุณ |
| 30 | ุจุฑุบู ูุดุฑู | ุชุฑูู | ุชูุชุงูููู | 8 | ุงูุฎููู |
|   | ุจุฑุบู ูุดุฑู | ุชุฑูู | ุณุชุงููุณ | 5 | ุงูุฎููู |
|   | ุจุฑุบู ูุดุฑู | ููุฏู | ุณุชุงููุณ | 4 | ุงุจู ุงููููุณ |

## ููุฎุต ุงูููุฑุฏูู

| ุงูููุฑุฏ | ุงููููุฒุงุช |
|--------|----------|
| **ุงุจู ุงููููุณ** | ุฃูู ุณุนุฑ ููููุชุฌุงุช ุงูููุฏูุฉ ุงูุณุชุงููุณ |
| **ุงูุฎููู** | ุฃูู ุณุนุฑ ููููุชุฌุงุช ุงูููุฏูุฉ ุงูุชูุชุงูููู |
| **ุงูุงุฎููู** | ุฃูู ุณุนุฑ ููููุชุฌุงุช ุงูุชุฑููุฉ |

## ูุตุงุฆุญ ุงูุดุฑุงุก

1. **ููููุฒุงููุฉ ุงููุญุฏูุฏุฉ**: ุงุฎุชุฑ ุงูููุชุฌุงุช ุงูููุฏูุฉ ุณุชุงููุณ ูู ุงุจู ุงููููุณ
2. **ููุฌูุฏุฉ ุงููุชูุณุทุฉ**: ุงุฎุชุฑ ุงูููุชุฌุงุช ุงูููุฏูุฉ ุชูุชุงูููู ูู ุงูุฎููู
3. **ููุฌูุฏุฉ ุงูุนุงููุฉ**: ุงุฎุชุฑ ุงูููุชุฌุงุช ุงูุชุฑููุฉ ูู ุงูุงุฎููู
4. **ููููุงุตู ุงูุงุตุทูุงุนูุฉ**: ุงุฎุชุฑ ุญุณุจ ุงูููุฒุงููุฉ ูุงูููุงุตูุงุช ุงููุทููุจุฉ

================================================================================

ุงุณู ุงูููู: Text_snippets-main/medical/orthopedics/supplies-prices.md
----------------------------------------
# ุฃุณุนุงุฑ ุงููุณุชูุฒูุงุช ุงูุทุจูุฉ ุงูุนุธููุฉ | Orthopedic Supplies Prices

## ููุฎุต
ุฌุฏูู ููุงุฑูุฉ ุฃุณุนุงุฑ ุงููุณุชูุฒูุงุช ุงูุทุจูุฉ ุงูุนุธููุฉ ูู ุนุฏุฉ ููุฑุฏููุ ูุดูู ุงูุบุฑุณุงุช ูุงููุณุงููุฑ ูุงูููุงุตู ุงูุงุตุทูุงุนูุฉ.

---

## ๐ ุฌุฏูู ุงูุฃุณุนุงุฑ

| # | ุงููุงุฏุฉ | ุงูููุงุตูุงุช | ุงููุตุฏุฑ/ุจูุฏ ุงูููุดุฃ | ููุน ุงููุนุฏู | ุงูุงุฎููู ($) | ุฌูุฏูู&ุณุจุงุนู&ุดุนุงุฑ ($) | ุงูุฎููู ($) | ุงุจู ุงููููุณ ($) | ุฃุฏูู ุณุนุฑ ($) |
|---|--------|-----------|-------------------|-------------|-------------|----------------------|------------|----------------|--------------|
| 1 | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ุชุฑูู | ุชูุชุงูููู | 180 | 180 | - | - | 180 |
|   | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ููุฏู | ุชูุชุงูููู | 120 | 120 | 120 | - | 120 |
|   | ุตูุงุฆุญ ุชุดุฑูุญูุฉ | ููููุฉ | ููุฏู | ุณุชุงููุณ | 90 | 90 | 90 | 90 | 90 |
| 2 | ุตูุงุฆุญ ูุณุชูููุฉ/ุณุงุนุฏ | ููููุฉ/ุบูุฑ ููููุฉ | ุชุฑูู | ุชูุชุงูููู | 150 | 150 | - | - | 150 |
|   | ุตูุงุฆุญ ูุณุชูููุฉ/ุณุงุนุฏ | ููููุฉ/ุบูุฑ ููููุฉ | ููุฏู | ุชูุชุงูููู | 100 | 100 | 100 | - | 100 |
| 3 | ุจุฑูุดุงุช/ูุฑุดูุฑ | ุบูุฑ ููููุฉ | ููุฏู | ุณุชุงููุณ | 4 | 4 | 4 | 4 | 4 |
| 4 | ุงุณูุงุฎ ูุฑูุฉ | - | ุชุฑูู | ุชูุชุงูููู | 70 | 70 | - | - | 70 |
|   | ุงุณูุงุฎ ูุฑูุฉ | - | ููุฏู | ุชูุชุงูููู | 20 | 20 | 20 | - | 20 |
|   | ุงุณูุงุฎ ูุฑูุฉ | - | ููุฏู | ุณุชุงููุณ | 15 | 15 | 15 | 15 | 15 |
| 5 | ุณููุฏ/ูุฎุฐ | - | ุชุฑูู | ุชูุชุงูููู | 250 | - | 250 | - | 250 |
|   | ุณููุฏ/ูุฎุฐ | - | ููุฏู | ุชูุชุงูููู | 170 | 170 | 170 | 170 | 170 |
|   | ุณููุฏ/ูุฎุฐ | - | ููุฏู | ุณุชุงููุณ | 100 | 100 | 100 | 100 | 100 |
| 6 | ุณููุฏ/ุณุงู | - | ุชุฑูู | ุชูุชุงูููู | 250 | - | 250 | - | 250 |
|   | ุณููุฏ/ุณุงู | - | ููุฏู | ุชูุชุงูููู | 150 | 150 | 150 | 150 | 150 |

---

## ๐ฆด ุงูููุงุตู ุงูุงุตุทูุงุนูุฉ

### ููุตู ูุฎุฐู/ุชูุชุงู (ุงุณููุชู)

| ุงููุตุฏุฑ | ุงูุณุนุฑ ($) |
|--------|-----------|
| ุฃูุฑููู | 1300 |
| ุฃูุฑุจู | 750 |
| ุชุฑูู ุงูุชูุงุฒ ุฃูุฑุจู | 650 |
| ุชุฑูู | 450 |
| ููุฏู | 400 |

### ููุตู ูุฎุฐู/ุชูุชุงู (ูุง ุงุณููุชู)

| ุงููุตุฏุฑ | ุงูุณุนุฑ ($) |
|--------|-----------|
| ุฃูุฑููู | 1900 |
| ุฃูุฑุจู | 1500 |
| ุชุฑูู ุงูุชูุงุฒ ุฃูุฑุจู | 1100 |
| ุชุฑูู | 800 |
| ููุฏู | 800 |

### ููุตู ุฑูุจุฉ

| ุงููุตุฏุฑ | ุงูุณุนุฑ ($) |
|--------|-----------|
| ุฃูุฑููู | 1900 |
| ุฃูุฑุจู | 1500 |
| ุชุฑูู ุงูุชูุงุฒ ุฃูุฑุจู | 1100 |
| ููุฏู | 1000 |

---

## ๐ฉ ุงููุณุงููุฑ ูุงูุจุฑุงุบู ุงููุชุฎุตุตุฉ

| ุงููุงุฏุฉ | ุงููุตุฏุฑ | ุงูููุน | ุงูุณุนุฑ ($) |
|--------|--------|-------|-----------|
| ุจุฑุงุบู ูุนุจู | ุชุฑูู | ุชูุชุงูููู | 45 |
| ุจุฑุงุบู ูุนุจู | ููุฏู | ุชูุชุงูููู | 20 |
| ุจุฑุงุบู ูุงููููุชูุฏ | ุชุฑูู | ุชูุชุงูููู | 75 |
| ุจุฑุงุบู ูุงููููุชูุฏ | ููุฏู | ุชูุชุงูููู | 25 |
| ุจุฑุงุบู ุงุณููุฌูุฉ | ุชุฑูู | ุชูุชุงูููู | 9 |
| ุบุงูุงููู | ููุฏู | ุชูุชุงูููู | 200 |
| ุบุงูุงููู | ููุฏู | ุณุชุงููุณ | 175 |
| ุจุฑุงุบู ููุฑุจุฑุช | ุชุฑูู | ุชูุชุงูููู | 85 |
| ุจุฑุงุบู ููุฑุจุฑุช | ููุฏู | ุชูุชุงูููู | 70 |
| ุจุฑุบู ููู | ุชุฑูู | ุชูุชุงูููู | 12 |
| ุจุฑุบู ููู | ููุฏู | ุณุชุงููุณ | 7 |

---

## โ๏ธ ุฃุฌูุฒุฉ ุงูุชุซุจูุช ุงูุฎุงุฑุฌู

| ุงูุฌูุงุฒ | ุงููุตุฏุฑ | ุงููุงุฏุฉ | ุงูุณุนุฑ ($) |
|--------|--------|--------|-----------|
| ุฌูุงุฒ ุชุซุจูุช ุฎุงุฑุฌู (ุงุฑุซููููุณ) | ุชุฑูู | ูุฑุจูู | 170 |
| ุฌูุงุฒ ุชุซุจูุช ุฎุงุฑุฌู (ุงุฑุซููููุณ) | ุชุฑูู | ุฃูููููู | 100 |
| ุฌูุงุฒ ุชุซุจูุช ุฎุงุฑุฌู (AO) | ููุฏู | ุณุชุงููุณ | 90 |
| ุฌูุงุฒ ุชุทููู ุงุตูุตูุฑ | ุชุฑูู | ูุฑุจูู | 300 |
| ุฌูุงุฒ ุชุทููู ุงุตูุตูุฑ | ููุฏู | ุณุชุงููุณ | 200 |
| ุฌูุงุฒ ุงููุฒุงุฑูู (ููุญููุฉ) | ุชุฑูู | ูุฑุจูู | 75 |
| ุฌูุงุฒ ุงููุฒุงุฑูู (ููุญููุฉ) | ููุฏู | ุณุชุงููุณ | 25 |

---

## ๐ก ููุงุญุธุงุช ูููุฉ

### ุงูุนูุงูู ุงููุคุซุฑุฉ ุนูู ุงูุณุนุฑ:
1. **ุจูุฏ ุงูููุดุฃ**: ุงูููุชุฌุงุช ุงูุฃูุฑูููุฉ ูุงูุฃูุฑุจูุฉ ุฃุบูู ุจูุณุจุฉ 50-100%
2. **ููุน ุงููุนุฏู**: ุงูุชูุชุงูููู ุฃุบูู ูู ุงูุณุชุงููุณ ุณุชูู ุจูุณุจุฉ 30-50%
3. **ุงูุถูุงู ูุงูุดูุงุฏุงุช**: ุงูููุชุฌุงุช ุงููุนุชูุฏุฉ ุฃุบูู

### ุชูุตูุงุช ุงูุดุฑุงุก:
- ูููุฑุถู ุฐูู ุงูุฏุฎู ุงููุญุฏูุฏ: ุงูููุชุฌุงุช ุงูููุฏูุฉ ุชููุฑ ุฌูุฏุฉ ููุจููุฉ ุจุฃุณุนุงุฑ ูุนูููุฉ
- ููุญุงูุงุช ุงูุญุฑุฌุฉ: ุงูููุชุฌุงุช ุงูุชุฑููุฉ ุฃู ุงูุฃูุฑุจูุฉ ุชููุฑ ุถูุงูุฉ ุฃูุถู
- ููุนูููุงุช ุงููุนูุฏุฉ: ุงุณุชุดุงุฑุฉ ุงูุฌุฑุงุญ ูุงุฎุชูุงุฑ ุงูููุน ุงูููุงุณุจ

---

## ๐ ุงููุตุทูุญุงุช ุงูุทุจูุฉ

| ุงููุตุทูุญ ุงูุนุฑุจู | ุงููุตุทูุญ ุงูุฅูุฌููุฒู |
|----------------|-------------------|
| ุตูุงุฆุญ ุชุดุฑูุญูุฉ | Anatomical Plates |
| ุณููุฏ | Intramedullary Nail |
| ุจุฑุบู ูุงููููุชูุฏ | Cannulated Screw |
| ุบุงูุงููู | Gamma Nail |
| ุฌูุงุฒ ุงููุฒุงุฑูู | Ilizarov Fixator |
| ููุตู ุชูุชุงู | Total Joint Replacement |
| ุงุณููุชู | Cemented |
| ูุง ุงุณููุชู | Uncemented |

---

> โ๏ธ **ุชูุจูู**: ุงูุฃุณุนุงุฑ ูุฐููุฑุฉ ููุฃุบุฑุงุถ ุงููุฑุฌุนูุฉ ููุท ููุฏ ุชุฎุชูู ุญุณุจ ุงูุณูู ูุงููููุฉ. ุงุณุชุดุฑ ููุฑุฏูู ุงููุญูููู ููุญุตูู ุนูู ุฃุณุนุงุฑ ูุญุฏุซุฉ.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/references/analysis-plan.md
----------------------------------------
# ุชุญููู ููุงุฑู ูุฎุทุฉ ุนูู ูุญุณูุฉ

## ๐ ููุงุฑูุฉ ุจูู ุนูู Qwen ูุนูู Super Z

| ุงููุนูุงุฑ | ุนูู Qwen | ุนูู Super Z |
|---------|----------|-------------|
| **ุงูุชูุณูู** | ููู ูุตู ูุงุญุฏ ููุญุฏ | ูุณุชูุฏุน GitHub ููุธู ุจูุฌูุฏุงุช |
| **ุงูุชุตููู** | 4 ุชุตูููุงุช ูู ููู ูุงุญุฏ | ูููุงุช ูููุตูุฉ ููู ุชุตููู |
| **ุงูุชูุงุตูู** | ูุฎุชุตุฑุฉ ููุฑูุฒุฉ | ุชูุตูููุฉ ูุน ุฌุฏุงูู |
| **ุงูุฑูุน** | ุฎุทูุงุช ูุฏููุฉ ูุทููุจุฉ | ุชู ุงูุฑูุน ูุนููุงู |
| **ุงูุงุณุชุฎุฏุงู** | ููุงุณุจ ููุชุฏุฑูุจ ุงูุณุฑูุน | ููุงุณุจ ูููุฑุฌุนูุฉ ูุงูุจุญุซ |

## ๐ฏ ุฑุฃูู ูู ุนูู Qwen

### ุงูููุงุท ุงูุฅูุฌุงุจูุฉ โ
- ุชูุณูู ููุญุฏ ูุณูู ุงููุฑุงุกุฉ
- ุชุตููู ุฐูู ูููุญุชูู
- ุงุณุชุฎุฑุงุฌ ุงูููุงุท ุงูุฑุฆูุณูุฉ
- ููุงุณุจ ูุชุฏุฑูุจ ููุงุฐุฌ AI

### ุงูููุงุท ุงูุณูุจูุฉ โ
- ูุง ููุฌุฏ ุฑูุน ูุนูู ูู GitHub
- ููู ูุงุญุฏ ูุจูุฑ (ุตุนุจ ุงูุจุญุซ)
- ุชูุงุตูู ุฃูู ูู ุงููุทููุจ
- ูุง ููุฌุฏ ููุฑุณ ูููุญุชููุงุช

## ๐ก ุงูุฎุทุฉ ุงููุญุณูุฉ: ุงูุชุนุงูู ุจูู ุงูููุงุฐุฌ

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                    ุฎุทุฉ ุงูุนูู ุงููุชูุงููุฉ                           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ                                                                 โ
โ  ูุฑุญูุฉ 1: ุงูุชุญููู ุงูุฃููู                                        โ
โ  โโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโ         โ
โ  โ   DeepSeek  โโโโโถโ    Qwen     โโโโโถโ   Super Z   โ         โ
โ  โ (ููู ุณูุงู)  โ    โ (ุชุตููู)     โ    โ (ุชูููุฐ)     โ         โ
โ  โโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโ         โ
โ                                                                 โ
โ  ูุฑุญูุฉ 2: ุงููุนุงูุฌุฉ ูุงูุชูุซูู                                     โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ   โ
โ  โ  Super Z: ุฅูุดุงุก ูููู GitHub + ุฑูุน + ูุซุงุฆู               โ   โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ   โ
โ                                                                 โ
โ  ูุฑุญูุฉ 3: ุงูุฅุซุฑุงุก ูุงูุชุญุณูู                                      โ
โ  โโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโ                            โ
โ  โ  DeepSeek   โโโโโถโ  ุฅุซุฑุงุก      โ                            โ
โ  โ (ูุฑุงุฌุนุฉ)    โ    โ (ุฅุถุงูุฉ)     โ                            โ
โ  โโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโ                            โ
โ                                                                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

## ๐ ุฏูุฑ ูู ูููุฐุฌ ูู ุงูุฎุทุฉ

### DeepSeek (ุงููุญูู ุงูุฐูู)
- ููู ุงูุณูุงู ุงูุนููู ูููุญุชูู
- ุงูุชุฑุงุญ ุงูุชุตูููุงุช ุงูุฏูุงููุฉ
- ุชุญุฏูุฏ ุงูุนูุงูุงุช ุจูู ุงูููุงุถูุน
- ูุฑุงุฌุนุฉ ุงูุฌูุฏุฉ

### Qwen (ุงููุตูู ุงูุณุฑูุน)
- ุชุตููู ุณุฑูุน ูููููุงุช ุงููุจูุฑุฉ
- ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ
- ุชูููุฏ ููุฎุตุงุช
- ุฅุนุฏุงุฏ ุจูุงูุงุช ุงูุชุฏุฑูุจ

### Super Z (ุงููููุฐ)
- ุฅูุดุงุก ุงููููู ุงููุนูู
- ูุชุงุจุฉ ุงููููุงุช
- ุงูุฑูุน ุฅูู GitHub
- ุงูุชูููุฐ ุงูููุฑู

## ๐ ุฎุทุฉ ุงูุนูู ุงูููุชุฑุญุฉ ูููุฑุญูุฉ ุงููุงุฏูุฉ

### ุงูุฃุณุจูุน 1: ุฌูุน ูุชุตููู
```
1. DeepSeek: ุชุญููู ุงููููุงุช ุงูุฌุฏูุฏุฉ
2. Qwen: ุชุตููู ุณุฑูุน
3. Super Z: ุฅุถุงูุฉ ูููุณุชูุฏุน
```

### ุงูุฃุณุจูุน 2: ุฅุซุฑุงุก ุงููุญุชูู
```
1. ูุฑุงุฌุนุฉ ุงููุญุชูู ุงููุฑููุน
2. ุฅุถุงูุฉ ูุนูููุงุช ููููุฏุฉ
3. ุชุญุณูู ุงูุชูุณูู
4. ุฅูุดุงุก ููุฑุณ ุจุญุซ
```

### ุงูุฃุณุจูุน 3: ุชุฏุฑูุจ ุงูููุงุฐุฌ
```
1. ุชุญููู ุงูุจูุงูุงุช ูุตูุบุฉ TSV/JSONL
2. ุฑูุน ูู Hugging Face
3. ุชุฏุฑูุจ ูููุฐุฌ ุชุฑุฌูุฉ ุทุจู
```

## ๐ ุฃูุงูุฑ ูููุฏุฉ ููุฎุทูุฉ ุงููุงุฏูุฉ

```bash
# ุชูููุฐ ุณูุฑูุจุช ุงูุญุฐู
chmod +x /home/z/my-project/download/remove_processed_files.sh
./remove_processed_files.sh

# ุฅุถุงูุฉ ูููุงุช ุฌุฏูุฏุฉ ูููุณุชูุฏุน
cd /home/z/my-project/download/knowledge-base
git add -A
git commit -m "Add: Qwen corpus and analysis"

# ุงูุฑูุน
git push origin main
```

## ๐ ุชูุตูุชู ุงูููุงุฆูุฉ

1. **ุงุญุชูุธ ุจุนูู Qwen** ูู `corpus_unified.txt` ูู ุงููุณุชูุฏุน
2. **ุงุณุชุฎุฏู Super Z** ููุจููุฉ ูุงูุชูุธูู
3. **ุงุณุชุฎุฏู DeepSeek** ูููุฑุงุฌุนุฉ ูุงูุฅุซุฑุงุก
4. **ุงุฏูุฌ ุฌููุฏ ุงูููุงุฐุฌ ุงูุซูุงุซุฉ** ููุชูุฌุฉ ูุซูู

================================================================================

ุงุณู ุงูููู: Text_snippets-main/references/file_index.json
----------------------------------------
[
  {
    "file": "Aaa2_zip_contents (1).txt",
    "path": "/home/z/my-project/upload/Aaa2_zip_contents (1).txt",
    "category": "technical",
    "size": 12764461,
    "translations_found": 212
  },
  {
    "file": "Aaa_zip_contents (1).txt",
    "path": "/home/z/my-project/upload/Aaa_zip_contents (1).txt",
    "category": "technical",
    "size": 9992795,
    "translations_found": 158
  },
  {
    "file": "Archive_zip_contents.txt",
    "path": "/home/z/my-project/upload/Archive_zip_contents.txt",
    "category": "technical",
    "size": 2348873,
    "translations_found": 0
  },
  {
    "file": "Garuda-Ultimate-Enhanced-v5.0-FINAL_folder_contents.txt",
    "path": "/home/z/my-project/upload/Garuda-Ultimate-Enhanced-v5.0-FINAL_folder_contents.txt",
    "category": "technical",
    "size": 80920,
    "translations_found": 1
  },
  {
    "file": "Garuda-Ultimate-Enhanced-v5.0-MODIFIED_zip_contents (2).txt",
    "path": "/home/z/my-project/upload/Garuda-Ultimate-Enhanced-v5.0-MODIFIED_zip_contents (2).txt",
    "category": "technical",
    "size": 89687,
    "translations_found": 1
  },
  {
    "file": "New Folder_zip_contents.txt",
    "path": "/home/z/my-project/upload/New Folder_zip_contents.txt",
    "category": "technical",
    "size": 292141,
    "translations_found": 0
  },
  {
    "file": "garuda-ultimate-zorin-edition-v2.0.tar-1_file_contents.txt",
    "path": "/home/z/my-project/upload/garuda-ultimate-zorin-edition-v2.0.tar-1_file_contents.txt",
    "category": "misc",
    "size": 16327,
    "translations_found": 0
  },
  {
    "file": "packages_backup_20260202.txt",
    "path": "/home/z/my-project/upload/packages_backup_20260202.txt",
    "category": "technical",
    "size": 5794,
    "translations_found": 0
  },
  {
    "file": "packages_backup_selective_20260202.txt",
    "path": "/home/z/my-project/upload/packages_backup_selective_20260202.txt",
    "category": "technical",
    "size": 5118,
    "translations_found": 0
  }
]

================================================================================

ุงุณู ุงูููู: Text_snippets-main/references/topics-index.md
----------------------------------------
# ููุฑุณ ุงูููุงุถูุน ูุงููุญุงุฏุซุงุช

## ุชูููุฉ - Linux ู Garuda

| # | ุงูููุถูุน |
|---|---------|
| 1 | ุชุซุจูุช ูุชุดุบูู Garuda Ultimate Enhanced v5.0 |
| 2 | ุฅุตูุงุญ ูุดุงูู ุชุญุฏูุซ ูุฑุงูุง Garuda Linux |
| 3 | ุชุทุจูู ุฎุทูุท Windows 11 ูู Linux |
| 4 | ุชุซุจูุช ุฃุฏูุงุช DICOM ุงูุทุจูุฉ ุนูู Linux |
| 5 | ุชุทููุฑ ุณูุฑูุจุช ูุฏุนู ุฃูุชุฏุงุฏุงุช ุงูุถุบุท |
| 6 | ุฅุฏุงุฑุฉ ุจูุฆุงุช ุณุทุญ ุงูููุชุจ ูู Garuda Linux |
| 7 | ุชูููู GNOME ููุธูุฑ Windows 11 |
| 8 | ุญู ูุดุงูู Zorin OS ุนูู Wayland |
| 9 | ุญููู ูุชุซุจูุช ูุชูููู ุฅุนุฏุงุฏุงุช Zorin OS |
| 10 | Garuda Fusion v4.0 Error Registration Issue |
| 11 | ุชุซุจูุช ูุญู ูุดุงูู Latte Dock ูู Garuda |
| 12 | ุชููู ุชุญููู ููู ูู ุฅุนุฏุงุฏ Linux |
| 13 | ุชุซุจูุช ุณูุฑูุจุช ุงุณุชุฎุฑุงุฌ ุงูุฃุฑุดููุงุช |
| 14 | ุชุญุฏูุซุงุช ูููุฒุงุช ุฌุฏูุฏุฉ ูู ุฅุตุฏุงุฑ v4.0 |
| 15 | ุชุซุจูุช ุฃุฏูุงุช DICOM ููุดุงูุฏุฉ ุงูุณุฌูุงุช |
| 16 | ุจุฑูุงูุฌ ูุชุญููู ุงููููุงุช ุงููุถุบูุทุฉ ุฅูู ูุต |
| 17 | ูุดุฑูุน Garuda Ultimate Enhanced v5.0 ุดุงูู |
| 18 | ุชูุฒูู ุญุฒูุฉ ุฎุท ุงููุตูุต Amiri |
| 19 | ุฅูุดุงุก ุดุฑูุท ููุงู ูุดุจู ูููุฏูุฒ ูู Garuda |
| 20 | ููุฎุต ูุดุฑูุน Garuda Ultimate Fusion |
| 21 | ูุดุฑูุน Garuda Linux ุจุชุฎุตูุต ุนุฑุจู ูุชูุงูู |
| 22 | ุฅุตุฏุงุฑ ุซุงูู ููุดุฑูุน Garuda Ultimate Zorin |
| 23 | ุชุตุญูุญ ุณูุฑูุจุช ุชุซุจูุช ุจุฑุงูุฌ Garuda Linux |
| 24 | ุชุทููุฑ Garuda Linux ูุชุดุจู Zorin OS |
| 25 | ูุดุฑูุน ุชููู ุจุฑูุญ ุฅูุณุงููุฉ ูู ุญูุต |
| 26 | ุฏูุฌ ูููุงุช ูุดุฑูุน Garuda Linux ุนูู GitHub |
| 27 | ุฏููู ูุฌูู ูุฅุฏุงุฑุฉ Garuda Linux |
| 28 | ุฅุตูุงุญ ูุดุงูู Garuda Linux ูุญููููุง |
| 29 | ุงุณุชุฎุฑุงุฌ ุชุทุจููุงุช Zorin ุนูู Garuda |
| 30 | ุฌุนู Garuda ูุดุจู Zorin OS 18 Pro |
| 31 | ุชุซุจูุช Outline VPN ุนูู ููููุณ Garuda |
| 32 | ุชุญูู ูู Zorin OS ุฅูู Garuda Linux |

## Wine ู CrossOver ู Bottles

| # | ุงูููุถูุน |
|---|---------|
| 33 | ุงููุฑู ุจูู CrossOver ู Wine ู Bottles |
| 34 | Analyzing Wine Wrapper Binary File Content |
| 35 | ุชุซุจูุช CrossOver ุนูู Linux Garuda |
| 36 | ุฅุตูุงุญ ูุดุงูู ุงูุฎุทูุท ูู Wine |
| 37 | ุชุดุบูู ุณูุฑูุจุช ุงุณุชุฎุฑุงุฌ ูุต ูู Garuda |

## VPN ูุฃุฏูุงุช ุงูุดุจูุฉ

| # | ุงูููุถูุน |
|---|---------|
| 38 | Installing Outline Client on Arch Linux |
| 39 | ุชุซุจูุช ุบูุบู ูุฑูู ุนูู Garuda |
| 40 | ุญู ูุดููุฉ Outline VPN ูุง ูุบูุฑ ุงููููุน |
| 41 | Install Outline VPN on Zorin Linux |

## ุจุฑูุฌุฉ ูุชุทููุฑ

| # | ุงูููุถูุน |
|---|---------|
| 42 | ุชุซุจูุช ุจูุฆุฉ ุชุทููุฑ ูุงููุฉ ุนูู Linux |
| 43 | ุชุซุจูุช ุจูุฆุฉ ุชุทููุฑ ุจุนุฏ ุฅุตูุงุญ ุงูุฃุฎุทุงุก |
| 44 | ุชุซุจูุช ุงูุจุฑุงูุฌ ุงูููููุฏุฉ ูู Linux |
| 45 | ุชุญููู ุจุฑุงูุฌ ูุซุจุชุฉ ุนูู ูุธุงููู |
| 46 | ุชุดุบูู ูููุฏูุฒ ูู VM ุนูู ุงููุงุฑุฏ ุฏูุณู |
| 47 | ุชุซุจูุช ูุชูููู ุจุฑุงูุฌ ุนูู Arch Linux |
| 48 | ุฅุนุฏุงุฏุงุช ูุฅุถุงูุงุช WPS Office ุนูู Arch |
| 49 | ุฏุฎูู Linux Garuda ูู ูููุฏูุฒ |
| 50 | ุญู ูุดููุฉ ุงููุตูู ุฅูู ูุญุฑู ุงูุฃูุฑุงุต |

## ุณูุฑูุจุชุงุช ูุฃุชูุชุฉ

| # | ุงูููุถูุน |
|---|---------|
| 51 | ุชุฎุตูุต ุณูุฑูุจุช ูุชุญููู ุงููููุงุช ูุงููุญุงุฏุซุงุช |
| 52 | ุณูุฑูุจุช ูุนุงูุฌุฉ ูููุงุช ููุฌูุฏุงุช ุงููุฌูุฏ |
| 53 | ุชุทุจูู ุชูุณูู ูุญุณูู ูููู PowerShell |
| 54 | ุชุซุจูุช ูุฅุตูุงุญ Software Packages ุนูู Ubuntu |
| 55 | ุชุซุจูุช ุจุฑุงูุฌ ูุญู ูุดุงูู ูู Ubuntu |

## ุทุจูุฉ

| # | ุงูููุถูุน |
|---|---------|
| 56 | ุชุญููู ุงุณุชุซูุงุฑ ูุดุฑูุน ุฌูุงุฒ CT Scan |
| 57 | ุงุฎุชูุงุฑ ุถูุงุฏ ูุฌุฑูุญ ุงูุนุธู ุงูููุดููุฉ |
| 58 | ุทุจูุนุฉ ุงูุฑุนุงูุฉ ุงูุฃูุซู ูุฅุตุงุจุงุช ูุชุนุฏุฏุฉ ุงูุฃุนุฑุงุถ |
| 59 | ุฃุณุนุงุฑ ูุงูููุงุช ุงูุชุตููุฑ ุงูููุชุจุงุช |
| 60 | ุจุญุซ ุณุนุฑ ููุชุฌุงุช ุนุธููุฉ ููุฏูุฉ |

## ุฐูุงุก ุงุตุทูุงุนู

| # | ุงูููุถูุน |
|---|---------|
| 61 | ุฏูุฌ Mistral AI ูู ูุนุงูุฌุฉ ุงููุตูุต ุงูุนุฑุจูุฉ |
| 62 | ุฅุนุฏุงุฏ Ollama ูุงุณุชุฎุฏุงู API Key ููููุฐุฌ |
| 63 | ุฅุฏุงุฑุฉ ููุงุชูุญ API ุจุดูู ุขูู ูุณุฑูุน |
| 64 | ุงุณุชูุดุงู ูุดููุฉ API OpenRouter ูู Continue |
| 65 | ุชุทุจูู Continue ูุน ููุงุฐุฌ DeepSeek ุนูู PyCharm |
| 66 | ุญู ูุดููุฉ OpenRouter ูุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงููุญููุฉ |
| 67 | Troubleshooting API Key Authentication Issues |
| 68 | ุฅุถุงูุฉ ุงูุนุฑุจูุฉ ูุชุทุจูู DeepL ุจุณูููุฉ |
| 69 | ุจุฏุงุฆู ูุฌุงููุฉ ููุนุงูุฌุฉ ุงูููุฏ ูู PyCharm |
| 70 | ุชุญููู ููุงุฉ ููุชููุจ ุนุฑุจูุฉ ููุจุฑูุฌุฉ ุฅูู ูุต |
| 71 | ุฅุถุงูุงุช ูุฌุงููุฉ ูู VSCode ู PyCharm |
| 72 | ููุงุฐุฌ ุฐูุงุก ุงุตุทูุงุนู ูุฌุงููุฉ ุบูุฑ ูุญุฏูุฏุฉ |
| 73 | ููุงูุน ูุฌุงููุฉ ููููุชุงุฌ ููุฏูู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู |

## SmartTextETL Project

| # | ุงูููุถูุน |
|---|---------|
| 74 | SmartTextETL v8 ุชูุตูุงุช ููุฎุตุฉ ูุฃููููุงุช ุนุงุฌูุฉ |
| 75 | ุชุญุณููุงุช ูุฅุตูุงุญุงุช ูุดุฑูุน SmartTextETL v8 |
| 76 | SmartTextETL v6 JWT Authentication Guide |
| 77 | SmartTextETL v6 Project Overview and Features |
| 78 | SmartTextETL v6: Data Processing for AI Models |
| 79 | ่งฃๅณPythonๅ็ผ่ฏๅคฑ่ดฅ้ฎ้ข |
| 80 | ููุฎุต ูููู ูููุฒุงุช SmartTextETL |
| 81 | SmartTextETL v6.0 ุชุญุณูู ุงูุฃุฏุงุก ูุงูุชูุณุน |
| 82 | SmartTextETL v6 Text Intelligence Engine Overview |
| 83 | ุญู ูุดููุฉ PyTorch ูู Windows |
| 84 | Python Package Installation Path Issues Resolved |
| 85 | Fixing Python Syntax Error in run.py File |
| 86 | SmartTextETL v4.3 ุฅุตูุงุญ ุฎุทุฃ ููุดุงูู ุจุฑูุฌูุฉ |
| 87 | ุชุตุญูุญ ุฃุฎุทุงุก ูุชุญุณูู ุฃุฏุงุก SmartTextETL |
| 88 | ุชูุฒูู ุญุฒูุฉ SmartTextETL ูุงููุฉ ูุฌุงูุฒุฉ |
| 89 | Fixing StateManager.update_progress Parameter Conflict |
| 90 | Fix React App Dependency Issues Guide |
| 91 | Troubleshooting Failed PDF File Loading Issues |

## ุชุฑุฌูุฉ ูุชูุธูู

| # | ุงูููุถูุน |
|---|---------|
| 92 | ุชูุธูู ูุชูุซูู ุงููุตูุต ูุชููุนุฉ |
| 93 | ุฅุถุงูุฉ ุญูู ุงูููุงุญุธุงุช ูู ููู Excel |
| 94 | ุชูุธูู ูุชุตุญูุญ ูุงุฆูุฉ ุฌุฑุฏ ุงููุณุชูุฒูุงุช |
| 95 | ุชุตุฏูุฑ ูุงุณุชูุฑุงุฏ ุงูุตูุฑ ูุน ุฅูุณู |

## ูุชูุฑูุงุช

| # | ุงูููุถูุน |
|---|---------|
| 96 | ุชุนุฑูู ุชุทุจูู Discord ูุงุณุชุฎุฏุงูุงุชู ุงูุฑุฆูุณูุฉ |
| 97 | ุชุทููุฑ ุงูุฐูุงุก ุงูุงุฌุชูุงุนู ุนุจุฑ ุงูููุงุทุน ุงูุตูุชูุฉ |
| 98 | ุชูุฏูุฏุงุช ููุบุฉ ุชุญุฑูุถูุฉ ุชุฌุงู ุงูุณูุฑููู |
| 99 | ุณุจุจ ูููุงู ูุฃุณุงุฉ ุงูุฎุงุดูุฌู |
| 100 | ุทุฑู ุฌูุจ ุงูุจุฑุงูุฌ ูู ูููุฏูุฒ 11 |
| 101 | Installing and Failing Software Packages on Ubuntu |
| 102 | ุฅุตูุงุญ ูุดููุฉ Conda ูู Fish |

## ุงูุฅุญุตุงุฆูุงุช

| ุงูุชุตููู | ุงูุนุฏุฏ |
|---------|-------|
| ุชูููุฉ - Linux/Garuda | 32 |
| Wine/CrossOver | 5 |
| VPN/ุดุจูุงุช | 4 |
| ุจุฑูุฌุฉ ูุชุทููุฑ | 11 |
| ุณูุฑูุจุชุงุช | 5 |
| ุทุจูุฉ | 5 |
| ุฐูุงุก ุงุตุทูุงุนู | 13 |
| SmartTextETL | 18 |
| ุชุฑุฌูุฉ ูุชูุธูู | 4 |
| ูุชูุฑูุงุช | 7 |
| **ุงููุฌููุน** | **104** |

================================================================================

ุงุณู ุงูููู: Text_snippets-main/requirements.txt
----------------------------------------
# ========================================
# Text Snippets Knowledge Base - Requirements
# ========================================

# ----------------------------------------
# Core Dependencies
# ----------------------------------------

# Text Processing
python-docx>=0.8.11
openpyxl>=3.1.2
chardet>=5.2.0
tqdm>=4.66.1
beautifulsoup4>=4.10.0

# Archive handling
rarfile>=4.0

# ----------------------------------------
# Data Processing
# ----------------------------------------

# Data manipulation
numpy>=1.24.0
pandas>=2.0.0

# Deduplication
datasketch>=1.6.0

# ----------------------------------------
# Arabic Language Processing
# ----------------------------------------

language-tool-python>=2.7.0
qalsadi>=1.2.0

# ----------------------------------------
# Machine Learning (Optional)
# ----------------------------------------

# Classification & Training
scikit-learn>=1.3.0

# ----------------------------------------
# AI Integration (Optional)
# ----------------------------------------

# HTTP requests (for Ollama API)
requests>=2.28.0

# Uncomment if using AI features
# openai>=1.0.0
# anthropic>=0.18.0

# ----------------------------------------
# Web Interface (Optional)
# ----------------------------------------

# Streamlit for review app
streamlit>=1.30.0

# ----------------------------------------
# HuggingFace Export (Optional)
# ----------------------------------------

# For exporting to HuggingFace
datasets>=2.16.0
huggingface-hub>=0.20.0

# ----------------------------------------
# Development Tools (Optional)
# ----------------------------------------

# Testing
pytest>=7.4.0
pytest-cov>=4.1.0

# Code Quality
black>=23.0.0
flake8>=6.1.0
mypy>=1.5.0

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/CORPUS_PROCESSOR_GUIDE.md
----------------------------------------
# ุฏููู ุงุณุชุฎุฏุงู Corpus Processor Offline
# ูุนุงูุฌ ุงููููุงุช ุงููุตูุฉ ุงูุฃูููุงูู

## ูุธุฑุฉ ุนุงูุฉ

ุณูุฑูุจุช ุจุงูุซูู ุดุงูู ููุนุงูุฌุฉ ูููุงุช ูุจูุฑุฉ ูู ุงููููุงุช ุงููุตูุฉ ุฃูููุงููุ ูุตูู ููุชุนุงูู ูุน ุฃูุซุฑ ูู 5 ุบูุบุงุจุงูุช ูู ุงูุจูุงูุงุช ุจููุงุกุฉ.

## ุงููููุฒุงุช ุงูุฑุฆูุณูุฉ

### 1. ูุนุงูุฌุฉ ูุชุนุฏุฏุฉ ุงูุตูุบ
- **ูููุงุช ูุตูุฉ**: txt, md, py, js, json, csv, xml, sql, ูุบูุฑูุง
- **ูููุงุช Excel**: xls, xlsx (ูุน ุงุณุชุฎุฑุงุฌ ุงููุญุชูู ูุงูุชุฑุฌูุงุช)
- **ูููุงุช Word**: docx (ููุฑุงุช ูุฌุฏุงูู)
- **ูููุงุช HTML**: ุงุณุชุฎุฑุงุฌ ุงููุต ุงููุฑุฆู ููุท
- **ููุงุนุฏ ุงูุจูุงูุงุช**: SQLite (db, sqlite, sqlite3)
- **ุฃุฑุดููุงุช**: ZIP, RAR, TAR

### 2. ูุนุงูุฌุฉ ุฐููุฉ
- ุชุตููู ุชููุงุฆู ูููุญุชูู (ุทุจูุ ุชูููุ ุชุฑุฌูุฉุ ุฐูุงุก ุงุตุทูุงุนูุ ุจุฑูุฌุฉุ ูุชูุฑูุงุช)
- ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ
- ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ุซูุงุฆูุฉ ุงููุบุฉ (ุฅูุฌููุฒู-ุนุฑุจู)
- ุงุณุชุฎุฑุงุฌ ุงูุนูุงููู ูู ุงููุญุชูู

### 3. ุฅุฏุงุฑุฉ ุงููููุงุช ุงููุจูุฑุฉ
- ูุนุงูุฌุฉ ุนูู ุฏูุนุงุช (batch processing)
- ุญูุธ ุงูุชูุฏู ูุงุณุชุฆูุงู ุงููุนุงูุฌุฉ
- ูุงุฌูุฉ ุณุทุฑ ุฃูุงูุฑ ุณููุฉ

## ุงูุชุซุจูุช

### ุงููุชุทูุจุงุช ุงูุฃุณุงุณูุฉ
```bash
# ุงูููุชุจุงุช ุงูุฃุณุงุณูุฉ (ูุซุจุชุฉ ูุน ุจุงูุซูู)
# - os, sys, re, csv, json, zipfile, tarfile, sqlite3
# - pathlib, argparse, datetime, hashlib, pickle

# ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ (ููููุฒุงุช ุงูุฅุถุงููุฉ)
pip install pandas openpyxl      # ููุนุงูุฌุฉ Excel
pip install python-docx           # ููุนุงูุฌุฉ Word
pip install beautifulsoup4        # ููุนุงูุฌุฉ HTML
pip install rarfile               # ููุนุงูุฌุฉ RAR
```

## ุงูุงุณุชุฎุฏุงู

### ูุนุงูุฌุฉ ูุฌูุฏ ูุงูู
```bash
python corpus_processor_offline.py -i ./my_files -o ./processed
```

### ุงุณุชุฆูุงู ูุนุงูุฌุฉ ูุชูููุฉ
```bash
python corpus_processor_offline.py -i ./my_files -o ./processed --resume
```

### ุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุฉ
```bash
python corpus_processor_offline.py -i ./my_files -o ./processed --batch-size 100
```

### ูุนุงูุฌุฉ ููู ูุงุญุฏ
```bash
python corpus_processor_offline.py -i document.xlsx -o ./output
```

## ุฎูุงุฑุงุช ุณุทุฑ ุงูุฃูุงูุฑ

| ุงูุฎูุงุฑ | ุงููุตู | ุงูุงูุชุฑุงุถู |
|--------|-------|-----------|
| `-i, --input` | ูุณุงุฑ ุงูููู ุฃู ุงููุฌูุฏ ุงููุฏุฎู | (ูุทููุจ) |
| `-o, --output` | ูุณุงุฑ ุงูุฅุฎุฑุงุฌ | `./corpus_processed` |
| `-b, --batch-size` | ุนุฏุฏ ุงููููุงุช ูู ูู ุฏูุนุฉ | 50 |
| `-r, --resume` | ุงุณุชุฆูุงู ูุนุงูุฌุฉ ุณุงุจูุฉ | False |

## ูููู ุงูุฅุฎุฑุงุฌ

```
corpus_processed/
โโโ technical/
โ   โโโ linux/         # ูููุงุช ููููุณ
โ   โโโ windows/       # ูููุงุช ูููุฏูุฒ
โ   โโโ wine/          # Wine/CrossOver/Bottles
โ   โโโ vpn/           # VPN ูุฃุฏูุงุช ุงูุดุจูุฉ
โโโ medical/
โ   โโโ orthopedics/   # ูููุงุช ุงูุนุธุงู
โ   โโโ imaging/       # ูููุงุช ุงูุชุตููุฑ ุงูุทุจู
โโโ translation/       # ูููุงุช ุงูุชุฑุฌูุฉ
โโโ ai/               # ูููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู
โโโ programming/       # ูููุงุช ุงูุจุฑูุฌุฉ
โโโ misc/             # ูููุงุช ูุชูุฑูุฉ
โโโ extracted/        # ูุญุชูู ุงูุฃุฑุดููุงุช ุงููุณุชุฎุฑุฌ
โโโ scripts/          # ุงูุณูุฑูุจุชุงุช
โโโ translations.csv  # ุงูุชุฑุฌูุงุช ุงููุฌูุนุฉ
โโโ file_index.json   # ููุฑุณ ุงููููุงุช
โโโ PROCESSING_REPORT.md  # ุชูุฑูุฑ ุงููุนุงูุฌุฉ
โโโ .progress.pkl     # ููู ุงูุชูุฏู (ููุงุณุชุฆูุงู)
```

## ููู ุงูุชุฑุฌูุงุช (translations.csv)

| ุงูุนููุฏ | ุงููุตู |
|--------|-------|
| English | ุงููุต ุงูุฅูุฌููุฒู |
| Arabic (Original) | ุงููุต ุงูุนุฑุจู ุงูุฃุตูู |
| Arabic (Corrected) | ุงููุต ุงูุนุฑุจู ุจุนุฏ ุงูุชุตุญูุญ |
| Needs Correction | ูู ูุงู ููุงู ุชุตุญูุญุ (Yes/No) |

## ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงููุฏุนููุฉ

### ุงูููุฒุงุช
- ุงุฆูุง โ ุฅูุง
- ุฃูุดุงุก/ุงูุดุงุก โ ุฅูุดุงุก
- ุงูุฃู/ุงูุงู โ ุงูุขู

### ุงูุฃูู ุงููููุฉ
- ุงูู/ุงูู โ ุฅูู
- ุนูู โ ุนูู
- ุญุชู โ ุญุชู

### ูููุงุช ุดุงุฆุนุฉ
- ุงูุถุง/ุงูุธุง/ุงูุถุข โ ุฃูุถุงู
- ุฌุฏุง/ุฌุฏุข โ ุฌุฏุงู
- ุฐุงูู โ ุฐูู
- ุงูุฐู โ ุงูุฐู
- ุงูุชู โ ุงูุชู

## ูุซุงู ุนูู ุงูุชุดุบูู

```
๐ฆ ุญุงูุฉ ุงูููุชุจุงุช:
  โ pandas (Excel) - ูุซุจุช
  โ python-docx (Word) - ูุซุจุช
  โ beautifulsoup4 (HTML) - ูุซุจุช
  โ๏ธ rarfile (RAR) - pip install rarfile

======================================================================
๐ ุจุฏุก ูุนุงูุฌุฉ ุงููููุงุช
======================================================================
๐ ุฅุฌูุงูู ุงููููุงุช: 1250
๐ฆ ุญุฌู ุงูุฏูุนุฉ: 50
๐ ุงุณุชุฆูุงู: ูุง

[ุฏูุนุฉ 1] ุงููููุงุช 1-50 ูู 1250
--------------------------------------------------
๐ garuda-setup.md... โ ๐ป ุชููู
๐ medical-report.txt... โ ๐ฅ ุทุจู
๐ translations.xlsx... โ ๐ ุชุฑุฌูุฉ
...

๐ ุงูุชูุฏู: 4.0% | ูุนุงูุฌ: 50 | ุฃุฎุทุงุก: 0
```

## ูุตุงุฆุญ ููุฃุฏุงุก

### ูููููุงุช ุงููุจูุฑุฉ ุฌุฏุงู (10+ ุบูุบุงุจุงูุช)
1. ุงุณุชุฎุฏู ุญุฌู ุฏูุนุฉ ุฃุตุบุฑ (25-50 ููู)
2. ุดุบูู ุงููุนุงูุฌุฉ ุนูู SSD
3. ุงุณุชุฎุฏู `--resume` ูู ุญุงู ุชููู ุงููุนุงูุฌุฉ

### ูููููุงุช ุงูุตุบูุฑุฉ
1. ูููู ุงุณุชุฎุฏุงู ุญุฌู ุฏูุนุฉ ุฃูุจุฑ (100-200)
2. ุงููุนุงูุฌุฉ ุณุชููู ุณุฑูุนุฉ

## ุงุณุชูุดุงู ุงูุฃุฎุทุงุก

### ุฎุทุฃ: "pandas not found"
```bash
pip install pandas openpyxl
```

### ุฎุทุฃ: "python-docx not found"
```bash
pip install python-docx
```

### ุฎุทุฃ: "Permission denied"
- ุชุฃูุฏ ูู ุตูุงุญูุงุช ุงููุชุงุจุฉ ูู ูุฌูุฏ ุงูุฅุฎุฑุงุฌ
- ุฃุบูู ุฃู ุจุฑุงูุฌ ุชุณุชุฎุฏู ุงููููุงุช

### ุงููุนุงูุฌุฉ ูุชูููุฉ
- ุงุณุชุฎุฏู `--resume` ูุงุณุชุฆูุงู ุงููุนุงูุฌุฉ
- ุชุญูู ูู ูุฌูุฏ ููู `.progress.pkl`

## ุงูุชุฑุฎูุต

ูุฐุง ุงูุณูุฑูุจุช ูุฌุงูู ููุงุณุชุฎุฏุงู ุงูุดุฎุตู ูุงูุชุฌุงุฑู.

## ุงููุทูุฑ

ุชู ุชุทููุฑ ูุฐุง ุงูุณูุฑูุจุช ูุฌุฒุก ูู ูุดุฑูุน ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุนุฑุจูุฉ.

---

**ุขุฎุฑ ุชุญุฏูุซ:** ูุจุฑุงูุฑ 2026

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/DEDUP_AI_GUIDE.md
----------------------------------------
# ุฏููู Corpus Dedup & AI Processor
# ูุนุงูุฌ ุงููููุงุช ูุน ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุฐูุงุก ุงูุงุตุทูุงุนู

## ูุธุฑุฉ ุนุงูุฉ

ุณูุฑูุจุช ูุชูุฏู ููุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ ูุน ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนูุ ูุตูู ููุชุนุงูู ูุน ุงููููุงุช ุงููุจูุฑุฉ ูู ุงูุจูุงูุงุช.

## ุงููููุฒุงุช

### 1. ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ุงูุฐููุฉ
- **ูุดู ุงูุชุทุงุจู ุงูุชุงู**: hash ูุชุทุงุจู 100%
- **ูุดู ุงูุชุดุงุจู**: ุชุดุงุจู ูุณุจุฉ 85%+ (ูุงุจู ููุชุนุฏูู)
- **ุชูููุงุช ูุชุนุฏุฏุฉ**: Jaccard similarity + Sequence matching
- **ุชุทุจูุน ุงููุต**: ุชุฌุงูู ุงููุฑููุงุช ุงูุจุณูุทุฉ

### 2. ุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู
- **ุชูุฎูุต ุงููุญุชูู**: ุชูุฎูุต ุชููุงุฆู ููู ููู
- **ุชุตููู ุฐูู**: ุทุจู/ุชููู/ุชุฑุฌูุฉ/ุฐูุงุก ุงุตุทูุงุนู/ุจุฑูุฌุฉ/ูุชูุฑูุงุช
- **ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ**: ุชููุงุฆูุงู
- **ุชูููู ุงูุฌูุฏุฉ**: ุนุงููุฉ/ูุชูุณุทุฉ/ููุฎูุถุฉ

### 3. ุฏุนู APIs ูุฌุงููุฉ ูุชุนุฏุฏุฉ

| API | ุงูุชูููุฉ | ุงููุชุทูุจุงุช | ุงูุณุฑุนุฉ |
|-----|---------|-----------|--------|
| **Ollama** | ูุฌุงูู 100% | ุชุซุจูุช ูุญูู | ุณุฑูุน |
| **OpenRouter** | ููุงุฐุฌ ูุฌุงููุฉ | API key | ูุชูุณุท |
| **DeepSeek** | ุฑุฎูุต ุฌุฏุงู | API key | ุณุฑูุน |

## ุงูุชุซุจูุช

```bash
# ุงููุชุทูุจุงุช ุงูุฃุณุงุณูุฉ
pip install requests

# ุงุฎุชูุงุฑู - ูุฃุฏุงุก ุฃูุถู
pip install numpy scikit-learn
```

### ุชุซุจูุช Ollama (ูุฌุงูู ุชูุงูุงู)

```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# ุชุญููู ูููุฐุฌ
ollama pull llama3.2

# ุชุดุบูู ุงูุฎุงุฏู
ollama serve
```

## ุงูุงุณุชุฎุฏุงู

### 1. ุจุฏูู ุฐูุงุก ุงุตุทูุงุนู (ููุท ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช)

```bash
python corpus_dedup_ai.py -i ./files -o ./processed --no-ai
```

### 2. ุงุณุชุฎุฏุงู Ollama (ูุฌุงูู ูุญููุงู)

```bash
# ุชุดุบูู Ollama ุฃููุงู
ollama serve

# ูู ูุงูุฐุฉ ุฃุฎุฑู
python corpus_dedup_ai.py -i ./files -o ./processed --api ollama
```

### 3. ุงุณุชุฎุฏุงู OpenRouter (ููุงุฐุฌ ูุฌุงููุฉ)

```bash
# ุงูุญุตูู ุนูู API key ูุฌุงูู ูู openrouter.ai
python corpus_dedup_ai.py -i ./files -o ./processed \
    --api openrouter \
    --api-key sk-or-v1-xxxxx
```

### 4. ุงุณุชุฎุฏุงู DeepSeek (ุฑุฎูุต ุฌุฏุงู)

```bash
# ุงูุญุตูู ุนูู API key ูู platform.deepseek.com
python corpus_dedup_ai.py -i ./files -o ./processed \
    --api deepseek \
    --api-key sk-xxxxx
```

### 5. ุชุญุฏูุฏ ุนุชุจุฉ ุงูุชุดุงุจู

```bash
# ุนุชุจุฉ ุนุงููุฉ (90%) - ูุญุชูุธ ุจูููุงุช ุฃูุซุฑ ุชุดุงุจูุงู
python corpus_dedup_ai.py -i ./files -o ./processed --threshold 0.9

# ุนุชุจุฉ ููุฎูุถุฉ (70%) - ูุญุฐู ูููุงุช ุฃูู ุชุดุงุจูุงู
python corpus_dedup_ai.py -i ./files -o ./processed --threshold 0.7
```

## ุฎูุงุฑุงุช ุณุทุฑ ุงูุฃูุงูุฑ

| ุงูุฎูุงุฑ | ุงููุตู | ุงูุงูุชุฑุงุถู |
|--------|-------|-----------|
| `-i, --input` | ูุณุงุฑ ุงููููุงุช | (ูุทููุจ) |
| `-o, --output` | ูุณุงุฑ ุงูุฅุฎุฑุงุฌ | `./dedup_output` |
| `-t, --threshold` | ุนุชุจุฉ ุงูุชุดุงุจู (0-1) | 0.85 |
| `--api` | ููุน API | None |
| `--api-key` | ููุชุงุญ API | None |
| `--model` | ุงุณู ุงููููุฐุฌ | ุชููุงุฆู |
| `--no-ai` | ุชุนุทูู AI | False |

## ูููู ุงูุฅุฎุฑุงุฌ

```
dedup_output/
โโโ unique_files/           # ุงููููุงุช ุงููุฑูุฏุฉ
โโโ duplicates_report.json  # ุชูุฑูุฑ ุงูุชูุฑุงุฑุงุช
โโโ ai_analysis.json        # ุชุญูููุงุช AI
โโโ summaries.md            # ููุฎุตุงุช ุงููููุงุช
โโโ DEDUP_REPORT.md         # ุงูุชูุฑูุฑ ุงูููุงุฆู
```

## ุชูุฑูุฑ ุงูุชูุฑุงุฑุงุช (duplicates_report.json)

```json
{
  "duplicates": [
    {
      "original": "file1.txt",
      "duplicate": "file2.txt",
      "similarity": 95.5
    }
  ],
  "statistics": {
    "total_files_checked": 100,
    "unique_files": 75,
    "duplicates_found": 25,
    "exact_duplicates": 15,
    "near_duplicates": 10
  }
}
```

## ุชุญููู AI (ai_analysis.json)

```json
[
  {
    "file": "medical_report.txt",
    "size": 5432,
    "hash": "abc123...",
    "summary": "ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...",
    "classification": {
      "category": "ุทุจู",
      "topics": ["ุนุธุงู", "ุฌุฑุงุญุฉ"],
      "language": "ุงูุนุฑุจูุฉ",
      "quality": "ุนุงููุฉ"
    },
    "keywords": ["ุนุธุงู", "ุฌุฑุงุญุฉ", "ุนูุงุฌ", "ูุฑูุถ"]
  }
]
```

## ุฃูุซูุฉ ุนูููุฉ

### ูุซุงู 1: ุชูุธูู ูุงุนุฏุฉ ูุนุฑูุฉ

```bash
# ูุฏูู 1000 ููู ูุตูุ ุจุนุถูุง ููุฑุฑ
python corpus_dedup_ai.py -i ./knowledge_base -o ./clean_base --api ollama

# ุงููุชูุฌุฉ: ููุท ุงููููุงุช ุงููุฑูุฏุฉ ูุน ุชูุฎูุตุงุช
```

### ูุซุงู 2: ุฏูุฌ ูููุงุช ูู ูุตุงุฏุฑ ูุชุนุฏุฏุฉ

```bash
# ุฏูุฌ ูููุงุช ูู ุนุฏุฉ ูุฌูุฏุงุช
python corpus_dedup_ai.py -i ./all_sources -o ./merged --no-ai

# ุซู ุชุญููู ุงููุฑูุฏุฉ
python corpus_dedup_ai.py -i ./merged/unique_files -o ./analyzed --api ollama
```

### ูุซุงู 3: ูุนุงูุฌุฉ ูููุงุช ูุจูุฑุฉ

```bash
# ุนุชุจุฉ ุนุงููุฉ ูููููุงุช ุงููุจูุฑุฉ
python corpus_dedup_ai.py -i ./large_corpus -o ./processed \
    --threshold 0.95 \
    --api deepseek \
    --api-key YOUR_KEY
```

## ูุตุงุฆุญ

### ูุชุญุณูู ุงูุฃุฏุงุก

1. **ุงุณุชุฎุฏู Ollama ูุญููุงู** - ุฃุณุฑุน ูุฃูุงู
2. **ูุณู ุงููููุงุช ุงููุจูุฑุฉ** - ูุนุงูุฌุฉ ุนูู ูุฑุงุญู
3. **ุงุถุจุท ุงูุนุชุจุฉ** - 0.85 ููุงุณุจ ูููุตูุต ุงูุนุฑุจูุฉ

### ููุญุตูู ุนูู ุฃูุถู ุงููุชุงุฆุฌ

1. **ุชุฃูุฏ ูู ุชุฑููุฒ ุงููููุงุช** - UTF-8 ููุถู
2. **ุฃุฒู ุงููููุงุช ุบูุฑ ุงููุตูุฉ** - ูุจู ุงููุนุงูุฌุฉ
3. **ุฑุงุฌุน ุงูุชูุฑูุฑ** - ุชุญูู ูู ุงูุชูุฑุงุฑุงุช ุงููุญุฐููุฉ

## ุงุณุชูุดุงู ุงูุฃุฎุทุงุก

### Ollama ูุง ูุนูู
```bash
# ุชุญูู ูู ุชุดุบูู ุงูุฎุงุฏู
curl http://localhost:11434/api/tags

# ุฃุนุฏ ุชุดุบูู
ollama serve
```

### API key ุบูุฑ ุตุงูุญ
```bash
# ุชุฃูุฏ ูู ุตุญุฉ ุงูููุชุงุญ
# OpenRouter: https://openrouter.ai/keys
# DeepSeek: https://platform.deepseek.com/api_keys
```

### ูููุงุช ุบูุฑ ููุฑูุกุฉ
- ุชุญูู ูู ุชุฑููุฒ ุงูููู
- ุชุฃูุฏ ูู ุฃู ุงูููู ูุตู
- ุฌุฑุจ ุชุญููู ุงูุชุฑููุฒ

## ููุงุฑูุฉ APIs

| ุงูููุฒุฉ | Ollama | OpenRouter | DeepSeek |
|--------|--------|------------|----------|
| ุงูุชูููุฉ | ูุฌุงูู | ูุฌุงูู/ูุฏููุน | ุฑุฎูุต |
| ุงูุฎุตูุตูุฉ | 100% | ุนูู ุงูุณุญุงุจุฉ | ุนูู ุงูุณุญุงุจุฉ |
| ุงูุณุฑุนุฉ | ุณุฑูุน | ูุชูุณุท | ุณุฑูุน |
| ุงูุฌูุฏุฉ | ุฌูุฏ | ููุชุงุฒ | ููุชุงุฒ |
| ุงูุชุซุจูุช | ูุทููุจ | ุจุฏูู ุชุซุจูุช | ุจุฏูู ุชุซุจูุช |

---

**ุขุฎุฑ ุชุญุฏูุซ:** ูุจุฑุงูุฑ 2026

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/INTELLIGENT_PROCESSOR_GUIDE.md
----------------------------------------
# ๐ง ูุนุงูุฌ ุงููุตูุต ุงูุฐูู ุงูุดุงูู

## ูุธุฑุฉ ุนุงูุฉ

ุณูุฑูุจุช ูุชูุงูู ููุนุงูุฌุฉ ูููุงุช ูุตูุฉ ุถุฎูุฉ (30+ GB) ุฃูููุงูู ูุน ููุฒุงุช:

- โ **ูุงุฌูุฉ ุฑุณูููุฉ** ุณููุฉ (tkinter)
- โ **ูุธุงู logging ูุชุทูุฑ** ูุน ุฃููุงู
- โ **ุดุฑูุท ุชูุฏู ุชูุงุนูู**
- โ **ุชูุงูู ูุน Ollama** ููุชุตููู ูุงูุชุฑุฌูุฉ ุงูุฐููุฉ
- โ **ุฏุนู ุงูููุงุฐุฌ ุงููุญููุฉ** (qwen, llama, mistral...)
- โ **ุชูุณูู ุฐูู** ูููููุงุช ุงููุจูุฑุฉ
- โ **ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช** (ุฅูุฌููุฒู-ุนุฑุจู)
- โ **ุชุตููู ุงููุญุชูู** (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ูุฑุงุฌุน)
- โ **ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช** (MinHash LSH)
- โ **ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ** ุงูุนุฑุจูุฉ
- โ **ุชูููู ุงูุฌูุฏุฉ** ุงูุชููุงุฆู

---

## ๐ ุงูุชุซุจูุช

```bash
# ุงููุชุทูุจุงุช ุงูุฃุณุงุณูุฉ
pip install datasketch tqdm chardet

# ุงุฎุชูุงุฑู - ููุชุตููู ุงููุชูุฏู
pip install scikit-learn numpy
```

---

## ๐ ุงูุงุณุชุฎุฏุงู

### ุงููุนุงูุฌุฉ ุงููุงููุฉ

```bash
# ูุนุงูุฌุฉ ูุฌูุฏ ูุงูู
python intelligent_text_processor.py --input ./data --output ./processed

# ุงุณุชููุงู ูุนุงูุฌุฉ ูุชูููุฉ
python intelligent_text_processor.py --input ./data --output ./processed --resume

# ุนุฑุถ ุญุงูุฉ ุงููุนุงูุฌุฉ
python intelligent_text_processor.py --status ./processed
```

### ุฃูุถุงุน ุงููุนุงูุฌุฉ

| ุงููุถุน | ุงููุตู |
|-------|-------|
| `all` | ูุนุงูุฌุฉ ูุงููุฉ (ุงูุชุฑุงุถู) |
| `classify` | ุชุตููู ููุท |
| `translate` | ุงุณุชุฎุฑุงุฌ ุชุฑุฌูุงุช ููุท |
| `dedup` | ุฅุฒุงูุฉ ุชูุฑุงุฑุงุช ููุท |

---

## ๐ ุงูุชุตูููุงุช

| ุงููุฆุฉ | ุงููุตู | ุงููููุงุช ุงูููุชุงุญูุฉ |
|-------|-------|-------------------|
| ๐ฅ **medical** | ุทุจู | ุนุธุงูุ ุฌุฑุงุญุฉุ ูุณุชุดููุ ุนูุงุฌ... |
| ๐ป **technical** | ุชููู | ููููุณุ ุจุฑูุฌุฉุ ููุฏุ ูุธุงู... |
| ๐ **translation** | ุชุฑุฌูุฉ | ูุบุฉุ ูุงููุณุ ูุตุทูุญุ ูุนูู... |
| ๐ **reference** | ูุฑุงุฌุน | ููุฑุณุ ุฏูููุ ูุงุฆูุฉุ ุชูุฑูุฑ... |
| ๐ง **code** | ููุฏ | ุฏุงูุฉุ ูุชุบูุฑุ ููุงุณุ ุญููุฉ... |
| ๐ **misc** | ูุชูุฑูุงุช | ูุญุชูู ุนุงู |

---

## ๐ ูููู ุงูุฅุฎุฑุงุฌ

```
output/
โโโ segments/              # ุงูููุงุทุน ุงููุตููุฉ
โ   โโโ medical/          # ๐ฅ ุทุจู
โ   โโโ technical/        # ๐ป ุชููู
โ   โโโ translation/      # ๐ ุชุฑุฌูุฉ
โ   โโโ reference/        # ๐ ูุฑุงุฌุน
โ   โโโ code/             # ๐ง ููุฏ
โ   โโโ misc/             # ๐ ูุชูุฑูุงุช
โ
โโโ translations/          # ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ
โ   โโโ translations.csv  # CSV
โ   โโโ translations.tsv  # ููุชุฏุฑูุจ
โ   โโโ translations.jsonl # ูู HuggingFace
โ
โโโ reports/
โ   โโโ PROCESSING_REPORT.md
โ
โโโ progress.pkl          # ููู ุงูุชูุฏู
โโโ stats.json           # ุงูุฅุญุตุงุฆูุงุช
```

---

## โก ููุฒุงุช ูุชูุฏูุฉ

### 1. ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ุงูุฐููุฉ

ูุณุชุฎุฏู **MinHash LSH** ูููุดู ุนู ุงูุชูุฑุงุฑุงุช ุงููุชุดุงุจูุฉ (ูููุณ ุงููุชุทุงุจูุฉ ููุท):

```python
# ุนุชุจุฉ ุงูุชุดุงุจู (0.85 = 85%)
SIMILARITY_THRESHOLD = 0.85
```

### 2. ุชุตุญูุญ ุฅููุงุฆู ุชููุงุฆู

ููุดู ููุตุญุญ ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ:

```
ุงูุดุงุก โ ุฅูุดุงุก
ุงูุงู โ ุงูุขู
ุงูู โ ุฅูู
ุงูุถุงู โ ุฃูุถุงู
ุฐุงูู โ ุฐูู
```

### 3. ุชูููู ุงูุฌูุฏุฉ

ูุญุณุจ ููุงุท ุฌูุฏุฉ ููู ููุทุน:

- ุทูู ุงููุต
- ูุณุจุฉ ุงูุฑููุฒ ุบูุฑ ุงููููุฏุฉ
- ูุฌูุฏ garbage characters
- ูุณุจุฉ ุงููููุงุช ุงููุชูุฑุฑุฉ

### 4. ุญูุธ ุงูุชูุฏู

- ุญูุธ ุชููุงุฆู ูู 100 ููู
- ุงุณุชููุงู ูู ุญูุซ ุชููู
- ุญูุงูุฉ ูู ููุฏุงู ุงูุจูุงูุงุช

---

## ๐ ุงูุฃุฏุงุก

| ุญุฌู ุงูุจูุงูุงุช | ุงููููุงุช | ุงูููุช ุงูุชูุฑูุจู | ุงูุฐุงูุฑุฉ |
|--------------|---------|----------------|---------|
| 1 GB | ~1000 | 10-15 ุฏูููุฉ | ~500 MB |
| 10 GB | ~10000 | 1-2 ุณุงุนุฉ | ~1 GB |
| 30 GB | ~30000 | 3-5 ุณุงุนุงุช | ~2 GB |

---

## ๐ง ุงูุชุฎุตูุต

### ุชุนุฏูู ุญุฌู ุงูููุงุทุน

```python
MAX_SEGMENT_SIZE = 50 * 1024  # 50 KB
MIN_SEGMENT_SIZE = 500        # 500 bytes
```

### ุฅุถุงูุฉ ูููุงุช ููุชุงุญูุฉ

```python
CATEGORY_KEYWORDS['medical']['ar'].extend([
    'ูููุฉ_ุฌุฏูุฏุฉ1',
    'ูููุฉ_ุฌุฏูุฏุฉ2'
])
```

### ุชุนุฏูู ุนุชุจุฉ ุงูุชูุฑุงุฑ

```python
SIMILARITY_THRESHOLD = 0.90  # ุฃูุซุฑ ุตุฑุงูุฉ
```

---

## โ๏ธ ููุงุญุธุงุช ูููุฉ

1. **ุงูุฐุงูุฑุฉ**: ูููููุงุช ุงูุถุฎูุฉุ ุชุฃูุฏ ูู ุชููุฑ 2+ GB RAM
2. **ุงููุฑุต**: ุชุฃูุฏ ูู ูุณุงุญุฉ ูุงููุฉ ููุฅุฎุฑุงุฌ
3. **ุงูุงุณุชููุงู**: ุงุณุชุฎุฏู `--resume` ูุงุณุชููุงู ูุนุงูุฌุฉ ูุชูููุฉ
4. **ุงููุณุฎ ุงูุงุญุชูุงุทู**: ุงุญุชูุธ ุจูุณุฎุฉ ูู ุงูุจูุงูุงุช ุงูุฃุตููุฉ

---

## ๐ ุงููุณุงุนุฏุฉ

```bash
python intelligent_text_processor.py --help
```

---

## ๐ ุงูุฑุฎุตุฉ

MIT License - ูููู ุงูุงุณุชุฎุฏุงู ุงูุญุฑ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/KNOWLEDGE_PROCESSOR_GUIDE.md
----------------------------------------
# ุฏููู ุงุณุชุฎุฏุงู Knowledge Processor

## ๐ฏ ูุธุฑุฉ ุนุงูุฉ

ุณูุฑูุจุช ุจุงูุซูู ุดุงูู ููุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ ุฃูููุงูู. ูููู ุจู:

1. **ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูุฃุฑุดููุงุช** (ZIP, RAR, TAR)
2. **ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงููุฌูุฏุงุช** ุฅูู ูููุงุช ูุตูุฉ
3. **ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช** ุซูุงุฆูุฉ ุงููุบุฉ ูู ูุฎุชูู ุงููููุงุช
4. **ุชุตููู ุงููุญุชูู** ุชููุงุฆูุงู (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ูุฑุงุฌุน)
5. **ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ** ุงูุนุฑุจูุฉ
6. **ุฌูุน ุงูุชุฑุฌูุงุช** ูู ููู CSV ููุญุฏ

---

## ๐ฆ ุงููุชุทูุจุงุช

### ุงูููุชุจุงุช ุงูุฃุณุงุณูุฉ (ูุถููุฉ ูู ุจุงูุซูู):
```bash
# ูุง ุญุงุฌุฉ ููุชุซุจูุช
- os, sys, re, csv, json, zipfile, tarfile, pathlib, argparse, datetime
```

### ููุชุจุงุช ุงุฎุชูุงุฑูุฉ:
```bash
# ููุฑุงุกุฉ ูููุงุช Excel
pip install openpyxl pandas

# ูุฏุนู ูููุงุช RAR
pip install rarfile
```

---

## ๐ ุทุฑููุฉ ุงูุงุณุชุฎุฏุงู

### 1. ูุนุงูุฌุฉ ููู ูุงุญุฏ:
```bash
python knowledge_processor.py -i document.xlsx -o ./output
python knowledge_processor.py -i archive.zip -o ./knowledge_base
```

### 2. ูุนุงูุฌุฉ ูุฌูุฏ ูุงูู:
```bash
python knowledge_processor.py -i ./my_files -o ./processed
```

### 3. ูู ุฏุงุฎู ุจุงูุซูู:
```python
from knowledge_processor import KnowledgeProcessor

processor = KnowledgeProcessor('./input_folder', './output_folder')
processor.process_all()
```

---

## ๐ ูููู ุงูุฅุฎุฑุงุฌ

```
output_folder/
โโโ technical/
โ   โโโ linux/          # ูููุงุช ููููุณ
โ   โโโ windows/        # ูููุงุช ูููุฏูุฒ
โโโ medical/
โ   โโโ orthopedics/    # ูููุงุช ุทุจูุฉ
โโโ translation/
โ   โโโ translations_collected.csv  # ุงูุชุฑุฌูุงุช ุงููุฌูุนู
โโโ extracted/
โ   โโโ *_extracted.txt # ูุญุชูู ุงูุฃุฑุดููุงุช ูุงููุฌูุฏุงุช
โโโ digests/            # ุงููููุงุช ุงููุฑุฌุนูุฉ
โโโ scripts/            # ุงูุณูุฑูุจุชุงุช
โโโ PROCESSING_REPORT.md  # ุชูุฑูุฑ ุงููุนุงูุฌุฉ
```

---

## ๐ ููู ุงูุชุฑุฌูุงุช (CSV)

| ุงูุนููุฏ | ุงููุตู |
|--------|-------|
| English | ุงููุต ุงูุฅูุฌููุฒู |
| Arabic (Original) | ุงููุต ุงูุนุฑุจู ุงูุฃุตูู |
| Arabic (Corrected) | ุงููุต ุงูุนุฑุจู ุจุนุฏ ุงูุชุตุญูุญ |
| Needs Correction | ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ |
| Source | ูุตุฏุฑ ุงูุชุฑุฌูุฉ |

---

## ๐ง ุงูุชุตููู ุงูุชููุงุฆู

ูุชู ุชุตููู ุงููููุงุช ุจูุงุกู ุนูู ุงููููุงุช ุงูููุชุงุญูุฉ:

| ุงูุชุตููู | ุฃููููุฉ | ูููุงุช ููุชุงุญูุฉ |
|---------|--------|---------------|
| ุทุจู | ๐ฅ | ุทุจูุ ูุฑุถุ ุนูุงุฌุ surgery, medical... |
| ุชููู | ๐ป | ุจุฑูุฌุฉุ ููููุณุ code, programming... |
| ุชุฑุฌูุฉ | ๐ | ุชุฑุฌูุฉุ ุฅูุฌููุฒูุ translation... |
| ูุฑุงุฌุน | ๐ | ูุง ุนุฏุง ุฐูู |

---

## โ๏ธ ุงูุชุตุญูุญ ุงูุฅููุงุฆู

ุงูุณูุฑูุจุช ูุตุญุญ ุงูุฃุฎุทุงุก ุงูุนุฑุจูุฉ ุงูุดุงุฆุนุฉ:

| ุงูุฎุทุฃ | ุงูุตูุงุจ |
|-------|--------|
| ุงูุดุงุก | ุฅูุดุงุก |
| ุงูุถุงู | ุฃูุถุงู |
| ุงูู | ุฅูู |
| ุฌุฏุง | ุฌุฏุงู |
| ุงูุงู | ุงูุขู |
| ุฐุงูู | ุฐูู |

---

## ๐ป ุงูุฎุทูุฉ ุงูุชุงููุฉ

ุจุนุฏ ุชุดุบูู ุงูุณูุฑูุจุช ุฃูููุงูู:

1. ุฑุงุฌุน ุงููููุงุช ูู ูุฌูุฏ ุงูุฅุฎุฑุงุฌ
2. ุชุญูู ูู ุงูุชุฑุฌูุงุช ูู `translations_collected.csv`
3. ุดุงุฑู ุงููุงุชุฌ ูุนู ููุชุญูู ุงูููุงุฆู
4. ุณุฃููู ุจุฑูุน ูู ุดูุก ุฅูู GitHub

---

## ๐ค ูุซุงู ูููุงุชุฌ

```markdown
# ุชูุฑูุฑ ูุนุงูุฌุฉ ุงููููุงุช

**ุชุงุฑูุฎ ุงููุนุงูุฌุฉ:** 2026-02-09 15:30:00

## ๐ ุฅุญุตุงุฆูุงุช

| ุงูุจูุฏ | ุงูุนุฏุฏ |
|-------|-------|
| ุงููููุงุช ุงููุนุงูุฌุฉ | 12 |
| ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | 237 |

## ๐ ุงูุชุตููู

### ๐ฅ medical
- orthopedics_guide.md
- supplies_prices.xlsx

### ๐ป technical
- package_lists.md
- vpn_tools.md
```

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/WORKFLOW_GUIDE.md
----------------------------------------
# ๐ ุฏููู ุณูุฑ ุงูุนูู ุงูุดุงูู - Corpus Processing Workflow

## ูุธุฑุฉ ุนุงูุฉ

ูุฐุง ุงูุฏููู ูุดุฑุญ ููููุฉ ุงุณุชุฎุฏุงู ูุฌููุนุฉ ุงูุณูุฑูุจุชุงุช ููุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ ุจุดูู ูุชูุงูู.

## ๐ ุณูุฑ ุงูุนูู ุงููุงูู

```
โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ
โ  ูููุงุช ุฃุฑุดูู    โโโโโโถโ   zip_rar_      โโโโโโถโ   combined_     โ
โ  (ZIP/RAR/TAR)  โ     โ   folder2txt.py โ     โ   text_splitter โ
โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ
                                                        โ
                        โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
                        โผ
โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ
โ   ูููุงุช ูุตูุฉ    โโโโโโถโ   corpus_       โโโโโโถโ   corpus_       โ
โ   ูููุตูุฉ        โ     โ   dedup_ai.py   โ     โ   processor_    โ
โโโโโโโโโโโโโโโโโโโ     โโโโโโโโโโโโโโโโโโโ     โ   offline.py    โ
                                                โโโโโโโโโโโโโโโโโโโ
                        โ                               โ
                        โผ                               โผ
                โโโโโโโโโโโโโโโโโโโ             โโโโโโโโโโโโโโโโโโโ
                โ translations.csvโ             โ ูููุงุช ูุตููุฉ    โ
                โ (ุชุฑุฌูุงุช)        โ             โ (ุทุจู/ุชููู/...) โ
                โโโโโโโโโโโโโโโโโโโ             โโโโโโโโโโโโโโโโโโโ
```

## ๐ฆ ุงูุณูุฑูุจุชุงุช ุงููุชุงุญุฉ

### 1. `zip_rar_folder2txt.py`
**ุงููุธููุฉ:** ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูุฃุฑุดููุงุช ูุงููุฌูุฏุงุช ุฅูู ููู ูุตู ูุฌูุน

```bash
# ุงุณุชุฎุฑุงุฌ ุฃุฑุดูู
python zip_rar_folder2txt.py -i archive.zip -o ./output

# ุงุณุชุฎุฑุงุฌ ูุฌูุฏ
python zip_rar_folder2txt.py -i ./my_folder -o ./output
```

**ุงููุชูุฌุฉ:** ููู ูุตู ูุญุชูู ุนูู ูุญุชูู ุฌููุน ุงููููุงุช

---

### 2. `combined_text_splitter.py`
**ุงููุธููุฉ:** ุชูุณูู ุงูููู ุงููุตู ุงููุฌูุน ุฅูู ูููุงุช ูููุตูุฉ

```bash
# ุชูุณูู ููู ูุงุญุฏ
python combined_text_splitter.py -i combined.txt -o ./split_files

# ุชูุณูู ุฌููุน ุงููููุงุช ูู ูุฌูุฏ
python combined_text_splitter.py -i ./upload -o ./split_files
```

**ุงููุชูุฌุฉ:** ูููุงุช ูุตูุฉ ูููุตูุฉ ููู ูุญุชูู ุฃุตูู

---

### 3. `corpus_dedup_ai.py`
**ุงููุธููุฉ:** ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู

```bash
# ุจุฏูู AI - ููุท ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
python corpus_dedup_ai.py -i ./files -o ./processed --no-ai

# ูุน Ollama (ูุฌุงูู ูุญููุงู)
python corpus_dedup_ai.py -i ./files -o ./processed --api ollama

# ูุน OpenRouter
python corpus_dedup_ai.py -i ./files -o ./processed --api openrouter --api-key KEY
```

**ุงููุชูุฌุฉ:** 
- ูููุงุช ูุฑูุฏุฉ
- ุชูุฑูุฑ ุงูุชูุฑุงุฑุงุช
- ููุฎุตุงุช AI (ุงุฎุชูุงุฑู)

---

### 4. `corpus_processor_offline.py`
**ุงููุธููุฉ:** ูุนุงูุฌุฉ ุดุงููุฉ ุฃูููุงูู ูููููุงุช ุงููุจูุฑุฉ

```bash
# ูุนุงูุฌุฉ ูุงููุฉ
python corpus_processor_offline.py -i ./files -o ./processed

# ุนูู ุฏูุนุงุช
python corpus_processor_offline.py -i ./files -o ./processed --batch-size 100

# ุงุณุชุฆูุงู
python corpus_processor_offline.py -i ./files -o ./processed --resume
```

**ุงููุชูุฌุฉ:**
- ูููุงุช ูุตููุฉ
- ุชุฑุฌูุงุช ูุณุชุฎุฑุฌุฉ
- ุชูุฑูุฑ ุดุงูู

---

## ๐ ูุซุงู ูุงูู

### ุงูุฎุทูุฉ 1: ุงุณุชุฎุฑุงุฌ ุงูุฃุฑุดูู
```bash
# ูุฏูู ููู ุฃุฑุดูู ูุญุชูู ุนูู ูููุงุช ูุชุนุฏุฏุฉ
python scripts/zip_rar_folder2txt.py -i data.zip -o ./temp
```

### ุงูุฎุทูุฉ 2: ุชูุณูู ุงูููู ุงููุฌูุน
```bash
# ุชูุณูู ุฅูู ูููุงุช ูููุตูุฉ
python scripts/combined_text_splitter.py -i ./temp/data_zip_contents.txt -o ./split_files
```

### ุงูุฎุทูุฉ 3: ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
```bash
# ุฅุฒุงูุฉ ุงููููุงุช ุงูููุฑุฑุฉ
python scripts/corpus_dedup_ai.py -i ./split_files -o ./unique_files --no-ai
```

### ุงูุฎุทูุฉ 4: ุงููุนุงูุฌุฉ ุงูููุงุฆูุฉ
```bash
# ูุนุงูุฌุฉ ูุชุตููู
python scripts/corpus_processor_offline.py -i ./unique_files/unique_files -o ./final_output
```

### ุงููุชูุฌุฉ:
```
final_output/
โโโ technical/       # ูููุงุช ุชูููุฉ
โโโ medical/         # ูููุงุช ุทุจูุฉ
โโโ translation/     # ูููุงุช ุชุฑุฌูุฉ
โโโ ai/              # ูููุงุช ุฐูุงุก ุงุตุทูุงุนู
โโโ translations.csv # ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ
โโโ PROCESSING_REPORT.md
```

---

## ๐ ููุงุฑูุฉ ุงูุณูุฑูุจุชุงุช

| ุงูููุฒุฉ | zip_rar_2txt | splitter | dedup_ai | processor_offline |
|--------|--------------|----------|----------|-------------------|
| ุงุณุชุฎุฑุงุฌ ุฃุฑุดููุงุช | โ | โ | โ | โ |
| ุชูุณูู ูููุงุช | โ | โ | โ | โ |
| ุฅุฒุงูุฉ ุชูุฑุงุฑุงุช | โ | โ | โ | โ |
| ุชุตููู ุชููุงุฆู | โ | โ | โ | โ |
| ุงุณุชุฎุฑุงุฌ ุชุฑุฌูุงุช | โ | โ | โ | โ |
| ุชุตุญูุญ ุฅููุงุฆู | โ | โ | โ | โ |
| AI ุชุญููู | โ | โ | โ | โ |
| ุฃูููุงูู | โ | โ | โ | โ |

---

## ๐ก ูุตุงุฆุญ

### ูููููุงุช ุงููุจูุฑุฉ (>1GB)
1. ุงุณุชุฎุฏู `--batch-size` ููุงุณุจ
2. ูุนูู `--resume` ููุงุณุชุฆูุงู
3. ูุณู ุงููููุงุช ุนูู ูุฑุงุญู

### ููุญุตูู ุนูู ุฃูุถู ูุชุงุฆุฌ
1. ุงุจุฏุฃ ุจู `combined_text_splitter.py` ูุชูุณูู ุงููููุงุช
2. ุงุณุชุฎุฏู `corpus_dedup_ai.py` ูุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
3. ุฃุฎูุฑุงู `corpus_processor_offline.py` ูููุนุงูุฌุฉ ุงูููุงุฆูุฉ

### ููุชุฑุฌูุงุช
1. ุชุฃูุฏ ูู ูุฌูุฏ ููุท "English - Arabic" ูู ุงููููุงุช
2. ุงูุณูุฑูุจุช ุณูุณุชุฎุฑุฌ ููุตุญุญ ุชููุงุฆูุงู

---

## ๐ ุงูุฑูุงุจุท

- **GitHub:** https://github.com/DrAbdulmalek/Text_snippets
- **Scripts:** `/scripts/`
- **Translations:** `/translation/translations.csv`

---

**ุขุฎุฑ ุชุญุฏูุซ:** ูุจุฑุงูุฑ 2026

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/combined_text_splitter.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
          Combined Text Splitter - ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุงููุฏู:
- ุชูุณูู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ (ุงูุชู ุฃูุดุฃูุง zip_rar_folder2txt.py)
- ูู ูุญุชูู ููู ุฅูู ููู ูุตู ูููุตู
- ุงูุญูุงุธ ุนูู ูููู ุงููุฌูุฏุงุช ุงูุฃุตูู
- ุชุณููู ุงููุนุงูุฌุฉ ูุงูุฑูุน

ุงูุงุณุชุฎุฏุงู:
    python combined_text_splitter.py -i input.txt -o ./output_folder
    python combined_text_splitter.py -i ./upload -o ./split_files
"""

import os
import re
import argparse
import datetime
import hashlib
from pathlib import Path

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุณุงุนุฏุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def sanitize_filename(filename):
    """ุชุญููู ุงุณู ุงูููู ุฅูู ุงุณู ุตุงูุญ"""
    # ุฅุฒุงูุฉ ุงูุฃุญุฑู ุบูุฑ ุงููุณููุญุฉ
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    filename = filename.strip()
    # ุชุญููู ุงููุณุงูุงุช ุงููุชุนุฏุฏุฉ ุฅูู ุดุฑุทุฉ ุณูููุฉ
    filename = re.sub(r'\s+', '_', filename)
    # ุชุญุฏูุฏ ุทูู ุงูุงุณู
    if len(filename) > 200:
        name, ext = os.path.splitext(filename)
        filename = name[:190] + ext
    return filename or 'unnamed_file'

def get_unique_path(base_path, filename):
    """ุงูุญุตูู ุนูู ูุณุงุฑ ูุฑูุฏ"""
    full_path = os.path.join(base_path, filename)
    if not os.path.exists(full_path):
        return full_path
    
    name, ext = os.path.splitext(filename)
    counter = 1
    while os.path.exists(full_path):
        new_name = f"{name}_{counter}{ext}"
        full_path = os.path.join(base_path, new_name)
        counter += 1
    
    return full_path

def get_content_hash(content):
    """ุญุณุงุจ hash ูููุญุชูู"""
    return hashlib.md5(content.encode('utf-8', errors='replace')).hexdigest()[:8]

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุณู ุงููููุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class CombinedTextSplitter:
    """ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ"""
    
    def __init__(self, output_dir):
        self.output_dir = output_dir
        self.files_created = []
        self.processed_hashes = set()
        self.stats = {
            'total_files': 0,
            'total_size': 0,
            'duplicates': 0,
            'empty_files': 0,
            'errors': 0
        }
        
        # ุฅูุดุงุก ุงููุฌูุฏ
        os.makedirs(output_dir, exist_ok=True)
    
    def split_combined_file(self, filepath):
        """ุชูุณูู ููู ูุตู ูุฌูุน"""
        filename = os.path.basename(filepath)
        print(f"\n๐ ูุนุงูุฌุฉ: {filename}")
        
        try:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
        except Exception as e:
            print(f"   โ ุฎุทุฃ ูู ุงููุฑุงุกุฉ: {str(e)}")
            self.stats['errors'] += 1
            return
        
        # ุชูุณูู ุงููุญุชูู
        self._parse_and_split(content, filepath)
    
    def _parse_and_split(self, content, source_file):
        """ุชุญููู ุงููุญุชูู ูุชูุณููู"""
        lines = content.split('\n')
        current_file = None
        current_content = []
        in_content = False
        file_count = 0
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            # ุงูุจุญุซ ุนู ุงุณู ุงูููู
            if line.startswith('ุงุณู ุงูููู:'):
                # ุญูุธ ุงูููู ุงูุณุงุจู
                if current_file and current_content:
                    self._save_file(current_file, current_content)
                    file_count += 1
                
                # ุงุณุชุฎุฑุงุฌ ุงุณู ุงูููู ุงูุฌุฏูุฏ
                current_file = line.replace('ุงุณู ุงูููู:', '').strip()
                current_content = []
                in_content = False
            
            # ุงูุจุญุซ ุนู ูุงุตู ุงููุญุชูู
            elif line.startswith('---') and current_file:
                in_content = True
            
            # ุงูุจุญุซ ุนู ููุงูุฉ ุงููุณู
            elif line.startswith('===') and current_file and in_content:
                # ุญูุธ ุงูููู
                if current_content:
                    self._save_file(current_file, current_content)
                    file_count += 1
                current_file = None
                current_content = []
                in_content = False
            
            # ุฅุถุงูุฉ ุงููุญุชูู
            elif in_content and current_file:
                current_content.append(line)
            
            i += 1
        
        # ุญูุธ ุขุฎุฑ ููู
        if current_file and current_content:
            self._save_file(current_file, current_content)
            file_count += 1
        
        print(f"   โ ุชู ุงุณุชุฎุฑุงุฌ {file_count} ููู")
    
    def _save_file(self, original_path, content_lines):
        """ุญูุธ ููู ูููุตู"""
        content = '\n'.join(content_lines).strip()
        
        # ุชุฌุงูู ุงููููุงุช ุงููุงุฑุบุฉ
        if not content or len(content) < 5:
            self.stats['empty_files'] += 1
            return
        
        # ุงูุชุญูู ูู ุงูุชูุฑุงุฑ
        content_hash = get_content_hash(content)
        if content_hash in self.processed_hashes:
            self.stats['duplicates'] += 1
            return
        
        self.processed_hashes.add(content_hash)
        
        # ุฅูุดุงุก ุงุณู ุงูููู
        safe_name = sanitize_filename(original_path)
        if not safe_name.endswith('.txt'):
            safe_name += '.txt'
        
        # ุฅูุดุงุก ุงููุฌูุฏุงุช ุงููุฑุนูุฉ ุฅุฐุง ูุงู ุงููุณุงุฑ ูุญุชูู ุนูู ูุฌูุฏุงุช
        if '/' in original_path or '\\' in original_path:
            parts = original_path.replace('\\', '/').split('/')
            subdir = '/'.join(parts[:-1])
            subdir_clean = sanitize_filename(subdir)
            full_dir = os.path.join(self.output_dir, subdir_clean)
            os.makedirs(full_dir, exist_ok=True)
            safe_name = sanitize_filename(parts[-1])
            if not safe_name.endswith('.txt'):
                safe_name += '.txt'
            output_path = get_unique_path(full_dir, safe_name)
        else:
            output_path = get_unique_path(self.output_dir, safe_name)
        
        # ุญูุธ ุงูููู
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.files_created.append({
                'path': output_path,
                'original': original_path,
                'size': len(content),
                'hash': content_hash
            })
            
            self.stats['total_files'] += 1
            self.stats['total_size'] += len(content)
        except Exception as e:
            print(f"   โ๏ธ ุฎุทุฃ ูู ุญูุธ {safe_name}: {str(e)}")
            self.stats['errors'] += 1
    
    def process_directory(self, dirpath):
        """ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช ูู ูุฌูุฏ"""
        print("\n" + "=" * 60)
        print("๐ ูุนุงูุฌุฉ ุงููุฌูุฏ")
        print("=" * 60)
        
        files = []
        for f in os.listdir(dirpath):
            if f.endswith('.txt') or f.endswith('_contents.txt'):
                files.append(os.path.join(dirpath, f))
        
        print(f"\n๐ ุนุฏุฏ ุงููููุงุช: {len(files)}")
        
        for filepath in sorted(files):
            self.split_combined_file(filepath)
    
    def create_report(self):
        """ุฅูุดุงุก ุชูุฑูุฑ"""
        report_path = os.path.join(self.output_dir, 'SPLIT_REPORT.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ุชูุฑูุฑ ุชูุณูู ุงููููุงุช ุงููุตูุฉ\n\n")
            f.write(f"**ุงูุชุงุฑูุฎ:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ๐ ุฅุญุตุงุฆูุงุช\n\n")
            f.write("| ุงูุจูุฏ | ุงูุนุฏุฏ |\n|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงููุณุชุฎุฑุฌุฉ | {self.stats['total_files']} |\n")
            f.write(f"| ุงูุญุฌู ุงูููู | {self.stats['total_size'] / 1024:.1f} KB |\n")
            f.write(f"| ุงููููุงุช ุงูููุฑุฑุฉ | {self.stats['duplicates']} |\n")
            f.write(f"| ุงููููุงุช ุงููุงุฑุบุฉ | {self.stats['empty_files']} |\n")
            f.write(f"| ุงูุฃุฎุทุงุก | {self.stats['errors']} |\n\n")
            
            if self.files_created:
                f.write("## ๐ ูุงุฆูุฉ ุงููููุงุช\n\n")
                for i, file_info in enumerate(self.files_created[:100], 1):
                    f.write(f"{i}. `{os.path.basename(file_info['path'])}` ({file_info['size']} bytes)\n")
                
                if len(self.files_created) > 100:
                    f.write(f"\n... ู {len(self.files_created) - 100} ููู ุขุฎุฑ\n")
        
        print(f"\n๐ ุชูุฑูุฑ: {report_path}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุทุฉ ุงูุฏุฎูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def main():
    parser = argparse.ArgumentParser(
        description='ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  # ุชูุณูู ููู ูุงุญุฏ
  python combined_text_splitter.py -i combined.txt -o ./split
  
  # ุชูุณูู ุฌููุน ุงููููุงุช ูู ูุฌูุฏ
  python combined_text_splitter.py -i ./upload -o ./split_files
        """
    )
    
    parser.add_argument('-i', '--input', required=True,
                        help='ุงูููู ุฃู ุงููุฌูุฏ ุงููุฏุฎู')
    parser.add_argument('-o', '--output', default='./split_output',
                        help='ูุฌูุฏ ุงูุฅุฎุฑุงุฌ')
    parser.add_argument('--no-report', action='store_true',
                        help='ุนุฏู ุฅูุดุงุก ุชูุฑูุฑ')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"โ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {args.input}")
        return 1
    
    # ุฅูุดุงุก ุงูููุณู
    splitter = CombinedTextSplitter(args.output)
    
    # ุงููุนุงูุฌุฉ
    if os.path.isfile(args.input):
        splitter.split_combined_file(args.input)
    else:
        splitter.process_directory(args.input)
    
    # ุฅูุดุงุก ุงูุชูุฑูุฑ
    if not args.no_report:
        splitter.create_report()
    
    # ุนุฑุถ ุงูููุฎุต
    print("\n" + "=" * 60)
    print("โ ุงูุชูู ุงูุชูุณูู!")
    print("=" * 60)
    print(f"   ๐ ุงููุฌูุฏ: {args.output}")
    print(f"   ๐ ุงููููุงุช: {splitter.stats['total_files']}")
    print(f"   ๐ ุงูุญุฌู: {splitter.stats['total_size'] / 1024:.1f} KB")
    
    return 0

if __name__ == '__main__':
    exit(main())

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/corpus_dedup_ai.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ            Corpus Dedup & AI Processor - ูุนุงูุฌ ุงููููุงุช ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู         โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ
โ  ุณูุฑูุจุช ูุชูุฏู ููุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ ูุน ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                 โ
โ  1. ูุฑุงุกุฉ ูุชุฃููุฉ ููู ููู                                                        โ
โ  2. ูุดู ุงููุญุชูู ุงูููุฑุฑ (ููุณ ุฃู ูุชุดุงุจู)                                           โ
โ  3. ุฏูุฌ ุฃู ุญุฐู ุงูุชูุฑุงุฑุงุช                                                        โ
โ  4. ุชุญููู ุงููุญุชูู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู                                              โ
โ  5. ุชูุฎูุต ูุชุตููู ุฐูู                                                            โ
โ  6. ุฏุนู APIs ูุฌุงููุฉ ูุชุนุฏุฏุฉ                                                       โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุงูุงุณุชุฎุฏุงู:
    python corpus_dedup_ai.py --input <ูุณุงุฑ> --output <ูุณุงุฑ> --api <ููุน_ุงููAPI>
    
ุฃูุซูุฉ:
    # ุงุณุชุฎุฏุงู Ollama (ูุญูู - ูุฌุงูู ุชูุงูุงู)
    python corpus_dedup_ai.py -i ./files -o ./processed --api ollama
    
    # ุงุณุชุฎุฏุงู OpenRouter (ููุงุฐุฌ ูุฌุงููุฉ)
    python corpus_dedup_ai.py -i ./files -o ./processed --api openrouter --api-key YOUR_KEY
    
    # ุงุณุชุฎุฏุงู DeepSeek (ุฑุฎูุต ุฌุฏุงู)
    python corpus_dedup_ai.py -i ./files -o ./processed --api deepseek --api-key YOUR_KEY
    
    # ุจุฏูู AI - ููุท ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
    python corpus_dedup_ai.py -i ./files -o ./processed --no-ai
"""

import os
import sys
import re
import json
import csv
import hashlib
import argparse
import datetime
import time
import threading
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from difflib import SequenceMatcher
from typing import List, Dict, Tuple, Optional, Set
import textwrap

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุญุต ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

REQUESTS_AVAILABLE = False
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    pass

NUMPY_AVAILABLE = False
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    pass

SKLEARN_AVAILABLE = False
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    pass

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุซูุงุจุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

TEXT_EXTENSIONS = {
    '.txt', '.py', '.js', '.json', '.xml', '.csv', '.md', '.yml', '.yaml',
    '.ini', '.cfg', '.conf', '.java', '.c', '.cpp', '.h', '.cs', '.php',
    '.rb', '.go', '.rs', '.swift', '.kt', '.sql', '.sh', '.bat', '.ps1',
    '.log', '.tex', '.rst', '.adoc', '.ts', '.tsx', '.jsx', '.vue'
}

IGNORE_FOLDERS = {'venv', '__pycache__', '.git', '.idea', '.vscode', 'node_modules', '.venv'}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุชุดุงุจู ุงููุตูุต
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def get_text_hash(text: str) -> str:
    """ุญุณุงุจ hash ูููุต"""
    return hashlib.md5(text.encode('utf-8', errors='replace')).hexdigest()

def normalize_text(text: str) -> str:
    """ุชุทุจูุน ุงููุต ููููุงุฑูุฉ"""
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    text = re.sub(r'\s+', ' ', text)
    # ุฅุฒุงูุฉ ุนูุงูุงุช ุงูุชุฑููู
    text = re.sub(r'[^\w\s\u0600-\u06FF]', '', text)
    # ุชุญููู ูุญุฑูู ุตุบูุฑุฉ
    text = text.lower()
    return text.strip()

def similarity_ratio(text1: str, text2: str) -> float:
    """ุญุณุงุจ ูุณุจุฉ ุงูุชุดุงุจู ุจูู ูุตูู (0-1)"""
    if not text1 or not text2:
        return 0.0
    
    # ุชุดุงุจู ุณุฑูุน ุจูุงุกู ุนูู Hash
    if get_text_hash(text1) == get_text_hash(text2):
        return 1.0
    
    # ุชุทุจูุน ุงููุตูุต
    norm1 = normalize_text(text1)
    norm2 = normalize_text(text2)
    
    # ุชุดุงุจู Hash ุจุนุฏ ุงูุชุทุจูุน
    if get_text_hash(norm1) == get_text_hash(norm2):
        return 0.99
    
    # ุชุดุงุจู SequenceMatcher (ุฃุจุทุฃ ููู ุฃุฏู)
    return SequenceMatcher(None, norm1, norm2).ratio()

def jaccard_similarity(text1: str, text2: str) -> float:
    """ุญุณุงุจ ุชุดุงุจู Jaccard ุจูุงุกู ุนูู ุงููููุงุช"""
    words1 = set(normalize_text(text1).split())
    words2 = set(normalize_text(text2).split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union) if union else 0.0

def calculate_content_similarity(text1: str, text2: str) -> float:
    """ุญุณุงุจ ุงูุชุดุงุจู ุงููุฑูุจ"""
    # ุฏูุฌ ุนุฏุฉ ุทุฑู ููุญุตูู ุนูู ูุชูุฌุฉ ุฃุฏู
    seq_sim = similarity_ratio(text1, text2)
    jac_sim = jaccard_similarity(text1, text2)
    
    # ูุชูุณุท ูุฑุฌุญ
    return (seq_sim * 0.6 + jac_sim * 0.4)

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุฆุฉ ูุดู ุงูุชูุฑุงุฑุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class DuplicateDetector:
    """ูุงุดู ุงูุชูุฑุงุฑุงุช ุงููุชูุฏู"""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.hash_index = {}  # hash -> file_path
        self.content_groups = defaultdict(list)  # group_id -> [file_paths]
        self.file_contents = {}  # file_path -> content
        self.duplicates = []  # list of (original, duplicate, similarity)
    
    def add_file(self, file_path: str, content: str) -> bool:
        """ุฅุถุงูุฉ ููู ูุงูุชุญูู ูู ุงูุชูุฑุงุฑ"""
        content_hash = get_text_hash(content)
        norm_hash = get_text_hash(normalize_text(content))
        
        # ูุญุต ุงูุชุทุงุจู ุงูุชุงู
        if content_hash in self.hash_index:
            original = self.hash_index[content_hash]
            self.duplicates.append((original, file_path, 1.0))
            return True  # ููุฑุฑ ุชูุงูุงู
        
        # ูุญุต ุงูุชุทุงุจู ุจุนุฏ ุงูุชุทุจูุน
        if norm_hash in self.hash_index:
            original = self.hash_index[norm_hash]
            self.duplicates.append((original, file_path, 0.99))
            return True
        
        # ูุญุต ุงูุชุดุงุจู ูุน ุงููููุงุช ุงูููุฌูุฏุฉ
        for existing_path, existing_content in self.file_contents.items():
            similarity = calculate_content_similarity(content, existing_content)
            if similarity >= self.similarity_threshold:
                self.duplicates.append((existing_path, file_path, similarity))
                return True
        
        # ููู ุฌุฏูุฏ - ุฅุถุงูุชู ููููุฑุณ
        self.hash_index[content_hash] = file_path
        self.hash_index[norm_hash] = file_path
        self.file_contents[file_path] = content
        return False
    
    def get_duplicates(self) -> List[Tuple]:
        """ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงูุชูุฑุงุฑุงุช"""
        return sorted(self.duplicates, key=lambda x: x[2], reverse=True)
    
    def get_unique_files(self) -> List[str]:
        """ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงููููุงุช ุงููุฑูุฏุฉ"""
        return list(self.file_contents.keys())
    
    def get_statistics(self) -> Dict:
        """ุงูุญุตูู ุนูู ุฅุญุตุงุฆูุงุช"""
        return {
            'total_files_checked': len(self.file_contents) + len(self.duplicates),
            'unique_files': len(self.file_contents),
            'duplicates_found': len(self.duplicates),
            'exact_duplicates': sum(1 for d in self.duplicates if d[2] == 1.0),
            'near_duplicates': sum(1 for d in self.duplicates if d[2] < 1.0)
        }

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุฆุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class AIAnalyzer:
    """ูุญูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู"""
    
    def __init__(self, api_type: str = 'ollama', api_key: str = None, model: str = None):
        self.api_type = api_type
        self.api_key = api_key
        self.model = model
        self.ollama_url = "http://localhost:11434"
        
        # ููุงุฐุฌ ุงูุชุฑุงุถูุฉ
        self.default_models = {
            'ollama': 'llama3.2:latest',
            'openrouter': 'meta-llama/llama-3.2-3b-instruct:free',
            'deepseek': 'deepseek-chat',
            'huggingface': 'mistralai/Mistral-7B-Instruct-v0.2'
        }
        
        if not model:
            self.model = self.default_models.get(api_type, 'llama3.2')
        
        # ุงูุชุญูู ูู ุงูุชููุฑ
        self.available = self._check_availability()
    
    def _check_availability(self) -> bool:
        """ุงูุชุญูู ูู ุชููุฑ ุงูู API"""
        if not REQUESTS_AVAILABLE:
            print("โ๏ธ ููุชุจุฉ requests ุบูุฑ ูุซุจุชุฉ: pip install requests")
            return False
        
        if self.api_type == 'ollama':
            try:
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
                return response.status_code == 200
            except:
                print("โ๏ธ Ollama ุบูุฑ ูุชุงุญ. ุชุฃูุฏ ูู ุชุดุบููู: ollama serve")
                return False
        
        elif self.api_type in ['openrouter', 'deepseek', 'huggingface']:
            if not self.api_key:
                print(f"โ๏ธ API key ูุทููุจ ูู {self.api_type}")
                return False
            return True
        
        return False
    
    def _call_ollama(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """ุงุณุชุฏุนุงุก Ollama API"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": 0.3
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json().get('response', '').strip()
        except Exception as e:
            print(f"โ ุฎุทุฃ ูู Ollama: {str(e)}")
        return None
    
    def _call_openrouter(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """ุงุณุชุฏุนุงุก OpenRouter API"""
        try:
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "HTTP-Referer": "https://github.com",
                    "X-Title": "Corpus Processor"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": 0.3
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content'].strip()
            else:
                print(f"โ ุฎุทุฃ OpenRouter: {response.status_code}")
        except Exception as e:
            print(f"โ ุฎุทุฃ ูู OpenRouter: {str(e)}")
        return None
    
    def _call_deepseek(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """ุงุณุชุฏุนุงุก DeepSeek API"""
        try:
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": 0.3
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"โ ุฎุทุฃ ูู DeepSeek: {str(e)}")
        return None
    
    def analyze(self, prompt: str, max_tokens: int = 500) -> Optional[str]:
        """ุชุญููู ุงููุต ุจุงุณุชุฎุฏุงู AI"""
        if not self.available:
            return None
        
        if self.api_type == 'ollama':
            return self._call_ollama(prompt, max_tokens)
        elif self.api_type == 'openrouter':
            return self._call_openrouter(prompt, max_tokens)
        elif self.api_type == 'deepseek':
            return self._call_deepseek(prompt, max_tokens)
        
        return None
    
    def summarize(self, content: str, max_length: int = 200) -> Optional[str]:
        """ุชูุฎูุต ุงููุญุชูู"""
        # ุฃุฎุฐ ุฃูู 2000 ุญุฑู ููุท ููุชูุฎูุต
        text_to_analyze = content[:2000] if len(content) > 2000 else content
        
        prompt = f"""ูู ุจุชูุฎูุต ุงููุต ุงูุชุงูู ูู ุญุฏูุฏ {max_length} ูููุฉ ุจุงููุบุฉ ุงูุนุฑุจูุฉ. 
ุฑูุฒ ุนูู ุงูููุงุท ุงูุฑุฆูุณูุฉ ูุงููุนูููุงุช ุงููููุฉ.

ุงููุต:
{text_to_analyze}

ุงูููุฎุต:"""
        
        return self.analyze(prompt, max_tokens=300)
    
    def classify(self, content: str) -> Optional[Dict]:
        """ุชุตููู ุงููุญุชูู"""
        text_to_analyze = content[:1500] if len(content) > 1500 else content
        
        prompt = f"""ุตูู ุงููุญุชูู ุงูุชุงูู ูุฃุนุฏ ุงูุฅุฌุงุจุฉ ุจุชูุณูู JSON ููุท:
{{"category": "ุงููุฆุฉ", "topics": ["ููุถูุน1", "ููุถูุน2"], "language": "ุงููุบุฉ ุงูุฃุณุงุฑูุฉ", "quality": "ุนุงููุฉ/ูุชูุณุทุฉ/ููุฎูุถุฉ"}}

ุงููุฆุงุช ุงููุชุงุญุฉ: ุทุจูุ ุชูููุ ุชุฑุฌูุฉุ ุฐูุงุก_ุงุตุทูุงุนูุ ุจุฑูุฌุฉุ ูุชูุฑูุงุช

ุงููุต:
{text_to_analyze}

ุงูุชุตููู (JSON ููุท):"""
        
        result = self.analyze(prompt, max_tokens=200)
        
        if result:
            try:
                # ุงุณุชุฎุฑุงุฌ JSON ูู ุงูุฑุฏ
                json_match = re.search(r'\{[^}]+\}', result)
                if json_match:
                    return json.loads(json_match.group())
            except:
                pass
        
        return None
    
    def extract_keywords(self, content: str, max_keywords: int = 10) -> Optional[List[str]]:
        """ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงูููุชุงุญูุฉ"""
        text_to_analyze = content[:1500] if len(content) > 1500 else content
        
        prompt = f"""ุงุณุชุฎุฑุฌ ุฃูู {max_keywords} ูููุงุช ููุชุงุญูุฉ ูู ุงููุต ุงูุชุงูู.
ุฃุนุฏ ุงูุฅุฌุงุจุฉ ููุงุฆูุฉ ููุตููุฉ ุจููุงุตู ููุท.

ุงููุต:
{text_to_analyze}

ุงููููุงุช ุงูููุชุงุญูุฉ:"""
        
        result = self.analyze(prompt, max_tokens=100)
        
        if result:
            # ุงุณุชุฎุฑุงุฌ ุงููููุงุช
            keywords = [k.strip() for k in result.replace('ุ', ',').split(',') if k.strip()]
            return keywords[:max_keywords]
        
        return None
    
    def compare_contents(self, content1: str, content2: str) -> Optional[Dict]:
        """ููุงุฑูุฉ ูุญุชูููู"""
        prompt = f"""ูุงุฑู ุจูู ุงููุตูู ุงูุชุงูููู ูุญุฏุฏ:
1. ูู ุงููุญุชูู ูุชุดุงุจูุ
2. ูุง ุงููุฑููุงุช ุงูุฑุฆูุณูุฉุ
3. ุฃูููุง ุฃูุถู/ุฃูููุ

ุงููุต ุงูุฃูู (ููุฎุต):
{content1[:500]}...

ุงููุต ุงูุซุงูู (ููุฎุต):
{content2[:500]}...

ุฃุนุฏ ุงูุฅุฌุงุจุฉ ุจุชูุณูู JSON:
{{"similar": true/false, "differences": "ุงููุฑููุงุช", "better": "ุงูุฃูุถู"}}"""
        
        result = self.analyze(prompt, max_tokens=300)
        
        if result:
            try:
                json_match = re.search(r'\{[^}]+\}', result)
                if json_match:
                    return json.loads(json_match.group())
            except:
                pass
        
        return None

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงููุนุงูุฌ ุงูุฑุฆูุณู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class CorpusDedupProcessor:
    """ูุนุงูุฌ ุงููููุงุช ูุน ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู"""
    
    def __init__(self, input_path: str, output_path: str, 
                 similarity_threshold: float = 0.85,
                 api_type: str = None, api_key: str = None, model: str = None,
                 use_ai: bool = True):
        
        self.input_path = input_path
        self.output_path = output_path
        self.similarity_threshold = similarity_threshold
        self.use_ai = use_ai
        
        # ุงูููููุงุช
        self.detector = DuplicateDetector(similarity_threshold)
        self.ai = None
        
        if use_ai and api_type:
            self.ai = AIAnalyzer(api_type, api_key, model)
            if not self.ai.available:
                print("โ๏ธ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุบูุฑ ูุชุงุญ - ุงููุชุงุจุนุฉ ุจุฏูู AI")
                self.ai = None
        
        # ุงูุฅุญุตุงุฆูุงุช
        self.stats = {
            'files_read': 0,
            'total_size': 0,
            'duplicates_removed': 0,
            'ai_analyses': 0,
            'errors': 0,
            'start_time': None,
            'end_time': None
        }
        
        # ุงููุชุงุฆุฌ
        self.file_analysis = []
        
        # ุฅูุดุงุก ูุฌูุฏ ุงูุฅุฎุฑุงุฌ
        os.makedirs(output_path, exist_ok=True)
    
    def read_file_content(self, file_path: str) -> Optional[str]:
        """ูุฑุงุกุฉ ูุญุชูู ููู ุจุชุฃูู"""
        try:
            # ุชุญุฏูุฏ ุงูุชุฑููุฒ
            encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1256', 'cp1252']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding, errors='replace') as f:
                        content = f.read()
                    
                    # ุงูุชุญูู ูู ุฃู ุงููุญุชูู ูุตู
                    if content and len(content.strip()) > 0:
                        return content
                except:
                    continue
            
            return None
        except Exception as e:
            print(f"  โ ุฎุทุฃ ูู ูุฑุงุกุฉ {os.path.basename(file_path)}: {str(e)}")
            return None
    
    def collect_files(self) -> List[str]:
        """ุฌูุน ูุงุฆูุฉ ุงููููุงุช"""
        files = []
        
        if os.path.isfile(self.input_path):
            ext = os.path.splitext(self.input_path)[1].lower()
            if ext in TEXT_EXTENSIONS:
                files.append(self.input_path)
        elif os.path.isdir(self.input_path):
            for root, dirs, filenames in os.walk(self.input_path):
                # ุชุฌุงูู ุงููุฌูุฏุงุช ุงููุฎููุฉ
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in IGNORE_FOLDERS]
                
                for filename in filenames:
                    ext = os.path.splitext(filename)[1].lower()
                    if ext in TEXT_EXTENSIONS:
                        files.append(os.path.join(root, filename))
        
        return sorted(files)
    
    def analyze_file_with_ai(self, file_path: str, content: str) -> Dict:
        """ุชุญููู ููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู"""
        result = {
            'file': os.path.basename(file_path),
            'path': file_path,
            'size': len(content),
            'hash': get_text_hash(content),
            'summary': None,
            'classification': None,
            'keywords': None
        }
        
        if self.ai:
            print("    ๐ค ุชุญููู AI...", end=' ')
            
            # ุชูุฎูุต
            summary = self.ai.summarize(content)
            if summary:
                result['summary'] = summary
            
            # ุชุตููู
            classification = self.ai.classify(content)
            if classification:
                result['classification'] = classification
            
            # ูููุงุช ููุชุงุญูุฉ
            keywords = self.ai.extract_keywords(content)
            if keywords:
                result['keywords'] = keywords
            
            self.stats['ai_analyses'] += 1
            print("โ")
        
        return result
    
    def process_files(self):
        """ูุนุงูุฌุฉ ุงููููุงุช"""
        self.stats['start_time'] = datetime.datetime.now()
        
        print("\n" + "=" * 70)
        print("๐ ุฌูุน ุงููููุงุช...")
        print("=" * 70)
        
        files = self.collect_files()
        total = len(files)
        
        print(f"๐ ุฅุฌูุงูู ุงููููุงุช: {total}")
        print(f"๐ฏ ุนุชุจุฉ ุงูุชุดุงุจู: {self.similarity_threshold * 100}%")
        print(f"๐ค ุงูุฐูุงุก ุงูุงุตุทูุงุนู: {'ููุนู - ' + self.ai.api_type if self.ai else 'ูุนุทู'}")
        print()
        
        # ูุนุงูุฌุฉ ูู ููู
        unique_files = []
        duplicate_files = []
        
        for i, file_path in enumerate(files, 1):
            print(f"[{i}/{total}] {os.path.basename(file_path)[:50]}...", end=' ')
            
            # ูุฑุงุกุฉ ุงููุญุชูู
            content = self.read_file_content(file_path)
            
            if not content:
                print("โ๏ธ ูุงุฑุบ ุฃู ุบูุฑ ูุงุจู ูููุฑุงุกุฉ")
                self.stats['errors'] += 1
                continue
            
            self.stats['files_read'] += 1
            self.stats['total_size'] += len(content)
            
            # ูุญุต ุงูุชูุฑุงุฑ
            is_duplicate = self.detector.add_file(file_path, content)
            
            if is_duplicate:
                print("๐ ููุฑุฑ")
                duplicate_files.append(file_path)
                self.stats['duplicates_removed'] += 1
            else:
                print("โ ูุฑูุฏ")
                unique_files.append(file_path)
                
                # ุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู
                if self.ai:
                    analysis = self.analyze_file_with_ai(file_path, content)
                    self.file_analysis.append(analysis)
            
            # ุญูุธ ุชูุฏู ูู 50 ููู
            if i % 50 == 0:
                self.save_progress()
        
        # ุญูุธ ุงููุชุงุฆุฌ
        self.stats['end_time'] = datetime.datetime.now()
        self.save_results(unique_files, duplicate_files)
        self.print_summary()
    
    def save_progress(self):
        """ุญูุธ ุงูุชูุฏู"""
        pass  # ูููู ุฅุถุงูุฉ ุญูุธ ุงูุชูุฏู ููุง
    
    def save_results(self, unique_files: List[str], duplicate_files: List[str]):
        """ุญูุธ ุงููุชุงุฆุฌ"""
        # ุชูุฑูุฑ ุงูุชูุฑุงุฑุงุช
        duplicates_file = os.path.join(self.output_path, 'duplicates_report.json')
        with open(duplicates_file, 'w', encoding='utf-8') as f:
            json.dump({
                'duplicates': [
                    {
                        'original': d[0],
                        'duplicate': d[1],
                        'similarity': round(d[2] * 100, 2)
                    }
                    for d in self.detector.get_duplicates()
                ],
                'statistics': self.detector.get_statistics()
            }, f, ensure_ascii=False, indent=2)
        
        # ุชูุฑูุฑ ุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู
        if self.file_analysis:
            analysis_file = os.path.join(self.output_path, 'ai_analysis.json')
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_analysis, f, ensure_ascii=False, indent=2)
            
            # ุชูุฎูุตุงุช ูููุตูุฉ
            summaries_file = os.path.join(self.output_path, 'summaries.md')
            with open(summaries_file, 'w', encoding='utf-8') as f:
                f.write("# ููุฎุตุงุช ุงููููุงุช\n\n")
                for analysis in self.file_analysis:
                    f.write(f"## {analysis['file']}\n\n")
                    if analysis.get('summary'):
                        f.write(f"**ุงูููุฎุต:** {analysis['summary']}\n\n")
                    if analysis.get('classification'):
                        cat = analysis['classification'].get('category', 'ุบูุฑ ูุตูู')
                        f.write(f"**ุงูุชุตููู:** {cat}\n\n")
                    if analysis.get('keywords'):
                        f.write(f"**ุงููููุงุช ุงูููุชุงุญูุฉ:** {', '.join(analysis['keywords'])}\n\n")
                    f.write("---\n\n")
        
        # ูุณุฎ ุงููููุงุช ุงููุฑูุฏุฉ
        unique_dir = os.path.join(self.output_path, 'unique_files')
        os.makedirs(unique_dir, exist_ok=True)
        
        for file_path in unique_files:
            try:
                filename = os.path.basename(file_path)
                dest = os.path.join(unique_dir, filename)
                
                # ุชุฌูุจ ุงูุชูุฑุงุฑ
                counter = 1
                while os.path.exists(dest):
                    name, ext = os.path.splitext(filename)
                    dest = os.path.join(unique_dir, f"{name}_{counter}{ext}")
                    counter += 1
                
                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                with open(dest, 'w', encoding='utf-8') as f:
                    f.write(content)
            except Exception as e:
                print(f"  โ๏ธ ุฎุทุฃ ูู ูุณุฎ {filename}: {str(e)}")
        
        # ุชูุฑูุฑ ููุงุฆู
        self.save_final_report(unique_files, duplicate_files)
    
    def save_final_report(self, unique_files: List[str], duplicate_files: List[str]):
        """ุญูุธ ุงูุชูุฑูุฑ ุงูููุงุฆู"""
        report_file = os.path.join(self.output_path, 'DEDUP_REPORT.md')
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ุชูุฑูุฑ ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู\n\n")
            
            f.write(f"**ุชุงุฑูุฎ ุงููุนุงูุฌุฉ:** {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**ูุฏุฉ ุงููุนุงูุฌุฉ:** {duration:.1f} ุซุงููุฉ\n\n")
            
            f.write("## ๐ ุงูุฅุญุตุงุฆูุงุช\n\n")
            f.write("| ุงูุจูุฏ | ุงูุนุฏุฏ |\n|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงูููุฑูุกุฉ | {self.stats['files_read']} |\n")
            f.write(f"| ุงููููุงุช ุงููุฑูุฏุฉ | {len(unique_files)} |\n")
            f.write(f"| ุงูุชูุฑุงุฑุงุช ุงูููุชุดูุฉ | {self.stats['duplicates_removed']} |\n")
            f.write(f"| ุญุฌู ุงูุจูุงูุงุช | {self.stats['total_size'] / 1024 / 1024:.2f} MB |\n")
            f.write(f"| ุชุญูููุงุช AI | {self.stats['ai_analyses']} |\n\n")
            
            dedup_stats = self.detector.get_statistics()
            f.write("## ๐ ุชูุงุตูู ุงูุชูุฑุงุฑุงุช\n\n")
            f.write(f"- ุงูุชูุฑุงุฑุงุช ุงูุชุงูุฉ: {dedup_stats['exact_duplicates']}\n")
            f.write(f"- ุงูุชูุฑุงุฑุงุช ุงููุชุดุงุจูุฉ: {dedup_stats['near_duplicates']}\n\n")
            
            if duplicate_files:
                f.write("## ๐ ูุงุฆูุฉ ุงูุชูุฑุงุฑุงุช\n\n")
                for dup in self.detector.get_duplicates()[:50]:
                    f.write(f"- `{os.path.basename(dup[1])}` ููุฑุฑ ูู `{os.path.basename(dup[0])}` ({dup[2]*100:.0f}%)\n")
                
                if len(self.detector.get_duplicates()) > 50:
                    f.write(f"\n... ู {len(self.detector.get_duplicates()) - 50} ุชูุฑุงุฑ ุขุฎุฑ\n")
            
            f.write("\n## โ ุงููููุงุช ุงููุฑูุฏุฉ\n\n")
            for i, file_path in enumerate(unique_files[:100], 1):
                f.write(f"{i}. {os.path.basename(file_path)}\n")
            
            if len(unique_files) > 100:
                f.write(f"\n... ู {len(unique_files) - 100} ููู ุขุฎุฑ\n")
        
        print(f"\n๐ ุชูุฑูุฑ: {report_file}")
    
    def print_summary(self):
        """ุนุฑุถ ููุฎุต"""
        print("\n" + "=" * 70)
        print("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
        print("=" * 70)
        
        print(f"\n๐ ุงูุฅุญุตุงุฆูุงุช:")
        print(f"   โข ุงููููุงุช ุงูููุฑูุกุฉ: {self.stats['files_read']}")
        print(f"   โข ุงููููุงุช ุงููุฑูุฏุฉ: {self.stats['files_read'] - self.stats['duplicates_removed']}")
        print(f"   โข ุงูุชูุฑุงุฑุงุช ุงูููุชุดูุฉ: {self.stats['duplicates_removed']}")
        print(f"   โข ุญุฌู ุงูุจูุงูุงุช: {self.stats['total_size'] / 1024 / 1024:.2f} MB")
        print(f"   โข ุชุญูููุงุช AI: {self.stats['ai_analyses']}")
        
        print(f"\n๐ ุงููููุงุช ุงููุงุชุฌุฉ:")
        print(f"   โข unique_files/ - ุงููููุงุช ุงููุฑูุฏุฉ")
        print(f"   โข duplicates_report.json - ุชูุฑูุฑ ุงูุชูุฑุงุฑุงุช")
        if self.file_analysis:
            print(f"   โข ai_analysis.json - ุชุญูููุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู")
            print(f"   โข summaries.md - ููุฎุตุงุช ุงููููุงุช")
        print(f"   โข DEDUP_REPORT.md - ุงูุชูุฑูุฑ ุงูููุงุฆู")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุทุฉ ุงูุฏุฎูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def main():
    parser = argparse.ArgumentParser(
        description='ูุนุงูุฌ ุงููููุงุช ูุน ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุงูุชุญููู ุจุงูุฐูุงุก ุงูุงุตุทูุงุนู',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  # ุจุฏูู AI - ููุท ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
  python corpus_dedup_ai.py -i ./files -o ./processed --no-ai
  
  # ุงุณุชุฎุฏุงู Ollama (ูุญูู - ูุฌุงูู)
  python corpus_dedup_ai.py -i ./files -o ./processed --api ollama
  
  # ุงุณุชุฎุฏุงู OpenRouter (ููุงุฐุฌ ูุฌุงููุฉ)
  python corpus_dedup_ai.py -i ./files -o ./processed --api openrouter --api-key YOUR_KEY
  
  # ุงุณุชุฎุฏุงู DeepSeek (ุฑุฎูุต)
  python corpus_dedup_ai.py -i ./files -o ./processed --api deepseek --api-key YOUR_KEY
  
  # ุชุญุฏูุฏ ุนุชุจุฉ ุงูุชุดุงุจู
  python corpus_dedup_ai.py -i ./files -o ./processed --threshold 0.9

ุฎูุงุฑุงุช API ุงููุฌุงููุฉ:
  - Ollama: ูุญูู ุชูุงูุงูุ ูุชุทูุจ ุชุซุจูุช ูุชุดุบูู Ollama
  - OpenRouter: ููุงุฐุฌ ูุฌุงููุฉ ูุซู llama-3.2-3b-instruct:free
  - DeepSeek: ุฑุฎูุต ุฌุฏุงู (~$0.001 ููู 1000 ุฑูุฒ)
        """
    )
    
    parser.add_argument('-i', '--input', required=True, help='ูุณุงุฑ ุงููููุงุช ุงููุฏุฎูุฉ')
    parser.add_argument('-o', '--output', default='./dedup_output', help='ูุณุงุฑ ุงูุฅุฎุฑุงุฌ')
    parser.add_argument('-t', '--threshold', type=float, default=0.85, 
                        help='ุนุชุจุฉ ุงูุชุดุงุจู (0-1)ุ ุงูุชุฑุงุถู 0.85')
    parser.add_argument('--api', choices=['ollama', 'openrouter', 'deepseek', 'huggingface'],
                        help='ููุน API ููุฐูุงุก ุงูุงุตุทูุงุนู')
    parser.add_argument('--api-key', help='ููุชุงุญ API (ูุทููุจ ูู openrouter/deepseek)')
    parser.add_argument('--model', help='ุงุณู ุงููููุฐุฌ (ุงุฎุชูุงุฑู)')
    parser.add_argument('--no-ai', action='store_true', help='ุชุนุทูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู')
    
    args = parser.parse_args()
    
    # ุงูุชุญูู ูู ุงููุณุงุฑ
    if not os.path.exists(args.input):
        print(f"โ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {args.input}")
        sys.exit(1)
    
    # ุงูุชุญูู ูู ุงูู API key
    if args.api and args.api != 'ollama' and not args.api_key:
        print(f"โ๏ธ API key ูุทููุจ ูู {args.api}")
        print(f"   ุงุณุชุฎุฏู: --api-key YOUR_KEY")
    
    # ุจุฏุก ุงููุนุงูุฌุฉ
    processor = CorpusDedupProcessor(
        input_path=args.input,
        output_path=args.output,
        similarity_threshold=args.threshold,
        api_type=args.api if not args.no_ai else None,
        api_key=args.api_key,
        model=args.model,
        use_ai=not args.no_ai
    )
    
    processor.process_files()

if __name__ == '__main__':
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/corpus_processor_offline.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ              Corpus Processor Offline - ูุนุงูุฌ ุงููููุงุช ุงููุตูุฉ ุงูุฃูููุงูู           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ
โ  ุณูุฑูุจุช ุดุงูู ููุนุงูุฌุฉ ูููุงุช ูุจูุฑุฉ ูู ุงููููุงุช ุงููุตูุฉ ุฃูููุงูู                        โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                        โ
โ  1. ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูุฃุฑุดููุงุช (ZIP, RAR, TAR)                                   โ
โ  2. ูุนุงูุฌุฉ ูููุงุช Excel/Word/HTML/SQLite ุจุดูู ุตุญูุญ                             โ
โ  3. ุงุณุชุฎุฑุงุฌ ุงูุนูุงููู ูุงููุญุชูู ูู ุงููุญุงุฏุซุงุช                                     โ
โ  4. ุชุตููู ุงููุญุชูู ุชููุงุฆูุงู (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ุฐูุงุก ุงุตุทูุงุนู/ุจุฑูุฌุฉ/ูุชูุฑูุงุช)          โ
โ  5. ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ                                            โ
โ  6. ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ุซูุงุฆูุฉ ุงููุบุฉ (ุฅูุฌููุฒู-ุนุฑุจู)                               โ
โ  7. ุฅูุดุงุก ููุงุฑุณ ููุธูุฉ ูููููุงุช                                                  โ
โ  8. ุญูุธ ุงูุชูุฏู ูุงุณุชุฆูุงู ุงููุนุงูุฌุฉ                                               โ
โ  9. ูุนุงูุฌุฉ ุนูู ุฏูุนุงุช ูููููุงุช ุงููุจูุฑุฉ                                           โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุงูุงุณุชุฎุฏุงู:
    python corpus_processor_offline.py --input <ูุณุงุฑ> --output <ูุณุงุฑ>
    
ุฃูุซูุฉ:
    # ูุนุงูุฌุฉ ูุฌูุฏ ูุงูู
    python corpus_processor_offline.py -i ./files -o ./processed
    
    # ูุนุงูุฌุฉ ููู ูุงุญุฏ
    python corpus_processor_offline.py -i document.xlsx -o ./output
    
    # ุงุณุชุฆูุงู ูุนุงูุฌุฉ ูุชูููุฉ
    python corpus_processor_offline.py -i ./files -o ./processed --resume
    
    # ูุนุงูุฌุฉ ูุน ุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุฉ
    python corpus_processor_offline.py -i ./files -o ./processed --batch-size 100
"""

import os
import sys
import re
import csv
import json
import zipfile
import tarfile
import pathlib
import argparse
import datetime
import tempfile
import sqlite3
import hashlib
import pickle
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุญุต ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

PANDAS_AVAILABLE = False
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    pass

OPENPYXL_AVAILABLE = False
try:
    from openpyxl import load_workbook
    OPENPYXL_AVAILABLE = True
except ImportError:
    pass

DOCX_AVAILABLE = False
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    pass

HTML_AVAILABLE = False
try:
    from bs4 import BeautifulSoup
    HTML_AVAILABLE = True
except ImportError:
    pass

RAR_AVAILABLE = False
try:
    import rarfile
    RAR_AVAILABLE = True
except ImportError:
    pass

# Add support for Arabic language processing tools
LANGTOOL_AVAILABLE = False
try:
    import language_tool_python
    LANGTOOL_AVAILABLE = True
except ImportError:
    pass

QALSADI_AVAILABLE = False
try:
    from qalsadi.analex import Analex
    QALSADI_AVAILABLE = True
except ImportError:
    pass

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุซูุงุจุช ูุฃููุงุท
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

# ุฃููุงุน ุงููููุงุช
TEXT_EXTENSIONS = {
    '.txt', '.py', '.js', '.json', '.xml', '.csv', '.md', '.yml', '.yaml',
    '.ini', '.cfg', '.conf', '.java', '.c', '.cpp', '.h', '.cs', '.php',
    '.rb', '.go', '.rs', '.swift', '.kt', '.sql', '.sh', '.bat', '.ps1',
    '.r', '.m', '.f', '.for', '.f90', '.f95', '.properties', '.toml',
    '.lock', '.log', '.tex', '.rst', '.adoc', '.asm', '.v', '.vhdl',
    '.ts', '.tsx', '.jsx', '.vue', '.svelte'
}

DB_EXTENSIONS = {'.db', '.sqlite', '.sqlite3'}
WORD_EXTENSIONS = {'.docx'}
EXCEL_EXTENSIONS = {'.xls', '.xlsx'}
HTML_EXTENSIONS = {'.html', '.htm'}
ARCHIVE_EXTENSIONS = {'.zip', '.rar'}
TAR_EXTENSIONS = {'.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz'}

BINARY_EXTENSIONS = {
    '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin', '.obj',
    '.o', '.a', '.lib', '.dylib', '.bundle', '.class', '.jar',
    '.war', '.ear', '.apk', '.ipa', '.app', '.dmg', '.iso',
    '.img', '.raw', '.dat', '.pkl', '.pickle', '.npy', '.npz',
    '.pt', '.pth', '.h5', '.hdf', '.fits', '.parquet', '.feather',
    '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.webp',
    '.mp3', '.mp4', '.wav', '.avi', '.mkv', '.mov', '.pdf'
}

IGNORE_FOLDERS = {'venv', '__pycache__', '.git', '.idea', '.vscode', 'node_modules', '.venv'}

# ุงููููุงุช ุงูููุชุงุญูุฉ ููุชุตููู
CATEGORY_KEYWORDS = {
    'medical': [
        'ุทุจู', 'ุทุจ', 'ูุฑุถ', 'ุนูุงุฌ', 'ุฏูุงุก', 'ูุณุชุดูู', 'ุทุจูุจ', 'ุฌุฑุงุญุฉ', 'ุนุธุงู', 'ูุณุฑ',
        'ุบุถุฑูู', 'ููุตู', 'ูุฑู', 'ุฑูุจุฉ', 'ุนููุฏ ููุฑู', 'ุฃุดุนุฉ', 'ุชุตููุฑ', 'ุชุดุฎูุต',
        'ููุฑุจูุฏุชู', 'ุฅุนุงูุฉ', 'ุฅุตุงุจุฉ', 'ูุณูุฌ', 'ุนุธู', 'ุฏู', 'ููุจ', 'ุดุฑุงููู',
        'ุถูุงุฏ', 'ุฌุฑุญ', 'CT Scan', 'ุฏูููู', 'DICOM', 'ุฃูุฑุซูุจูุฏู',
        'medical', 'hospital', 'doctor', 'surgery', 'orthopedic', 'fracture',
        'bone', 'joint', 'hip', 'knee', 'spine', 'x-ray', 'diagnosis', 'patient',
        'treatment', 'disease', 'medicine', 'clinical', 'surgical', 'anatomy'
    ],
    'technical': [
        'ุจุฑูุฌุฉ', 'ููุฏ', 'ุณูุฑูุฑ', 'ููููุณ', 'ูููุฏูุฒ', 'ุดุจูุฉ', 'ูุงุนุฏุฉ ุจูุงูุงุช', 'ุณูุฑูุจุช',
        'ุชุซุจูุช', 'ุฅุนุฏุงุฏ', 'ุฃูุฑ', 'ูุญุทุฉ', 'ุญุฒูุฉ', 'ุชุญุฏูุซ', 'ุฎุทุฃ', 'ุฅุตูุงุญ',
        'ุจุงููุฌ', 'ุฃุฑุด', 'ูุงูุฌุงุฑู', 'ุบุงุฑูุฏุง', 'ูุฏู', 'ุฌููู', 'ุจุงูุซูู', 'ููุฏ',
        'ุจุฑูุฌูุงุช', 'ูุธุงู', 'ุชูุฒูุนุฉ', 'ูุงุฌูุฉ', 'ุณุทุญ ููุชุจ', 'ููุฑูู', 'ุฏุฑุงููุฑ',
        'programming', 'code', 'server', 'linux', 'windows', 'network', 'database',
        'script', 'install', 'setup', 'command', 'terminal', 'package', 'update',
        'error', 'fix', 'arch', 'manjaro', 'garuda', 'kde', 'gnome', 'python',
        'git', 'docker', 'api', 'config', 'bash', 'shell', 'kernel', 'zorin'
    ],
    'translation': [
        'ุชุฑุฌูุฉ', 'ูุชุฑุฌู', 'ูุบุฉ', 'ุฅูุฌููุฒู', 'ุนุฑุจู', 'ุชุนุฑูุจ', 'ูุต ูุชุฑุฌู',
        'ุซูุงุฆู ุงููุบุฉ', 'ูุนูู', 'ูุงููุณ', 'ูููุฉ', 'ุฌููุฉ', 'ูุบุงุช', 'ุชุฑุฌูุฉ ุขููุฉ',
        'translation', 'translate', 'language', 'english', 'arabic', 'bilingual',
        'meaning', 'dictionary', 'word', 'sentence', 'localized', 'deepl'
    ],
    'ai': [
        'ุฐูุงุก ุงุตุทูุงุนู', 'ุชุนูู ุขูู', 'ุดุจูุฉ ุนุตุจูุฉ', 'ูููุฐุฌ', 'ุชุฏุฑูุจ', 'ุชูุจุค',
        'ูุนุงูุฌุฉ ูุบุฉ', 'ุชุนูู ุนููู', 'ุจูุงูุงุช ุถุฎูุฉ', 'ุฎูุงุฑุฒููุฉ', 'ุชูููุฏ ูุต',
        'artificial intelligence', 'machine learning', 'neural network', 'model',
        'training', 'prediction', 'nlp', 'deep learning', 'big data', 'algorithm',
        'ollama', 'mistral', 'deepseek', 'gpt', 'llm', 'hugging face', 'api key',
        'openrouter', 'continue', 'pycharm', 'vscode', 'text generation'
    ],
    'programming': [
        'ุจุฑูุฌุฉ', 'ุชุทููุฑ', 'ููุฏ', 'ูุดุฑูุน', 'ููุชุจุฉ', 'ุฅุทุงุฑ ุนูู', 'ูุงุฌูุฉ ุจุฑูุฌุฉ',
        'ุชุทุจูู', 'ููุจ', 'ูููุน', 'ุฎุงุฏู', 'ุนููู', 'ูุงุนุฏุฉ ุจูุงูุงุช', 'ุงุณุชุนูุงู',
        'programming', 'development', 'code', 'project', 'library', 'framework',
        'api', 'application', 'web', 'website', 'server', 'client', 'database',
        'smarttextetl', 'jwt', 'authentication', 'react', 'node', 'python'
    ]
}

# ุฃููุงุท ุงุณุชุฎุฑุงุฌ ุงูุนูุงููู
TITLE_PATTERNS = [
    # ุนููุงู Markdown
    r'^#\s+(.+)$',
    r'^##\s+(.+)$',
    # ุนููุงู ุจุนูุงูุฉ ุชุณุงูู
    r'^([^\n]+)\n=+$',
    # ุนููุงู ุจููุงุท
    r'^([^\n]+)\n-+$',
    # ุนููุงู ุจูู ูุฌูู
    r'^\*+\s*(.+?)\s*\*+$',
]

# ุฃููุงุท ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
TRANSLATION_PATTERNS = [
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*[-โโ:]\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*:\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*=\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    r'"?([A-Za-z][A-Za-z\s,\-\(\)]+?)"?\s*[,;]\s*"?([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+?)"?$',
]

# ูุงููุณ ุงูุชุตุญูุญ ุงูุฅููุงุฆู
ARABIC_SPELLING_CORRECTIONS = {
    # ุงูููุฒุงุช
    'ุงุฆูุง': 'ุฅูุง', 'ุงุฆู': 'ุฅู', 'ุฃูุดุงุก': 'ุฅูุดุงุก', 'ุงูุดุงุก': 'ุฅูุดุงุก',
    'ูุฃู': 'ูุฃู', 'ูุงู': 'ูุฃู', 'ุงูุฃู': 'ุงูุขู', 'ุงูุงู': 'ุงูุขู',
    # ุงูุฃูู ุงููููุฉ
    'ุงูู': 'ุฅูู', 'ุงูู': 'ุฅูู', 'ุนูู': 'ุนูู', 'ุญุชู': 'ุญุชู',
    # ุงูุชุงุก ุงููุฑุจูุทุฉ
    'ุงูุฐู': 'ุงูุฐู', 'ุงูุชู': 'ุงูุชู', 'ูุงุฆ': 'ูุง',
    # ุงููููุงุช ุงูุดุงุฆุนุฉ
    'ุงูุถุง': 'ุฃูุถุงู', 'ุงูุธุง': 'ุฃูุถุงู', 'ุงูุถุข': 'ุฃูุถุงู',
    'ููุทุข': 'ููุท', 'ุฌุฏุง': 'ุฌุฏุงู', 'ุฌุฏุข': 'ุฌุฏุงู',
    'ูุซูุฑุง': 'ูุซูุฑุงู', 'ูุซูุฑุข': 'ูุซูุฑุงู',
    'ุฐุงูู': 'ุฐูู', 'ุงููุฉ': 'ุงููู',
    'ูุงูููู': 'ูุง ูููู', 'ูุงููุฌุฏ': 'ูุง ููุฌุฏ',
    'ูุงุชูุฌุฏ': 'ูุง ุชูุฌุฏ', 'ูุงููู': 'ูุง ููู',
    'ุงููุง': 'ุฃููุงู', 'ุซุงููุง': 'ุซุงููุงู', 'ุซุงูุซุง': 'ุซุงูุซุงู',
    'ุงุฎูุฑุง': 'ุฃุฎูุฑุงู', 'ุงุฎุต': 'ุฃุฎุต',
    'ุงู ุดุงุก ุงููู': 'ุฅู ุดุงุก ุงููู', 'ุงูุดุงุก ุงููู': 'ุฅู ุดุงุก ุงููู',
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุงูุชุตููู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def detect_category(text):
    """ุชุตููู ุงููุต ุชููุงุฆูุงู"""
    text_lower = text.lower()
    scores = defaultdict(int)
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                scores[category] += 1
    
    return max(scores, key=scores.get) if scores else 'misc'

def get_category_icon(category):
    """ุฅุฑุฌุงุน ุฃููููุฉ ุงูุชุตููู"""
    icons = {
        'medical': '๐ฅ',
        'technical': '๐ป',
        'translation': '๐',
        'ai': '๐ค',
        'programming': '๐จโ๐ป',
        'misc': '๐'
    }
    return icons.get(category, '๐')

def get_category_arabic(category):
    """ุฅุฑุฌุงุน ุงุณู ุงูุชุตููู ุจุงูุนุฑุจูุฉ"""
    names = {
        'medical': 'ุทุจู',
        'technical': 'ุชููู',
        'translation': 'ุชุฑุฌูุฉ',
        'ai': 'ุฐูุงุก ุงุตุทูุงุนู',
        'programming': 'ุจุฑูุฌุฉ',
        'misc': 'ูุชูุฑูุงุช'
    }
    return names.get(category, 'ุบูุฑ ูุตูู')

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุงูุชุตุญูุญ ุงูุฅููุงุฆู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def correct_arabic_spelling(text):
    """ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ"""
    corrected = text
    for wrong, correct in ARABIC_SPELLING_CORRECTIONS.items():
        # ุชุตุญูุญ ูุน ูุฑุงุนุงุฉ ุญุฏูุฏ ุงููููุฉ
        pattern = r'\b' + re.escape(wrong) + r'\b'
        corrected = re.sub(pattern, correct, corrected, flags=re.IGNORECASE)
    
    # ุฅุตูุงุญ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    corrected = re.sub(r'\s+', ' ', corrected)
    
    # ุฅุตูุงุญ ุนูุงูุงุช ุงูุชุฑููู
    corrected = corrected.replace(' ,', ',').replace(' .', '.').replace('ุุ', 'ุ')
    
    return corrected.strip()

def correct_file_content(content):
    """ุชุตุญูุญ ูุญุชูู ููู ูุงูู"""
    lines = content.split('\n')
    corrected_lines = []
    corrections_made = 0
    
    for line in lines:
        corrected = correct_arabic_spelling(line)
        if corrected != line:
            corrections_made += 1
        corrected_lines.append(corrected)
    
    return '\n'.join(corrected_lines), corrections_made

# ============================================
# ุฏูุงู ูุชูุฏูุฉ ููุนุงูุฌุฉ ุงููุบุฉ ุงูุนุฑุจูุฉ
# ============================================

def correct_with_languagetool(text: str, lang='ar') -> str:
    """ุชุตุญูุญ ุงููุต ุจุงุณุชุฎุฏุงู LanguageTool"""
    if not LANGTOOL_AVAILABLE:
        return text
    try:
        # ุฅูุดุงุก ุฃุฏุงุฉ (ูููู ุฌุนููุง ูุชุบูุฑุงู ุนุงูุงู ูุชุฌูุจ ุฅุนุงุฏุฉ ุงูุชุญููู)
        if not hasattr(correct_with_languagetool, "tool"):
            correct_with_languagetool.tool = language_tool_python.LanguageTool(lang)
        matches = correct_with_languagetool.tool.check(text)
        corrected = language_tool_python.utils.correct(text, matches)
        return corrected
    except Exception as e:
        print(f"โ๏ธ ุฎุทุฃ ูู LanguageTool: {e}")
        return text

def extract_stems_with_qalsadi(text: str) -> list[str]:
    """ุงุณุชุฎุฑุงุฌ ุงูุฌุฐูุฑ ุจุงุณุชุฎุฏุงู Qalsadi"""
    if not QALSADI_AVAILABLE:
        return []
    try:
        if not hasattr(extract_stems_with_qalsadi, "analex"):
            extract_stems_with_qalsadi.analex = Analex()
        analyzer = extract_stems_with_qalsadi.analex
        words = text.split()
        stems = []
        for word in words:
            # ุงูุชุญููู ุงูุตุฑูู ูุนูุฏ ูุงุฆูุฉ ูู ุงููุชุงุฆุฌ ุงููุญุชููุฉ
            analysis = analyzer.check_word(word)
            if analysis:
                # ูุฃุฎุฐ ุงูุฌุฐุฑ ูู ุฃูู ูุชูุฌุฉ (ูุฏ ุชุญุชุงุฌ ุงุฎุชูุงุฑ ุงูุฃูุณุจ)
                stem = analysis[0].get('root', word)
                stems.append(stem)
            else:
                stems.append(word)
        return stems
    except Exception as e:
        print(f"โ๏ธ ุฎุทุฃ ูู Qalsadi: {e}")
        return text.split()  # ุฅุฑุฌุงุน ุงููููุงุช ุงูุฃุตููุฉ

def enhanced_detect_category(text: str) -> str:
    """ุชุตููู ูุญุณู ุจุงุณุชุฎุฏุงู ุงูุฌุฐูุฑ ูุงููููุงุช ุงูููุชุงุญูุฉ"""
    # ุงุญุตู ุนูู ุงูุฌุฐูุฑ
    stems = extract_stems_with_qalsadi(text)
    stem_text = ' '.join(stems)
    
    # ุงุณุชุฎุฏู ุงูุชุตููู ุงูุญุงูู (ูุนุชูุฏ ุนูู ุงููููุงุช ุงูููุชุงุญูุฉ)
    category_old = detect_category(text)
    
    # ูููู ุฅุถุงูุฉ ููุทู ุฌุฏูุฏ: ุนูู ุณุจูู ุงููุซุงู ุฅุฐุง ุธูุฑุช ุฌุฐูุฑ ุทุจูุฉ ุจูุซุฑุฉ
    medical_stems = ['ุท ุจ', 'ุน ุธ ู', 'ุฌ ุฑ ุญ', 'ู ุณ ุฑ']  # ุฌุฐูุฑ ุทุจูุฉ ุดุงุฆุนุฉ
    medical_count = sum(1 for stem in medical_stems if stem in stem_text)
    
    if medical_count >= 3:
        return 'medical'
    # ูุฅูุง ุงุณุชุฎุฏู ุงูุชุตููู ุงููุฏูู
    return category_old

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุงุณุชุฎุฑุงุฌ ุงูุนูุงููู ูุงููุญุชูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def extract_title(content, filename):
    """ุงุณุชุฎุฑุงุฌ ุงูุนููุงู ูู ุงููุญุชูู"""
    lines = content.strip().split('\n')
    
    for line in lines[:20]:  # ุงูุจุญุซ ูู ุฃูู 20 ุณุทุฑ
        line = line.strip()
        if not line:
            continue
        
        for pattern in TITLE_PATTERNS:
            match = re.match(pattern, line)
            if match:
                return match.group(1).strip()
    
    # ุงุณุชุฎุฏุงู ุงุณู ุงูููู ูุนููุงู
    return pathlib.Path(filename).stem.replace('_', ' ').replace('-', ' ')

def extract_translations_from_text(text):
    """ุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงููุชุฑุฌูุฉ ูู ูุต"""
    translations = []
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
        for pattern in TRANSLATION_PATTERNS:
            match = re.match(pattern, line)
            if match:
                english = match.group(1).strip()
                arabic = match.group(2).strip()
                if len(english) > 2 and len(arabic) > 2:
                    # ุงุณุชุฎุฏุงู LanguageTool ููุชุตุญูุญ ุฅุฐุง ูุงู ููุนูุงู
                    if LANGTOOL_AVAILABLE:
                        arabic_corrected = correct_with_languagetool(arabic)
                    else:
                        arabic_corrected = correct_arabic_spelling(arabic)
                    translations.append({
                        'english': english,
                        'arabic': arabic,
                        'arabic_corrected': arabic_corrected,
                        'needs_correction': arabic != arabic_corrected,
                        'source': 'text'
                    })
                break
    return translations

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุนุงูุฌุฉ ุงููููุงุช ุงููุชุฎุตุตุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def convert_timestamp(value):
    """ุชุญููู ุงูุทูุงุจุน ุงูุฒูููุฉ ุฅูู ุชูุณูู ุนุฑุจู (ุต/ู)"""
    try:
        if PANDAS_AVAILABLE and pd.isna(value) or value is None:
            return "NULL"
        
        if isinstance(value, (int, float)):
            try:
                value = datetime.datetime.fromtimestamp(value)
            except:
                pass
        
        if isinstance(value, str):
            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                try:
                    value = datetime.datetime.strptime(value, fmt)
                    break
                except:
                    continue
        
        if isinstance(value, datetime.datetime):
            hour = value.hour
            hour_12 = 12 if hour == 0 else (12 if hour == 12 else (hour if hour < 12 else hour - 12))
            meridiem = "ุต" if hour < 12 else "ู"
            return f"{value.day:02d}/{value.month:02d}/{value.year} {hour_12}:{value.minute:02d}:{value.second:02d} {meridiem}"
        
        return str(value)
    except:
        return str(value)

def extract_db_to_text(db_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุงุนุฏุฉ ุจูุงูุงุช SQLite ุฅูู ูุต"""
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
        tables = [row[0] for row in cursor.fetchall()]
        
        if not tables:
            conn.close()
            return None, 0
        
        total_rows = 0
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ูุงุนุฏุฉ ุงูุจูุงูุงุช: {os.path.basename(db_path)}\n")
            f.write(f"ุชุงุฑูุฎ ุงูุงุณุชุฎุฑุงุฌ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            for table in tables:
                f.write(f"๐ ุฌุฏูู: {table}\n")
                f.write("-" * 40 + "\n")
                cursor.execute(f'SELECT * FROM "{table}"')
                rows = cursor.fetchall()
                cols = [d[0] for d in cursor.description]
                f.write("\t".join(cols) + "\n")
                
                for row in rows:
                    row_str = []
                    for val in row:
                        if val is None:
                            row_str.append("NULL")
                        elif isinstance(val, bytes):
                            row_str.append("<binary>")
                        else:
                            row_str.append(convert_timestamp(val))
                    f.write("\t".join(row_str) + "\n")
                f.write("\n")
                total_rows += len(rows)
            
            f.write("=" * 80 + "\n")
            f.write(f"ุนุฏุฏ ุงูุฌุฏุงูู: {len(tables)} | ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
        
        conn.close()
        return output_file, total_rows
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช: {str(e)}")
        return None, 0

def extract_excel_to_text(excel_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชูู Excel ููุต ููุธู"""
    if not PANDAS_AVAILABLE:
        print("  โ๏ธ ููุชุจุฉ pandas ุบูุฑ ูุซุจุชุฉ - ุฌุฑุจ: pip install pandas openpyxl")
        return None, 0
    
    try:
        excel_file = pd.ExcelFile(excel_path)
        total_rows = 0
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ููู Excel: {os.path.basename(excel_path)}\n")
            f.write("=" * 80 + "\n\n")
            
            for sheet in excel_file.sheet_names:
                df = pd.read_excel(excel_path, sheet_name=sheet)
                f.write(f"๐ ูุฑูุฉ: {sheet}\n")
                f.write("-" * 40 + "\n")
                f.write("\t".join(map(str, df.columns)) + "\n")
                
                for _, row in df.iterrows():
                    row_str = [str(v) if pd.notna(v) else "NULL" for v in row]
                    f.write("\t".join(row_str) + "\n")
                f.write("\n")
                total_rows += len(df)
            
            f.write("=" * 80 + "\n")
            f.write(f"ุนุฏุฏ ุงูุฃูุฑุงู: {len(excel_file.sheet_names)} | ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
        
        return output_file, total_rows
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู Excel: {str(e)}")
        return None, 0

def extract_docx_to_text(docx_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู Word"""
    if not DOCX_AVAILABLE:
        print("  โ๏ธ ููุชุจุฉ python-docx ุบูุฑ ูุซุจุชุฉ - ุฌุฑุจ: pip install python-docx")
        return None, 0
    
    try:
        doc = Document(docx_path)
        paras_count = tables_count = 0
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ูุณุชูุฏ Word: {os.path.basename(docx_path)}\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("๐ ุงูููุฑุงุช:\n" + "-" * 40 + "\n")
            for para in doc.paragraphs:
                if para.text.strip():
                    f.write(para.text + "\n")
                    paras_count += 1
            
            if doc.tables:
                f.write("\n๐ ุงูุฌุฏุงูู:\n" + "-" * 40 + "\n")
                for i, table in enumerate(doc.tables, 1):
                    f.write(f"ุฌุฏูู {i}:\n")
                    for row in table.rows:
                        f.write("\t".join(cell.text.strip() for cell in row.cells) + "\n")
                    f.write("\n")
                    tables_count += 1
            
            f.write("=" * 80 + "\n")
            f.write(f"ุงูููุฑุงุช: {paras_count} | ุงูุฌุฏุงูู: {tables_count}\n")
        
        return output_file, paras_count + tables_count
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู Word: {str(e)}")
        return None, 0

def extract_html_to_text(html_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู HTML"""
    try:
        with open(html_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        
        if HTML_AVAILABLE:
            soup = BeautifulSoup(content, 'html.parser')
            for tag in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):
                tag.decompose()
            text = soup.get_text(separator='\n', strip=True)
        else:
            text = re.sub(r'<[^>]+>', '', content)
            text = re.sub(r'\s+', ' ', text).strip()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ุตูุญุฉ ุงูููุจ: {os.path.basename(html_path)}\n")
            f.write("=" * 80 + "\n\n")
            f.write(text)
        
        return output_file, len(text.split('\n'))
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู HTML: {str(e)}")
        return None, 0

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุนุงูุฌุฉ ุงูุฃุฑุดููุงุช ูุงููุฌูุฏุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def should_ignore_path(path):
    """ุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงููุณุงุฑ"""
    for part in path.replace('\\', '/').split('/'):
        if part.startswith('.') or part in IGNORE_FOLDERS:
            return True
    return False

def is_text_content(content_bytes):
    """ูุญุต ูุง ุฅุฐุง ูุงู ุงููุญุชูู ูุตูุงู"""
    try:
        content_bytes.decode('utf-8')
        return True
    except UnicodeDecodeError:
        try:
            content_bytes.decode('latin-1')
            return True
        except:
            return False

def extract_archive_to_text(archive_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุฃุฑุดูู ุฅูู ููู ูุตู"""
    ext = pathlib.Path(archive_path).suffix.lower()
    
    if ext == '.zip':
        archive_type = 'zip'
    elif ext in TAR_EXTENSIONS or tarfile.is_tarfile(archive_path):
        archive_type = 'tar'
    elif RAR_AVAILABLE and ext == '.rar':
        archive_type = 'rar'
    else:
        return None, 0, 0
    
    processed = skipped = 0
    
    try:
        with open(output_file, 'w', encoding='utf-8') as out:
            out.write("=" * 80 + "\n")
            out.write(f"ูุญุชูู ุงูุฃุฑุดูู: {os.path.basename(archive_path)}\n")
            out.write("=" * 80 + "\n\n")
            
            if archive_type == 'zip':
                with zipfile.ZipFile(archive_path, 'r') as archive:
                    for name in sorted(archive.namelist()):
                        if name.endswith('/') or should_ignore_path(name):
                            skipped += 1
                            continue
                        file_ext = pathlib.Path(name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS or file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                            skipped += 1
                            continue
                        try:
                            with archive.open(name, 'r') as f:
                                content = f.read()
                            if not is_text_content(content):
                                skipped += 1
                                continue
                            out.write(f"ููู: {name}\n" + "-" * 40 + "\n")
                            out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                            processed += 1
                        except:
                            skipped += 1
            
            elif archive_type == 'tar':
                with tarfile.open(archive_path, 'r:*') as archive:
                    for member in archive.getmembers():
                        if member.isdir() or should_ignore_path(member.name):
                            skipped += 1
                            continue
                        file_ext = pathlib.Path(member.name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS or file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                            skipped += 1
                            continue
                        try:
                            f = archive.extractfile(member)
                            if not f:
                                continue
                            content = f.read()
                            if not is_text_content(content):
                                skipped += 1
                                continue
                            out.write(f"ููู: {member.name}\n" + "-" * 40 + "\n")
                            out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                            processed += 1
                        except:
                            skipped += 1
            
            elif archive_type == 'rar':
                with rarfile.RarFile(archive_path, 'r') as archive:
                    for name in sorted(archive.namelist()):
                        if name.endswith('/') or should_ignore_path(name):
                            skipped += 1
                            continue
                        file_ext = pathlib.Path(name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS or file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                            skipped += 1
                            continue
                        try:
                            with archive.open(name, 'r') as f:
                                content = f.read()
                            if not is_text_content(content):
                                skipped += 1
                                continue
                            out.write(f"ููู: {name}\n" + "-" * 40 + "\n")
                            out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                            processed += 1
                        except:
                            skipped += 1
            
            out.write("=" * 80 + "\n")
            out.write(f"ููุฎุต: {processed} ูุนุงูุฌ | {skipped} ูุชุฌุงูู\n")
        
        return output_file, processed, skipped
    except Exception as e:
        print(f"  โ ุฎุทุฃ: {str(e)}")
        return None, 0, 0

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุนุงูุฌ ุงููููุงุช ุงูุฑุฆูุณู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class CorpusProcessor:
    """ูุนุงูุฌ ุงููููุงุช ุงููุตูุฉ ุงูุดุงูู"""
    
    def __init__(self, input_path, output_path, batch_size=50, resume=False, use_languagetool=False, use_qalsadi=False):
        self.input_path = input_path
        self.output_path = output_path
        self.batch_size = batch_size
        self.resume = resume
        self.use_languagetool = use_languagetool
        self.use_qalsadi = use_qalsadi
        
        # ุฅุญุตุงุฆูุงุช
        self.stats = {
            'files_processed': 0,
            'files_skipped': 0,
            'translations_found': 0,
            'corrections_made': 0,
            'errors': 0,
            'start_time': None,
            'end_time': None
        }
        
        # ุงูุจูุงูุงุช ุงููุฌูุนุฉ
        self.translations = []
        self.file_index = []
        self.categories = defaultdict(list)
        self.processed_hashes = set()
        
        # ูููุงุช ุงูุชูุฏู
        self.progress_file = os.path.join(output_path, '.progress.pkl')
        self.index_file = os.path.join(output_path, 'file_index.json')
        self.translations_file = os.path.join(output_path, 'translations.csv')
        
        # ุฅูุดุงุก ุงููุฌูุฏุงุช
        self.setup_dirs()
        
        # ุงุณุชุฆูุงู ุงูุชูุฏู
        if resume:
            self.load_progress()
    
    def setup_dirs(self):
        """ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช"""
        dirs = [
            self.output_path,
            os.path.join(self.output_path, 'technical', 'linux'),
            os.path.join(self.output_path, 'technical', 'windows'),
            os.path.join(self.output_path, 'technical', 'wine'),
            os.path.join(self.output_path, 'technical', 'vpn'),
            os.path.join(self.output_path, 'medical', 'orthopedics'),
            os.path.join(self.output_path, 'medical', 'imaging'),
            os.path.join(self.output_path, 'translation'),
            os.path.join(self.output_path, 'ai'),
            os.path.join(self.output_path, 'programming'),
            os.path.join(self.output_path, 'misc'),
            os.path.join(self.output_path, 'extracted'),
            os.path.join(self.output_path, 'scripts'),
        ]
        for d in dirs:
            os.makedirs(d, exist_ok=True)
    
    def get_file_hash(self, filepath):
        """ุญุณุงุจ hash ููููู"""
        try:
            with open(filepath, 'rb') as f:
                return hashlib.md5(f.read(8192)).hexdigest()
        except:
            return None
    
    def load_progress(self):
        """ุชุญููู ุงูุชูุฏู ุงููุญููุธ"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'rb') as f:
                    data = pickle.load(f)
                    self.processed_hashes = data.get('processed_hashes', set())
                    self.stats = data.get('stats', self.stats)
                    self.translations = data.get('translations', [])
                    print(f"โ ุชู ุงุณุชุฆูุงู ุงูุชูุฏู: {len(self.processed_hashes)} ููู ูุนุงูุฌ ุณุงุจูุงู")
            except Exception as e:
                print(f"โ๏ธ ุฎุทุฃ ูู ุชุญููู ุงูุชูุฏู: {str(e)}")
    
    def save_progress(self):
        """ุญูุธ ุงูุชูุฏู"""
        try:
            with open(self.progress_file, 'wb') as f:
                pickle.dump({
                    'processed_hashes': self.processed_hashes,
                    'stats': self.stats,
                    'translations': self.translations
                }, f)
        except Exception as e:
            print(f"โ๏ธ ุฎุทุฃ ูู ุญูุธ ุงูุชูุฏู: {str(e)}")
    
    def process_file(self, filepath):
        """ูุนุงูุฌุฉ ููู ูุงุญุฏ"""
        filename = os.path.basename(filepath)
        ext = pathlib.Path(filepath).suffix.lower()
        base_name = os.path.splitext(filename)[0]
        
        # ุงูุชุญูู ูู ุงููุนุงูุฌุฉ ุงูุณุงุจูุฉ
        file_hash = self.get_file_hash(filepath)
        if file_hash and file_hash in self.processed_hashes:
            return None
        
        print(f"๐ {filename[:50]}...", end=' ')
        
        result = {
            'filename': filename,
            'path': filepath,
            'category': 'misc',
            'title': '',
            'translations': 0,
            'corrections': 0,
            'size': 0,
            'status': 'processed'
        }
        
        try:
            # ูุนุงูุฌุฉ ุญุณุจ ููุน ุงูููู
            if ext in DB_EXTENSIONS:
                output = os.path.join(self.output_path, 'extracted', f"{base_name}_db.txt")
                _, rows = extract_db_to_text(filepath, output) or (None, 0)
                result['size'] = rows
                
            elif ext in EXCEL_EXTENSIONS:
                output = os.path.join(self.output_path, 'extracted', f"{base_name}_excel.txt")
                _, rows = extract_excel_to_text(filepath, output) or (None, 0)
                result['size'] = rows
                # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
                if PANDAS_AVAILABLE:
                    try:
                        df_dict = pd.read_excel(filepath, sheet_name=None)
                        for sheet_name, df in df_dict.items():
                            for col in df.columns:
                                col_lower = str(col).lower()
                                if 'english' in col_lower or 'arabic' in col_lower:
                                    trans = extract_translations_from_text(df.to_string())
                                    self.translations.extend(trans)
                                    result['translations'] += len(trans)
                    except:
                        pass
                        
            elif ext in WORD_EXTENSIONS:
                output = os.path.join(self.output_path, 'extracted', f"{base_name}_word.txt")
                _, count = extract_docx_to_text(filepath, output) or (None, 0)
                result['size'] = count
                
            elif ext in HTML_EXTENSIONS:
                output = os.path.join(self.output_path, 'extracted', f"{base_name}_html.txt")
                _, lines = extract_html_to_text(filepath, output) or (None, 0)
                result['size'] = lines
                
            elif ext in ARCHIVE_EXTENSIONS or ext in TAR_EXTENSIONS:
                output = os.path.join(self.output_path, 'extracted', f"{base_name}_archive.txt")
                _, processed, _ = extract_archive_to_text(filepath, output) or (None, 0, 0)
                result['size'] = processed
                
            elif ext in TEXT_EXTENSIONS or ext not in BINARY_EXTENSIONS:
                # ูุฑุงุกุฉ ุงูููู ุงููุตู
                try:
                    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                        content = f.read()
                except:
                    print("ุชุฎุทู")
                    return None
                
                # ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ
                corrected_content, corrections = correct_file_content(content)
                
                # ุงุณุชุฎุฏุงู LanguageTool ููุชุตุญูุญ ุงููุชูุฏู
                if self.use_languagetool:
                    corrected_content = correct_with_languagetool(corrected_content)
                
                result['corrections'] = corrections
                self.stats['corrections_made'] += corrections
                
                # ุงุณุชุฎุฑุงุฌ ุงูุนููุงู
                result['title'] = extract_title(content, filename)
                
                # ุชุตููู ุงููุญุชูู
                if self.use_qalsadi:
                    category = enhanced_detect_category(content[:5000])
                else:
                    category = detect_category(content[:5000])
                result['category'] = category
                
                # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
                trans = extract_translations_from_text(content)
                if trans:
                    self.translations.extend(trans)
                    result['translations'] = len(trans)
                
                # ุชุญุฏูุฏ ุงููุฌูุฏ ุงููุฏู
                dest_dir = self.get_destination_dir(category, content)
                dest_path = os.path.join(dest_dir, filename)
                
                # ุชุฌูุจ ุงููุชุงุจุฉ ููู ุงููููุงุช
                counter = 1
                while os.path.exists(dest_path):
                    dest_path = os.path.join(dest_dir, f"{base_name}_{counter}{ext}")
                    counter += 1
                
                # ูุชุงุจุฉ ุงูููู ุงููุตุญุญ
                with open(dest_path, 'w', encoding='utf-8') as f:
                    f.write(corrected_content)
                
                result['size'] = len(content)
                self.categories[category].append(filepath)
            
            else:
                result['status'] = 'skipped'
                self.stats['files_skipped'] += 1
                print("ุชุฎุทู (ุซูุงุฆู)")
                return result
            
            # ุชุณุฌูู ุงููุนุงูุฌุฉ
            if file_hash:
                self.processed_hashes.add(file_hash)
            
            self.stats['files_processed'] += 1
            self.stats['translations_found'] += result['translations']
            self.file_index.append(result)
            
            print(f"โ {get_category_icon(result['category'])} {get_category_arabic(result['category'])}")
            return result
            
        except Exception as e:
            result['status'] = 'error'
            result['error'] = str(e)
            self.stats['errors'] += 1
            print(f"โ {str(e)[:30]}")
            return result
    
    def get_destination_dir(self, category, content):
        """ุชุญุฏูุฏ ุงููุฌูุฏ ุงููุฏู"""
        if category == 'technical':
            content_lower = content.lower()
            if any(kw in content_lower for kw in ['linux', 'arch', 'garuda', 'ubuntu', 'manjaro', 'zorin']):
                return os.path.join(self.output_path, 'technical', 'linux')
            elif any(kw in content_lower for kw in ['wine', 'crossover', 'bottles']):
                return os.path.join(self.output_path, 'technical', 'wine')
            elif any(kw in content_lower for kw in ['vpn', 'outline', 'psiphon']):
                return os.path.join(self.output_path, 'technical', 'vpn')
            else:
                return os.path.join(self.output_path, 'technical', 'windows')
        elif category == 'medical':
            content_lower = content.lower()
            if any(kw in content_lower for kw in ['orthopedic', 'ุนุธุงู', 'ูุณุฑ', 'ููุตู']):
                return os.path.join(self.output_path, 'medical', 'orthopedics')
            elif any(kw in content_lower for kw in ['dicom', 'ct scan', 'ุชุตููุฑ', 'ุฃุดุนุฉ']):
                return os.path.join(self.output_path, 'medical', 'imaging')
            else:
                return os.path.join(self.output_path, 'medical', 'orthopedics')
        else:
            return os.path.join(self.output_path, category)
    
    def process_all(self):
        """ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช"""
        self.stats['start_time'] = datetime.datetime.now()
        
        print("\n" + "=" * 70)
        print("๐ ุจุฏุก ูุนุงูุฌุฉ ุงููููุงุช")
        print("=" * 70)
        
        # ุฌูุน ูุงุฆูุฉ ุงููููุงุช
        files_to_process = []
        
        if os.path.isfile(self.input_path):
            files_to_process = [self.input_path]
        elif os.path.isdir(self.input_path):
            for root, dirs, files in os.walk(self.input_path):
                # ุชุฌุงูู ุงููุฌูุฏุงุช ุงููุฎููุฉ
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in IGNORE_FOLDERS]
                
                for file in files:
                    filepath = os.path.join(root, file)
                    if not should_ignore_path(filepath):
                        files_to_process.append(filepath)
        
        total_files = len(files_to_process)
        print(f"๐ ุฅุฌูุงูู ุงููููุงุช: {total_files}")
        print(f"๐ฆ ุญุฌู ุงูุฏูุนุฉ: {self.batch_size}")
        print(f"๐ ุงุณุชุฆูุงู: {'ูุนู' if self.resume else 'ูุง'}")
        print()
        
        # ุงููุนุงูุฌุฉ ุนูู ุฏูุนุงุช
        batch_count = 0
        for i in range(0, total_files, self.batch_size):
            batch = files_to_process[i:i + self.batch_size]
            batch_count += 1
            
            print(f"\n[ุฏูุนุฉ {batch_count}] ุงููููุงุช {i+1}-{min(i+self.batch_size, total_files)} ูู {total_files}")
            print("-" * 50)
            
            for filepath in batch:
                self.process_file(filepath)
            
            # ุญูุธ ุงูุชูุฏู ุจุนุฏ ูู ุฏูุนุฉ
            self.save_progress()
            self.save_translations()
            
            # ุนุฑุถ ุงูุชูุฏู
            progress = (i + len(batch)) / total_files * 100
            print(f"\n๐ ุงูุชูุฏู: {progress:.1f}% | ูุนุงูุฌ: {self.stats['files_processed']} | ุฃุฎุทุงุก: {self.stats['errors']}")
        
        # ุญูุธ ุงููุชุงุฆุฌ ุงูููุงุฆูุฉ
        self.stats['end_time'] = datetime.datetime.now()
        self.save_translations()
        self.save_file_index()
        self.save_report()
        
        # ุญุฐู ููู ุงูุชูุฏู
        if os.path.exists(self.progress_file):
            os.remove(self.progress_file)
        
        print("\n" + "=" * 70)
        print("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
        print("=" * 70)
        self.print_summary()
    
    def save_translations(self):
        """ุญูุธ ุงูุชุฑุฌูุงุช ูู CSV"""
        if not self.translations:
            return
        
        # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
        unique = []
        seen = set()
        for t in self.translations:
            key = (t['english'].lower(), t['arabic'])
            if key not in seen:
                seen.add(key)
                unique.append(t)
        
        with open(self.translations_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['English', 'Arabic (Original)', 'Arabic (Corrected)', 'Needs Correction'])
            for t in unique:
                writer.writerow([t['english'], t['arabic'], t['arabic_corrected'], 
                               'Yes' if t['needs_correction'] else 'No'])
    
    def save_file_index(self):
        """ุญูุธ ููุฑุณ ุงููููุงุช"""
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.file_index, f, ensure_ascii=False, indent=2)
    
    def save_report(self):
        """ุญูุธ ุชูุฑูุฑ ุงููุนุงูุฌุฉ"""
        report_file = os.path.join(self.output_path, 'PROCESSING_REPORT.md')
        
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds() if self.stats['end_time'] else 0
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ุชูุฑูุฑ ูุนุงูุฌุฉ ุงููููุงุช\n\n")
            f.write(f"**ุชุงุฑูุฎ ุงููุนุงูุฌุฉ:** {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S') if self.stats['start_time'] else 'N/A'}\n\n")
            f.write(f"**ูุฏุฉ ุงููุนุงูุฌุฉ:** {duration:.1f} ุซุงููุฉ\n\n")
            
            f.write("## ๐ ุฅุญุตุงุฆูุงุช\n\n")
            f.write("| ุงูุจูุฏ | ุงูุนุฏุฏ |\n|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงููุนุงูุฌุฉ | {self.stats['files_processed']} |\n")
            f.write(f"| ุงููููุงุช ุงููุชุฌุงููุฉ | {self.stats['files_skipped']} |\n")
            f.write(f"| ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | {self.stats['translations_found']} |\n")
            f.write(f"| ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ | {self.stats['corrections_made']} |\n")
            f.write(f"| ุงูุฃุฎุทุงุก | {self.stats['errors']} |\n\n")
            
            f.write("## ๐ ุงูุชุตููู\n\n")
            for cat, files in self.categories.items():
                f.write(f"### {get_category_icon(cat)} {get_category_arabic(cat)} ({len(files)})\n\n")
                for file in files[:20]:  # ุฃูู 20 ููู ููุท
                    f.write(f"- {os.path.basename(file)}\n")
                if len(files) > 20:
                    f.write(f"- ... ู {len(files) - 20} ููู ุขุฎุฑ\n")
                f.write("\n")
            
            f.write("## ๐ ุงููููุงุช ุงููุนุงูุฌุฉ (ุขุฎุฑ 50)\n\n")
            for entry in self.file_index[-50:]:
                status_icon = 'โ' if entry['status'] == 'processed' else 'โ'
                f.write(f"{status_icon} {entry['filename']} - {get_category_arabic(entry['category'])}\n")
        
        print(f"\n๐ ุชูุฑูุฑ: {report_file}")
    
    def print_summary(self):
        """ุนุฑุถ ููุฎุต"""
        print(f"\n๐ ุงูุฅุญุตุงุฆูุงุช ุงูููุงุฆูุฉ:")
        print(f"   โข ุงููููุงุช ุงููุนุงูุฌุฉ: {self.stats['files_processed']}")
        print(f"   โข ุงููููุงุช ุงููุชุฌุงููุฉ: {self.stats['files_skipped']}")
        print(f"   โข ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ: {self.stats['translations_found']}")
        print(f"   โข ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ: {self.stats['corrections_made']}")
        print(f"   โข ุงูุฃุฎุทุงุก: {self.stats['errors']}")
        
        print(f"\n๐ ุงูุชุตููู:")
        for cat, files in self.categories.items():
            print(f"   {get_category_icon(cat)} {get_category_arabic(cat)}: {len(files)} ููู")
        
        print(f"\n๐ ุงููููุงุช ุงููุงุชุฌุฉ:")
        print(f"   โข ููุฑุณ ุงููููุงุช: {self.index_file}")
        print(f"   โข ุงูุชุฑุฌูุงุช: {self.translations_file}")
        print(f"   โข ุงูุชูุฑูุฑ: {os.path.join(self.output_path, 'PROCESSING_REPORT.md')}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุทุฉ ุงูุฏุฎูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def print_dependencies():
    """ุนุฑุถ ุญุงูุฉ ุงูููุชุจุงุช"""
    print("\n๐ฆ ุญุงูุฉ ุงูููุชุจุงุช:")
    print(f"  {'โ' if PANDAS_AVAILABLE else 'โ'} pandas (Excel) - {'ูุซุจุช' if PANDAS_AVAILABLE else 'pip install pandas openpyxl'}")
    print(f"  {'โ' if DOCX_AVAILABLE else 'โ'} python-docx (Word) - {'ูุซุจุช' if DOCX_AVAILABLE else 'pip install python-docx'}")
    print(f"  {'โ' if HTML_AVAILABLE else 'โ๏ธ'} beautifulsoup4 (HTML) - {'ูุซุจุช' if HTML_AVAILABLE else 'pip install beautifulsoup4'}")
    print(f"  {'โ' if RAR_AVAILABLE else 'โ๏ธ'} rarfile (RAR) - {'ูุซุจุช' if RAR_AVAILABLE else 'pip install rarfile'}")
    print(f"  {'โ' if LANGTOOL_AVAILABLE else 'โ๏ธ'} language-tool-python (LanguageTool) - {'ูุซุจุช' if LANGTOOL_AVAILABLE else 'pip install language-tool-python'}")
    print(f"  {'โ' if QALSADI_AVAILABLE else 'โ๏ธ'} qalsadi (ุชุญููู ุตุฑูู) - {'ูุซุจุช' if QALSADI_AVAILABLE else 'pip install qalsadi'}")
    print()

def main():
    parser = argparse.ArgumentParser(
        description='ูุนุงูุฌ ุงููููุงุช ุงููุตูุฉ ุงูุฃูููุงูู ุงูุดุงูู',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  # ูุนุงูุฌุฉ ูุฌูุฏ ูุงูู
  python corpus_processor_offline.py -i ./files -o ./processed
  
  # ุงุณุชุฆูุงู ูุนุงูุฌุฉ ูุชูููุฉ
  python corpus_processor_offline.py -i ./files -o ./processed --resume
  
  # ุชุญุฏูุฏ ุญุฌู ุงูุฏูุนุฉ
  python corpus_processor_offline.py -i ./files -o ./processed --batch-size 100
  
  # ูุนุงูุฌุฉ ููู ูุงุญุฏ
  python corpus_processor_offline.py -i document.txt -o ./output
        """
    )
    
    parser.add_argument('-i', '--input', required=True, help='ูุณุงุฑ ุงูููู ุฃู ุงููุฌูุฏ ุงููุฏุฎู')
    parser.add_argument('-o', '--output', default='./corpus_processed', help='ูุณุงุฑ ุงูุฅุฎุฑุงุฌ (ุงูุชุฑุงุถู: ./corpus_processed)')
    parser.add_argument('-b', '--batch-size', type=int, default=50, help='ุญุฌู ุงูุฏูุนุฉ (ุงูุชุฑุงุถู: 50)')
    parser.add_argument('-r', '--resume', action='store_true', help='ุงุณุชุฆูุงู ูุนุงูุฌุฉ ุณุงุจูุฉ')
    parser.add_argument('--use-languagetool', action='store_true',
                        help='ุชูุนูู LanguageTool ููุชุตุญูุญ ุงููุชูุฏู')
    parser.add_argument('--use-qalsadi', action='store_true',
                        help='ุชูุนูู Qalsadi ููุชุญููู ุงูุตุฑูู ูุชุญุณูู ุงูุชุตููู')
    
    args = parser.parse_args()
    
    # ุงูุชุญูู ูู ุงููุณุงุฑ
    if not os.path.exists(args.input):
        print(f"โ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {args.input}")
        sys.exit(1)
    
    # ุนุฑุถ ุญุงูุฉ ุงูููุชุจุงุช
    print_dependencies()
    
    # ุจุฏุก ุงููุนุงูุฌุฉ
    processor = CorpusProcessor(
        input_path=args.input,
        output_path=args.output,
        batch_size=args.batch_size,
        resume=args.resume,
        use_languagetool=args.use_languagetool,
        use_qalsadi=args.use_qalsadi
    )
    
    processor.process_all()
    
    print(f"\n๐ก ุงูุฎุทูุฉ ุงูุชุงููุฉ:")
    print(f"   1. ุฑุงุฌุน ุงููููุงุช ูู: {args.output}")
    print(f"   2. ุชุญูู ูู ุงูุชูุฑูุฑ: {os.path.join(args.output, 'PROCESSING_REPORT.md')}")
    print(f"   3. ุฑุงุฌุน ุงูุชุฑุฌูุงุช: {os.path.join(args.output, 'translations.csv')}")

if __name__ == '__main__':
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/enhance_translations.py
----------------------------------------
#!/usr/bin/env python3
"""
ุชุญุณูู ุฌูุฏุฉ ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ ุจุงุณุชุฎุฏุงู LanguageTool ูุงูุชุญูู ูู ุงูุชูุงุฒู.
"""
import pandas as pd
import language_tool_python
import argparse
import os
import sys

def enhance_translations(csv_path, output_path):
    """
    ุชุญุณูู ุฌูุฏุฉ ุงูุชุฑุฌูุงุช ูู ููู CSV
    """
    try:
        df = pd.read_csv(csv_path)
        tool = language_tool_python.LanguageTool('ar')
        
        # ุงูุชุญูู ูู ูุฌูุฏ ุงูุฃุนูุฏุฉ ุงููุทููุจุฉ
        required_columns = ['English', 'Arabic (Original)', 'Arabic (Corrected)']
        for col in required_columns:
            if col not in df.columns:
                print(f"โ ุงูุนููุฏ '{col}' ุบูุฑ ููุฌูุฏ ูู ุงูููู")
                return
        
        # ุชุตุญูุญ ุงูุนุฑุจูุฉ ุจุงุณุชุฎุฏุงู LanguageTool
        print("๐ง ุฌุงุฑู ุชุญุณูู ุงูุชุฑุฌูุงุช...")
        corrected_arabic = []
        for idx, arabic_text in enumerate(df['Arabic (Original)']):
            try:
                corrected = language_tool_python.utils.correct(str(arabic_text), tool.check(str(arabic_text)))
                corrected_arabic.append(corrected)
                if idx % 100 == 0:
                    print(f"  โ ุชู ูุนุงูุฌุฉ {idx} ุชุฑุฌูุฉ...")
            except Exception as e:
                print(f"  โ๏ธ ุฎุทุฃ ูู ุชุญุณูู ุงูุชุฑุฌูุฉ {idx}: {e}")
                corrected_arabic.append(str(arabic_text))  # ุงุณุชุฎุฏุงู ุงููุต ุงูุฃุตูู ูู ุญุงู ุงูุฎุทุฃ
        
        # ุฅุถุงูุฉ ุงูุฃุนูุฏุฉ ุงููุญุณูุฉ
        df['Arabic (Enhanced)'] = corrected_arabic
        df['Enhancement Made'] = df['Arabic (Original)'] != df['Arabic (Enhanced)']
        
        # ุญูุธ ุงููุชุงุฆุฌ
        df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"โ ุชู ุชุญุณูู {len(df)} ุชุฑุฌูุฉ ูู {output_path}")
        
        # ุนุฑุถ ุงูุฅุญุตุงุฆูุงุช
        enhancements_made = df['Enhancement Made'].sum()
        print(f"๐ ุนุฏุฏ ุงูุชุฑุฌูุงุช ุงูุชู ุชูุช ุชุญุณูููุง: {enhancements_made}")
        
    except Exception as e:
        print(f"โ ุฎุทุฃ ุฃุซูุงุก ุชุญุณูู ุงูุชุฑุฌูุงุช: {e}")

def main():
    parser = argparse.ArgumentParser(
        description='ุชุญุณูู ุฌูุฏุฉ ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ ุจุงุณุชุฎุฏุงู LanguageTool',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  # ุชุญุณูู ููู ุชุฑุฌูุงุช
  python enhance_translations.py -i ./translations.csv -o ./translations_enhanced.csv
  
  # ุชุญุณูู ููู ุชุฑุฌูุงุช ูุน ุชูุฑูุฑ
  python enhance_translations.py -i ./output/translations.csv -o ./output/translations_enhanced.csv
        """
    )
    
    parser.add_argument('-i', '--input', required=True, help='ูุณุงุฑ ููู ุงูุชุฑุฌูุงุช ุงููุฏุฎู')
    parser.add_argument('-o', '--output', required=True, help='ูุณุงุฑ ููู ุงูุชุฑุฌูุงุช ุงููุฎุฑุฌ')
    
    args = parser.parse_args()
    
    # ุงูุชุญูู ูู ูุฌูุฏ ุงูููู ุงููุฏุฎู
    if not os.path.exists(args.input):
        print(f"โ ุงูููู ุบูุฑ ููุฌูุฏ: {args.input}")
        sys.exit(1)
    
    # ุงูุชุญูู ูู ููุชุจุฉ LanguageTool
    try:
        import language_tool_python
    except ImportError:
        print("โ ููุชุจุฉ language-tool-python ุบูุฑ ูุซุจุชุฉ")
        print("  ูู ุจุงูุชุซุจูุช ุจุงุณุชุฎุฏุงู: pip install language-tool-python")
        sys.exit(1)
    
    # ุชุญุณูู ุงูุชุฑุฌูุงุช
    enhance_translations(args.input, args.output)

if __name__ == '__main__':
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/intelligent_text_processor.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
  ูุนุงูุฌ ุงููุตูุต ุงูุฐูู ุงูุดุงูู - Intelligent Text Processor
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุงููุฏู:
  ูุนุงูุฌุฉ ูููุงุช ูุตูุฉ ุถุฎูุฉ (30+ GB) ุฃูููุงูู ูุน:
  - ุชูุณูู ุฐูู ูููููุงุช ุงููุจูุฑุฉ
  - ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช (ุฅูุฌููุฒู-ุนุฑุจู)
  - ุชุตููู ุงููุญุชูู (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ูุฑุงุฌุน)
  - ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช (MinHash LSH)
  - ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ
  - ุชูููู ุงูุฌูุฏุฉ
  - ุญูุธ ุงูุชูุฏู (resumable)

ุงูุงุณุชุฎุฏุงู:
  python intelligent_text_processor.py --input /path/to/files --output /path/to/output
  
  ุฃูุงูุฑ ุฅุถุงููุฉ:
  python intelligent_text_processor.py --input ./data --output ./processed --mode all
  python intelligent_text_processor.py --input ./data --output ./processed --mode classify
  python intelligent_text_processor.py --status ./processed  # ุนุฑุถ ุงูุชูุฏู
  python intelligent_text_processor.py --resume ./processed  # ุงุณุชููุงู ุงููุนุงูุฌุฉ

ุงููุชุทูุจุงุช:
  pip install datasketch tqdm chardet
"""

import os
import re
import csv
import json
import pickle
import hashlib
import argparse
import signal
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Set, Generator, Any
from collections import defaultdict
from dataclasses import dataclass, field, asdict
import subprocess

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงููุญุงููุฉ ูุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("โ๏ธ tqdm ุบูุฑ ูุซุจุช. ุงูุชุซุจูุช: pip install tqdm")

try:
    from datasketch import MinHash, MinHashLSH
    HAS_DATASKETCH = True
except ImportError:
    HAS_DATASKETCH = False
    print("โ๏ธ datasketch ุบูุฑ ูุซุจุช. ุงูุชุซุจูุช: pip install datasketch")

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False
    print("โ๏ธ chardet ุบูุฑ ูุซุจุช. ุงูุชุซุจูุช: pip install chardet")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงูุซูุงุจุช ูุงูุฅุนุฏุงุฏุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

# ุฃุญุฌุงู ุงููููุงุช
KB = 1024
MB = 1024 * KB
GB = 1024 * MB

# ุฅุนุฏุงุฏุงุช ุงููุนุงูุฌุฉ
MAX_SEGMENT_SIZE = 50 * KB  # ุญุฌู ุงูููุทุน ุงูุฃูุตู
MIN_SEGMENT_SIZE = 500      # ุญุฌู ุงูููุทุน ุงูุฃุฏูู
SIMILARITY_THRESHOLD = 0.85  # ุนุชุจุฉ ุงูุชุดุงุจู ููุชูุฑุงุฑุงุช
BATCH_SIZE = 1000           # ุญุฌู ุงูุฏูุนุฉ ูููุนุงูุฌุฉ

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

SPELLING_CORRECTIONS = {
    # ุฃุฎุทุงุก ุงูููุฒุฉ
    'ุงูุดุงุก': 'ุฅูุดุงุก', 'ุงูุงู': 'ุงูุขู', 'ุงูู': 'ุฅูู', 'ุงุฐุง': 'ุฅุฐุง',
    'ุงูุง': 'ุฅูุง', 'ุงู': 'ุฃู', 'ุงูู': 'ุฃูู', 'ุงูุถุง': 'ุฃูุถุงู',
    'ุงูุถุงู': 'ุฃูุถุงู', 'ุงุดูุงุก': 'ุฃุดูุงุก', 'ุงุดูุงุก': 'ุฃุดูุงุก',
    
    # ุฃุฎุทุงุก ุดุงุฆุนุฉ
    'ุฐุงูู': 'ุฐูู', 'ูุงุฐุง': 'ูุฐุง', 'ูุงุฐู': 'ูุฐู', 'ูุงุฐุฉ': 'ูุฐู',
    'ุงููุฏ': 'ุฃููุฏ', 'ุงุณุชุทูุน': 'ุฃุณุชุทูุน', 'ุงุณุชุฎุฏู': 'ุฃุณุชุฎุฏู',
    'ุงูุณ': 'ุฃูุณ', 'ุงูุฑ': 'ุฃูุฑ', 'ุงุณุฆูุฉ': 'ุฃุณุฆูุฉ',
    
    # ุฃุฎุทุงุก ุงูุชุงุก ุงููุฑุจูุทุฉ
    'ุซูุฑุฉ': 'ุซูุฑุฉ', 'ูููุงุช': 'ูููุงุช', 'ูุดููุงุช': 'ูุดููุงุช',
    
    # ุฃุฎุทุงุก ุงููุงุก ูุงูุฃูู ุงูููุตูุฑุฉ
    'ูุญู': 'ูุญู', 'ุงุญู': 'ุฃุญูู', 'ุนูู': 'ุนูู', 'ูุฏู': 'ูุฏู',
    
    # ุฃุฎุทุงุก ุงูุชูููู
    'ุงุณููุจุง': 'ุฃุณููุจุงู', 'ุทุฑููุฉู': 'ุทุฑููุฉู', 'ูุซูุฑุง': 'ูุซูุฑุงู',
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูููุงุช ุงูุชุตููู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

CATEGORY_KEYWORDS = {
    'medical': {
        'ar': ['ุทุจู', 'ุทุจ', 'ุนุธุงู', 'ุฌุฑุงุญุฉ', 'ูุณุชุดูู', 'ุทุจูุจ', 'ุนูุงุฌ', 'ูุฑุถ',
               'ูุณุฑ', 'ุบุถุฑูู', 'ููุตู', 'ุฑูุจุฉ', 'ูุฎุฐ', 'ุนููุฏ ููุฑู', 'ุฃุดุนุฉ',
               'ูุณุชูุฒูุงุช', 'ุนุธููุฉ', 'ุชูุชุงูููู', 'ุบุฑุณุงุช', 'ูุณุงููุฑ', 'ุจุฑุงุบู',
               'ููุงุตู', 'ุงุตุทูุงุนูุฉ', 'ุบุงูุงููู', 'ุณููุฏ', 'ุตูุงุฆุญ', 'ุจุฑูุดุงุช',
               'ุนูุงููุฑ', 'ุฃุฏููุฉ', 'ุชุดุฎูุต', 'ุฃุนุฑุงุถ', 'ูุฑุถู', 'ุณุฑุทุงู', 'ูุฑู',
               'ุงูุชูุงุจ', 'ุนุฏูู', 'ุจูุชูุฑูุง', 'ููุฑูุณ', 'ููุงุญ', 'ุชุญููู', 'ุฏู'],
        'en': ['medical', 'orthopedic', 'surgery', 'hospital', 'doctor', 'treatment',
               'fracture', 'bone', 'joint', 'knee', 'hip', 'spine', 'implant',
               'screw', 'plate', 'titanium', 'prosthesis', 'dhs', 'pin', 'nail',
               'patient', 'diagnosis', 'symptom', 'disease', 'cancer', 'tumor',
               'infection', 'bacteria', 'virus', 'vaccine', 'blood', 'drug']
    },
    'technical': {
        'ar': ['ููููุณ', 'ูููุฏูุฒ', 'ุจุฑูุงูุฌ', 'ุชุซุจูุช', 'ุญุฒูุฉ', 'ูุธุงู', 'ุณูุฑูุฑ',
               'ููุฏ', 'ุจุฑูุฌุฉ', 'ุณูุฑูุจุช', 'ุฃูุฑ', 'ุทุฑููุฉ', 'ูุณุชุฎุฏู', 'ููู',
               'ูุฌูุฏ', 'git', 'github', 'python', 'bash', 'shell', 'kde',
               'gnome', 'plasma', 'arch', 'ูุงูุฌุงุฑู', 'ุบุงุฑูุฏุง', 'ุฃูุจููุชู',
               'docker', 'vm', 'ุขูุฉ', 'ุงูุชุฑุงุถูุฉ', 'vpn', 'proxy', 'ุดุจูุฉ',
               'ุฅูุชุฑูุช', 'ุฃูุงู', 'ุญูุงูุฉ', 'ุฎุงุฏู', 'ุนููู', 'ูุงุนุฏุฉ ุจูุงูุงุช'],
        'en': ['linux', 'windows', 'software', 'install', 'package', 'system',
               'server', 'code', 'programming', 'script', 'command', 'terminal',
               'user', 'file', 'folder', 'directory', 'kernel', 'desktop',
               'environment', 'window', 'manager', 'docker', 'kubernetes',
               'database', 'api', 'http', 'server', 'client', 'network', 'security']
    },
    'translation': {
        'ar': ['ุชุฑุฌูุฉ', 'ูุชุฑุฌู', 'ูุบุฉ', 'ุนุฑุจู', 'ุฅูุฌููุฒู', 'ูุงููุณ', 'ูุตุทูุญ',
               'ูุนูู', 'ูููุฉ', 'ุฌููุฉ', 'ูุต', 'ูุซููุฉ', 'ูุบูู', 'ุซูุงุฆู', 'ุชุนุฑูุจ',
               'ูุบุงุช', 'ุชุฑุฌู', 'ููุฑุฏุงุช', 'ุงุตุทูุงุญ'],
        'en': ['translation', 'translator', 'language', 'arabic', 'english',
               'dictionary', 'term', 'meaning', 'word', 'sentence', 'text',
               'document', 'bilingual', 'vocabulary', 'phrase', 'translate']
    },
    'reference': {
        'ar': ['ููุฑุณ', 'ูุฑุฌุน', 'ุฏููู', 'ูุงุฆูุฉ', 'ุฌุฏูู', 'ุจูุงูุงุช', 'ุฅุญุตุงุฆูุงุช',
               'ุชูุฑูุฑ', 'ููุฎุต', 'ุชุตููู', 'ูููุน', 'ุฑุงุจุท', 'ูุตุฏุฑ', 'ูุฑุงุฌุน',
               'ููุชุจุฉ', 'ูุชุงุจ', 'ุจุญุซ', 'ุฏุฑุงุณุฉ', 'ููุงู'],
        'en': ['index', 'reference', 'guide', 'list', 'table', 'data', 'statistics',
               'report', 'summary', 'classification', 'link', 'source', 'library',
               'book', 'research', 'study', 'article', 'documentation']
    },
    'code': {
        'ar': ['ุฏุงูุฉ', 'ูุชุบูุฑ', 'ููุงุณ', 'ูุงุฆู', 'ูุตูููุฉ', 'ุญููุฉ', 'ุดุฑุท',
               'ุงุณุชูุฑุงุฏ', 'ุชุตุฏูุฑ', 'ูุธููุฉ', 'ุจุฑูุฌูุฉ', 'ุฎูุงุฑุฒููุฉ'],
        'en': ['function', 'variable', 'class', 'object', 'array', 'loop',
               'condition', 'import', 'export', 'method', 'algorithm', 'api',
               'def', 'class', 'import', 'from', 'return', 'if', 'else', 'for']
    }
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฃููุงุท ุงูุชุนุฑู ุนูู ุงููุญุชูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

PATTERNS = {
    'translation_pair': [
        # English | Arabic
        r'^([A-Za-z][A-Za-z0-9\s\-_,\.]{2,50})\s*[\|\-โโ]\s*([\u0600-\u06FF][\u0600-\u06FF\s\-_,\.]{2,100})$',
        # "English": "Arabic"
        r'["\']([A-Za-z][^"\']{2,50})["\']\s*:\s*["\']([\u0600-\u06FF][^"\']{2,100})["\']',
        # Markdown table
        r'\|\s*([A-Za-z][A-Za-z0-9\s\-_,\.]{2,50})\s*\|\s*([\u0600-\u06FF][\u0600-\u06FF\s\-_,\.]{2,100})\s*\|',
    ],
    'code_block': r'```[\s\S]*?```',
    'url': r'https?://[^\s]+',
    'email': r'[\w\.-]+@[\w\.-]+\.\w+',
    'number': r'\d+(?:\.\d+)?',
    'arabic_text': r'[\u0600-\u06FF]+',
    'english_text': r'[A-Za-z]+',
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุฆุงุช ุงูุจูุงูุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

@dataclass
class TextSegment:
    """ููุทุน ูุตู"""
    id: str
    content: str
    source_file: str
    start_line: int
    end_line: int
    language: str  # 'ar', 'en', 'mixed'
    category: str = 'misc'
    confidence: float = 0.0
    quality_score: float = 0.0
    is_duplicate: bool = False
    translations: List[Dict] = field(default_factory=list)
    corrections: List[str] = field(default_factory=list)

@dataclass
class ProcessingStats:
    """ุฅุญุตุงุฆูุงุช ุงููุนุงูุฌุฉ"""
    total_files: int = 0
    processed_files: int = 0
    total_segments: int = 0
    unique_segments: int = 0
    duplicates_found: int = 0
    translations_extracted: int = 0
    corrections_made: int = 0
    categories: Dict[str, int] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    start_time: str = ''
    end_time: str = ''
    current_file: str = ''

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุณุงุนุฏุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def generate_id(content: str) -> str:
    """ุฅูุดุงุก ูุนุฑู ูุฑูุฏ ูููุญุชูู"""
    return hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()[:16]


def detect_encoding(file_path: Path) -> str:
    """ูุดู ุชุฑููุฒ ุงูููู"""
    if HAS_CHARDET:
        with open(file_path, 'rb') as f:
            raw = f.read(10000)
            result = chardet.detect(raw)
            return result.get('encoding', 'utf-8')
    return 'utf-8'


def is_arabic(text: str) -> bool:
    """ุงูุชุญูู ูู ูุฌูุฏ ูุต ุนุฑุจู"""
    return bool(re.search(PATTERNS['arabic_text'], text))


def is_english(text: str) -> bool:
    """ุงูุชุญูู ูู ูุฌูุฏ ูุต ุฅูุฌููุฒู"""
    return bool(re.search(PATTERNS['english_text'], text))


def detect_language(text: str) -> str:
    """ูุดู ูุบุฉ ุงููุต"""
    has_ar = is_arabic(text)
    has_en = is_english(text)
    
    if has_ar and has_en:
        return 'mixed'
    elif has_ar:
        return 'ar'
    elif has_en:
        return 'en'
    return 'other'


def normalize_text(text: str) -> str:
    """ุชุทุจูุน ุงููุต"""
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    text = re.sub(r'\s+', ' ', text.strip())
    # ุชูุญูุฏ ุงูุฃุณุทุฑ
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    return text


def correct_spelling(text: str) -> Tuple[str, List[str]]:
    """ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ"""
    corrections = []
    corrected = text
    
    for wrong, right in SPELLING_CORRECTIONS.items():
        # ุงูุจุญุซ ุนู ุงููููุฉ ูุงููุฉ
        pattern = r'\b' + re.escape(wrong) + r'\b'
        if re.search(pattern, corrected):
            corrected = re.sub(pattern, right, corrected)
            corrections.append(f"{wrong} โ {right}")
    
    return corrected, corrections


def calculate_quality_score(text: str) -> float:
    """ุญุณุงุจ ููุงุท ุฌูุฏุฉ ุงููุต"""
    if not text or len(text) < 10:
        return 0.0
    
    score = 1.0
    
    # ุทูู ุงููุต (ุฃูุถู ุจูู 100-10000 ุญุฑู)
    length = len(text)
    if length < 50:
        score *= 0.5
    elif length > 50000:
        score *= 0.7
    
    # ูุณุจุฉ ุงูุฑููุฒ ุบูุฑ ุงููููุฏุฉ
    symbol_count = len(re.findall(r'[^\w\s\u0600-\u06FF]', text))
    symbol_ratio = symbol_count / max(len(text), 1)
    if symbol_ratio > 0.3:
        score *= (1 - symbol_ratio)
    
    # ูุฌูุฏ garbage characters
    garbage = len(re.findall(r'[๏ฟฝ\x00-\x08\x0B\x0C\x0E-\x1F]', text))
    if garbage > 0:
        score *= max(0.1, 1 - (garbage / 100))
    
    # ูุณุจุฉ ุงููููุงุช ุงููุชูุฑุฑุฉ
    words = text.lower().split()
    if len(words) > 10:
        unique_ratio = len(set(words)) / len(words)
        if unique_ratio < 0.3:
            score *= unique_ratio
    
    return max(0.0, min(1.0, round(score, 3)))


def classify_text(text: str) -> Tuple[str, float]:
    """ุชุตููู ุงููุต ุจูุงุกู ุนูู ุงููููุงุช ุงูููุชุงุญูุฉ"""
    text_lower = text.lower()
    scores = defaultdict(float)
    
    for category, langs in CATEGORY_KEYWORDS.items():
        for lang, keywords in langs.items():
            for kw in keywords:
                if kw.lower() in text_lower:
                    scores[category] += 1
    
    if not scores:
        return 'misc', 0.0
    
    best_category = max(scores, key=scores.get)
    total_keywords = sum(len(kws) for kws in CATEGORY_KEYWORDS.get(best_category, {}).values())
    confidence = min(scores[best_category] / max(total_keywords, 1), 1.0)
    
    return best_category, round(confidence, 3)


def extract_translation_pairs(text: str) -> List[Dict]:
    """ุงุณุชุฎุฑุงุฌ ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ ูู ุงููุต"""
    pairs = []
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        
        for pattern in PATTERNS['translation_pair']:
            matches = re.findall(pattern, line, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    eng = normalize_text(match[0])
                    ara = normalize_text(match[1])
                    
                    if len(eng) > 2 and len(ara) > 2:
                        corrected_ara, corrections = correct_spelling(ara)
                        category, confidence = classify_text(f"{eng} {ara}")
                        
                        pairs.append({
                            'english': eng,
                            'arabic_original': ara,
                            'arabic_corrected': corrected_ara,
                            'corrections': corrections,
                            'category': category,
                            'confidence': confidence
                        })
    
    return pairs


def create_minhash(text: str, num_perm: int = 128) -> Optional['MinHash']:
    """ุฅูุดุงุก MinHash ูููุต"""
    if not HAS_DATASKETCH:
        return None
    
    # ุชูุณูู ุงููุต ุฅูู ูููุงุช
    words = re.findall(r'\w+', text.lower())
    if not words:
        return None
    
    mh = MinHash(num_perm=num_perm)
    for word in words:
        mh.update(word.encode('utf-8'))
    
    return mh


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงููุนุงูุฌ ุงูุฑุฆูุณู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class IntelligentTextProcessor:
    """ูุนุงูุฌ ุงููุตูุต ุงูุฐูู ุงูุดุงูู"""
    
    def __init__(self, input_dir: str, output_dir: str, mode: str = 'all'):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.mode = mode
        
        # ุฅูุดุงุก ูุฌูุฏุงุช ุงูุฅุฎุฑุงุฌ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'segments').mkdir(exist_ok=True)
        (self.output_dir / 'translations').mkdir(exist_ok=True)
        (self.output_dir / 'reports').mkdir(exist_ok=True)
        
        # ุงูุฅุญุตุงุฆูุงุช
        self.stats = ProcessingStats()
        self.stats.start_time = datetime.now().isoformat()
        
        # ุงูุชูุฏู ุงููุญููุธ
        self.progress_file = self.output_dir / 'progress.pkl'
        self.stats_file = self.output_dir / 'stats.json'
        self.seen_hashes: Set[str] = set()
        self.minhash_lsh = None
        
        # ุฅุนุฏุงุฏ LSH ููุชูุฑุงุฑุงุช
        if HAS_DATASKETCH:
            self.minhash_lsh = MinHashLSH(
                threshold=SIMILARITY_THRESHOLD,
                num_perm=128
            )
        
        # ูุนุงูุฌุฉ ุงูุฅุดุงุฑุงุช ููุฅููุงู ุงูุขูู
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self._interrupted = False
    
    def _signal_handler(self, signum, frame):
        """ูุนุงูุฌ ุฅุดุงุฑุงุช ุงูุฅููุงู"""
        print("\nโ๏ธ ุชู ุงุณุชูุงู ุฅุดุงุฑุฉ ุงูุฅููุงู. ุฌุงุฑู ุญูุธ ุงูุชูุฏู...")
        self._interrupted = True
        self.save_progress()
        sys.exit(0)
    
    def save_progress(self):
        """ุญูุธ ุงูุชูุฏู"""
        with open(self.progress_file, 'wb') as f:
            pickle.dump({
                'seen_hashes': self.seen_hashes,
                'stats': asdict(self.stats)
            }, f)
        
        with open(self.stats_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(self.stats), f, ensure_ascii=False, indent=2)
        
        print(f"๐พ ุชู ุญูุธ ุงูุชูุฏู ูู: {self.output_dir}")
    
    def load_progress(self) -> bool:
        """ุชุญููู ุงูุชูุฏู ุงููุญููุธ"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'rb') as f:
                    data = pickle.load(f)
                    self.seen_hashes = data.get('seen_hashes', set())
                    saved_stats = data.get('stats', {})
                    for key, value in saved_stats.items():
                        if hasattr(self.stats, key):
                            setattr(self.stats, key, value)
                print(f"๐ ุชู ุชุญููู ุงูุชูุฏู: {self.stats.processed_files} ููู ูุนุงูุฌ")
                return True
            except Exception as e:
                print(f"โ๏ธ ุฎุทุฃ ูู ุชุญููู ุงูุชูุฏู: {e}")
        return False
    
    def get_files(self) -> List[Path]:
        """ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงููููุงุช"""
        extensions = {'.txt', '.md', '.csv', '.json', '.log', '.py', '.js', '.html'}
        files = []
        
        for ext in extensions:
            files.extend(self.input_dir.rglob(f'*{ext}'))
        
        return sorted(set(files))
    
    def segment_file(self, file_path: Path) -> Generator[TextSegment, None, None]:
        """ุชูุณูู ุงูููู ุฅูู ููุงุทุน"""
        try:
            encoding = detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read()
        except Exception as e:
            self.stats.errors.append(f"ุฎุทุฃ ูู ูุฑุงุกุฉ {file_path.name}: {e}")
            return
        
        lines = content.split('\n')
        current_segment = []
        current_start = 1
        segment_idx = 0
        
        for i, line in enumerate(lines, 1):
            current_segment.append(line)
            current_content = '\n'.join(current_segment)
            
            # ุชูุณูู ุนูุฏ ุงูุญุฌู ุงูุฃูุตู ุฃู ุญุฏูุฏ ุทุจูุนูุฉ
            should_split = (
                len(current_content) >= MAX_SEGMENT_SIZE or
                (len(current_content) >= MIN_SEGMENT_SIZE and
                 (line.strip() == '' or
                  line.strip().startswith('#') or
                  re.match(r'^-{3,}$', line.strip())))
            )
            
            if should_split and len(current_content) >= MIN_SEGMENT_SIZE:
                content_str = '\n'.join(current_segment).strip()
                if content_str:
                    segment = TextSegment(
                        id=f"{file_path.stem}_{segment_idx}",
                        content=content_str,
                        source_file=file_path.name,
                        start_line=current_start,
                        end_line=i,
                        language=detect_language(content_str)
                    )
                    yield segment
                    segment_idx += 1
                
                current_segment = []
                current_start = i + 1
        
        # ุงูููุทุน ุงูุฃุฎูุฑ
        if current_segment:
            content_str = '\n'.join(current_segment).strip()
            if content_str and len(content_str) >= MIN_SEGMENT_SIZE:
                segment = TextSegment(
                    id=f"{file_path.stem}_{segment_idx}",
                    content=content_str,
                    source_file=file_path.name,
                    start_line=current_start,
                    end_line=len(lines),
                    language=detect_language(content_str)
                )
                yield segment
    
    def process_segment(self, segment: TextSegment) -> TextSegment:
        """ูุนุงูุฌุฉ ููุทุน ูุงุญุฏ"""
        # ุงูุชุตููู
        segment.category, segment.confidence = classify_text(segment.content)
        
        # ุชูููู ุงูุฌูุฏุฉ
        segment.quality_score = calculate_quality_score(segment.content)
        
        # ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ
        if segment.language in ('ar', 'mixed'):
            corrected, corrections = correct_spelling(segment.content)
            segment.corrections = corrections
        
        # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
        if segment.language == 'mixed':
            segment.translations = extract_translation_pairs(segment.content)
        
        # ูุญุต ุงูุชูุฑุงุฑ
        content_hash = generate_id(segment.content)
        if content_hash in self.seen_hashes:
            segment.is_duplicate = True
        else:
            self.seen_hashes.add(content_hash)
            
            # ูุญุต ุงูุชุดุงุจู ุจุงุณุชุฎุฏุงู MinHash
            if self.minhash_lsh and HAS_DATASKETCH:
                mh = create_minhash(segment.content)
                if mh:
                    # ุงูุชุญูู ูู ูุฌูุฏ ูุดุงุจู
                    similar = self.minhash_lsh.query(mh)
                    if similar:
                        segment.is_duplicate = True
                    else:
                        self.minhash_lsh.insert(content_hash, mh)
        
        return segment
    
    def save_segment(self, segment: TextSegment):
        """ุญูุธ ุงูููุทุน"""
        if segment.is_duplicate or segment.quality_score < 0.3:
            return
        
        # ุญูุธ ูู ุงููุฌูุฏ ุงูููุงุณุจ
        category_dir = self.output_dir / 'segments' / segment.category
        category_dir.mkdir(exist_ok=True)
        
        file_path = category_dir / f"{segment.id}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# ูุตุฏุฑ: {segment.source_file}\n")
            f.write(f"# ุฃุณุทุฑ: {segment.start_line}-{segment.end_line}\n")
            f.write(f"# ูุบุฉ: {segment.language}\n")
            f.write(f"# ุฌูุฏุฉ: {segment.quality_score}\n")
            f.write(f"# ุชุตููู: {segment.category} ({segment.confidence})\n")
            if segment.corrections:
                f.write(f"# ุชุตุญูุญุงุช: {', '.join(segment.corrections)}\n")
            f.write("\n" + "=" * 60 + "\n\n")
            f.write(segment.content)
    
    def save_translations(self, all_translations: List[Dict]):
        """ุญูุธ ุงูุชุฑุฌูุงุช"""
        if not all_translations:
            return
        
        # CSV
        csv_path = self.output_dir / 'translations' / 'translations.csv'
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['#', 'English', 'Arabic', 'Category', 'Confidence'])
            for i, t in enumerate(all_translations, 1):
                writer.writerow([
                    i,
                    t['english'],
                    t['arabic_corrected'],
                    t['category'],
                    t['confidence']
                ])
        
        # TSV
        tsv_path = self.output_dir / 'translations' / 'translations.tsv'
        with open(tsv_path, 'w', encoding='utf-8') as f:
            for t in all_translations:
                f.write(f"{t['english']}\t{t['arabic_corrected']}\n")
        
        # JSONL
        jsonl_path = self.output_dir / 'translations' / 'translations.jsonl'
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for t in all_translations:
                obj = {
                    'translation': {
                        'en': t['english'],
                        'ar': t['arabic_corrected']
                    },
                    'category': t['category']
                }
                f.write(json.dumps(obj, ensure_ascii=False) + '\n')
    
    def generate_report(self):
        """ุฅูุดุงุก ุชูุฑูุฑ ุงููุนุงูุฌุฉ"""
        report_path = self.output_dir / 'reports' / 'PROCESSING_REPORT.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ๐ ุชูุฑูุฑ ุงููุนุงูุฌุฉ ุงูุฐููุฉ\n\n")
            f.write(f"**ุชุงุฑูุฎ ุงูุจุฏุก:** {self.stats.start_time}\n")
            f.write(f"**ุชุงุฑูุฎ ุงูุงูุชูุงุก:** {self.stats.end_time}\n\n")
            
            f.write("## ๐ ุงูุฅุญุตุงุฆูุงุช ุงูุนุงูุฉ\n\n")
            f.write("| ุงูุจูุฏ | ุงูุนุฏุฏ |\n")
            f.write("|-------|-------|\n")
            f.write(f"| ุฅุฌูุงูู ุงููููุงุช | {self.stats.total_files} |\n")
            f.write(f"| ุงููููุงุช ุงููุนุงูุฌุฉ | {self.stats.processed_files} |\n")
            f.write(f"| ุฅุฌูุงูู ุงูููุงุทุน | {self.stats.total_segments} |\n")
            f.write(f"| ุงูููุงุทุน ุงููุฑูุฏุฉ | {self.stats.unique_segments} |\n")
            f.write(f"| ุงูุชูุฑุงุฑุงุช ุงูููุชุดูุฉ | {self.stats.duplicates_found} |\n")
            f.write(f"| ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | {self.stats.translations_extracted} |\n")
            f.write(f"| ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ | {self.stats.corrections_made} |\n\n")
            
            f.write("## ๐ ุงูุชุตููู\n\n")
            f.write("| ุงููุฆุฉ | ุงูุนุฏุฏ | ุงููุณุจุฉ |\n")
            f.write("|-------|-------|--------|\n")
            
            total = sum(self.stats.categories.values())
            icons = {'medical': '๐ฅ', 'technical': '๐ป', 'translation': '๐',
                    'reference': '๐', 'code': '๐ง', 'misc': '๐'}
            
            for cat, count in sorted(self.stats.categories.items(),
                                    key=lambda x: x[1], reverse=True):
                pct = (count / total * 100) if total > 0 else 0
                icon = icons.get(cat, '๐')
                f.write(f"| {icon} {cat} | {count} | {pct:.1f}% |\n")
            
            if self.stats.errors:
                f.write("\n## โ๏ธ ุงูุฃุฎุทุงุก\n\n")
                for error in self.stats.errors[:20]:
                    f.write(f"- {error}\n")
            
            f.write("\n## ๐ ูููู ุงูุฅุฎุฑุงุฌ\n\n")
            f.write("```\n")
            f.write("output/\n")
            f.write("โโโ segments/           # ุงูููุงุทุน ุงููุตููุฉ\n")
            f.write("โ   โโโ medical/\n")
            f.write("โ   โโโ technical/\n")
            f.write("โ   โโโ translation/\n")
            f.write("โ   โโโ ...\n")
            f.write("โโโ translations/       # ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ\n")
            f.write("โ   โโโ translations.csv\n")
            f.write("โ   โโโ translations.tsv\n")
            f.write("โ   โโโ translations.jsonl\n")
            f.write("โโโ reports/           # ุงูุชูุงุฑูุฑ\n")
            f.write("โโโ progress.pkl       # ููู ุงูุชูุฏู\n")
            f.write("โโโ stats.json         # ุงูุฅุญุตุงุฆูุงุช\n")
            f.write("```\n")
    
    def process(self):
        """ุชูููุฐ ุงููุนุงูุฌุฉ"""
        print("=" * 60)
        print("๐ง ูุนุงูุฌ ุงููุตูุต ุงูุฐูู ุงูุดุงูู")
        print("=" * 60)
        
        # ุชุญููู ุงูุชูุฏู ุงูุณุงุจู
        self.load_progress()
        
        # ุงูุญุตูู ุนูู ุงููููุงุช
        files = self.get_files()
        self.stats.total_files = len(files)
        
        print(f"๐ ูุฌุฏ {len(files)} ููู ูููุนุงูุฌุฉ")
        
        if not files:
            print("โ ูุง ุชูุฌุฏ ูููุงุช ูููุนุงูุฌุฉ")
            return
        
        # ูุงุฆูุฉ ุงููููุงุช ุงููุนุงูุฌุฉ
        processed_files = set()
        progress_data = self.load_progress()
        
        all_translations = []
        
        # ูุนุงูุฌุฉ ุงููููุงุช
        iterator = tqdm(files, desc="ูุนุงูุฌุฉ") if HAS_TQDM else files
        
        for file_path in iterator:
            if self._interrupted:
                break
            
            self.stats.current_file = file_path.name
            
            # ุชุฎุทู ุงููููุงุช ุงููุนุงูุฌุฉ
            if str(file_path) in processed_files:
                continue
            
            try:
                # ุชูุณูู ููุนุงูุฌุฉ
                for segment in self.segment_file(file_path):
                    self.stats.total_segments += 1
                    
                    # ูุนุงูุฌุฉ ุงูููุทุน
                    processed = self.process_segment(segment)
                    
                    # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
                    if processed.is_duplicate:
                        self.stats.duplicates_found += 1
                    else:
                        self.stats.unique_segments += 1
                        self.stats.categories[processed.category] = \
                            self.stats.categories.get(processed.category, 0) + 1
                        
                        if processed.corrections:
                            self.stats.corrections_made += 1
                        
                        if processed.translations:
                            self.stats.translations_extracted += len(processed.translations)
                            all_translations.extend(processed.translations)
                        
                        # ุญูุธ ุงูููุทุน
                        self.save_segment(processed)
                
                processed_files.add(str(file_path))
                self.stats.processed_files += 1
                
                # ุญูุธ ุฏูุฑู
                if self.stats.processed_files % 100 == 0:
                    self.save_progress()
                    self.save_translations(all_translations)
            
            except Exception as e:
                self.stats.errors.append(f"ุฎุทุฃ ูู {file_path.name}: {e}")
        
        # ุญูุธ ุงููุชุงุฆุฌ ุงูููุงุฆูุฉ
        self.stats.end_time = datetime.now().isoformat()
        self.save_translations(all_translations)
        self.generate_report()
        self.save_progress()
        
        print("\n" + "=" * 60)
        print("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
        print("=" * 60)
        print(f"๐ ุงูููุฎุต:")
        print(f"  - ุงููููุงุช: {self.stats.processed_files}/{self.stats.total_files}")
        print(f"  - ุงูููุงุทุน: {self.stats.unique_segments} ูุฑูุฏ")
        print(f"  - ุงูุชูุฑุงุฑุงุช: {self.stats.duplicates_found}")
        print(f"  - ุงูุชุฑุฌูุงุช: {self.stats.translations_extracted}")
        print(f"  - ุงูุชุตุญูุญุงุช: {self.stats.corrections_made}")
        print(f"\n๐ ุงููุชุงุฆุฌ ูู: {self.output_dir}")


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุงุฌูุฉ ุณุทุฑ ุงูุฃูุงูุฑ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def main():
    parser = argparse.ArgumentParser(
        description='ูุนุงูุฌ ุงููุตูุต ุงูุฐูู ุงูุดุงูู',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  # ูุนุงูุฌุฉ ูุงููุฉ
  python intelligent_text_processor.py --input ./data --output ./processed
  
  # ุงุณุชููุงู ูุนุงูุฌุฉ ูุชูููุฉ
  python intelligent_text_processor.py --input ./data --output ./processed --resume
  
  # ุนุฑุถ ุญุงูุฉ ุงููุนุงูุฌุฉ
  python intelligent_text_processor.py --status ./processed
        """
    )
    
    parser.add_argument('--input', '-i', help='ูุฌูุฏ ุงูุฅุฏุฎุงู')
    parser.add_argument('--output', '-o', help='ูุฌูุฏ ุงูุฅุฎุฑุงุฌ')
    parser.add_argument('--mode', '-m', default='all',
                       choices=['all', 'classify', 'translate', 'dedup'],
                       help='ูุถุน ุงููุนุงูุฌุฉ')
    parser.add_argument('--resume', '-r', action='store_true',
                       help='ุงุณุชููุงู ูุนุงูุฌุฉ ุณุงุจูุฉ')
    parser.add_argument('--status', '-s', help='ุนุฑุถ ุญุงูุฉ ุงููุนุงูุฌุฉ')
    
    args = parser.parse_args()
    
    # ุนุฑุถ ุงูุญุงูุฉ
    if args.status:
        stats_file = Path(args.status) / 'stats.json'
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
            
            print("๐ ุญุงูุฉ ุงููุนุงูุฌุฉ:")
            print(f"  - ุงููููุงุช ุงููุนุงูุฌุฉ: {stats.get('processed_files', 0)}")
            print(f"  - ุงูููุงุทุน: {stats.get('unique_segments', 0)}")
            print(f"  - ุงูุชุฑุฌูุงุช: {stats.get('translations_extracted', 0)}")
        else:
            print("โ ูุง ููุฌุฏ ุชูุฏู ูุญููุธ")
        return
    
    # ุงูุชุญูู ูู ุงููุฏุฎูุงุช
    if not args.input or not args.output:
        parser.print_help()
        return
    
    if not Path(args.input).exists():
        print(f"โ ูุฌูุฏ ุงูุฅุฏุฎุงู ุบูุฑ ููุฌูุฏ: {args.input}")
        return
    
    # ุชูููุฐ ุงููุนุงูุฌุฉ
    processor = IntelligentTextProcessor(
        input_dir=args.input,
        output_dir=args.output,
        mode=args.mode
    )
    
    processor.process()


if __name__ == "__main__":
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/knowledge_base_processor.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ                    Knowledge Base Processor - ุงููุณุฎุฉ ุงูููุญุฏุฉ                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโฃ
โ  ุณูุฑูุจุช ุดุงูู ููุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ ุฃูููุงูู                                  โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                               โ
โ  1. ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูุฃุฑุดููุงุช (ZIP, RAR, TAR)                                   โ
โ  2. ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงููุฌูุฏุงุช ุฅูู ูููุงุช ูุตูุฉ                                     โ
โ  3. ูุนุงูุฌุฉ ูููุงุช Excel/Word/HTML/SQLite ุจุดูู ุตุญูุญ                            โ
โ  4. ุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงููุชุฑุฌูุฉ ุซูุงุฆูุฉ ุงููุบุฉ (ุฅูุฌููุฒู-ุนุฑุจู)                        โ
โ  5. ุชุตููู ุงููุญุชูู ุชููุงุฆูุงู (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ูุฑุงุฌุน)                            โ
โ  6. ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ                                           โ
โ  7. ุฌูุน ุงูุชุฑุฌูุงุช ูู ููู CSV ููุญุฏ                                              โ
โ  8. ุฅูุดุงุก ุชูุฑูุฑ ููุฎุต                                                         โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุงูุงุณุชุฎุฏุงู:
    python knowledge_base_processor.py --input <ูุณุงุฑ> --output <ูุณุงุฑ>
    
ุฃูุซูุฉ:
    python knowledge_base_processor.py -i ./files -o ./processed
    python knowledge_base_processor.py -i document.xlsx -o ./output
    python knowledge_base_processor.py -i project.zip -o ./knowledge_base --via-excel
"""

import os
import sys
import re
import csv
import json
import zipfile
import tarfile
import pathlib
import argparse
import datetime
import tempfile
import sqlite3
from collections import defaultdict

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุญุต ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

PANDAS_AVAILABLE = False
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    pass

OPENPYXL_AVAILABLE = False
try:
    from openpyxl import load_workbook
    OPENPYXL_AVAILABLE = True
except ImportError:
    pass

DOCX_AVAILABLE = False
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    pass

HTML_AVAILABLE = False
try:
    from bs4 import BeautifulSoup
    HTML_AVAILABLE = True
except ImportError:
    pass

RAR_AVAILABLE = False
try:
    import rarfile
    RAR_AVAILABLE = True
except ImportError:
    pass

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุซูุงุจุช ูุฃููุงุท
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

# ุฃููุงุน ุงููููุงุช
TEXT_EXTENSIONS = {
    '.txt', '.py', '.js', '.json', '.xml', '.csv', '.md', '.yml', '.yaml',
    '.ini', '.cfg', '.conf', '.java', '.c', '.cpp', '.h', '.cs', '.php',
    '.rb', '.go', '.rs', '.swift', '.kt', '.sql', '.sh', '.bat', '.ps1',
    '.r', '.m', '.f', '.for', '.f90', '.f95', '.properties', '.toml',
    '.lock', '.log', '.tex', '.rst', '.adoc', '.asm', '.v', '.vhdl',
    '.ts', '.tsx', '.jsx', '.vue', '.svelte'
}

DB_EXTENSIONS = {'.db', '.sqlite', '.sqlite3'}
WORD_EXTENSIONS = {'.docx'}
EXCEL_EXTENSIONS = {'.xls', '.xlsx'}
HTML_EXTENSIONS = {'.html', '.htm'}
ARCHIVE_EXTENSIONS = {'.zip', '.rar'}
TAR_EXTENSIONS = {'.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz'}

BINARY_EXTENSIONS = {
    '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin', '.obj',
    '.o', '.a', '.lib', '.dylib', '.bundle', '.class', '.jar',
    '.war', '.ear', '.apk', '.ipa', '.app', '.dmg', '.iso',
    '.img', '.raw', '.dat', '.pkl', '.pickle', '.npy', '.npz',
    '.pt', '.pth', '.h5', '.hdf', '.fits', '.parquet', '.feather'
}

IGNORE_FOLDERS = {'venv', '__pycache__', '.git', '.idea', '.vscode', 'node_modules', '.venv'}

# ุงููููุงุช ุงูููุชุงุญูุฉ ููุชุตููู
CATEGORY_KEYWORDS = {
    'medical': [
        'ุทุจู', 'ุทุจ', 'ูุฑุถ', 'ุนูุงุฌ', 'ุฏูุงุก', 'ูุณุชุดูู', 'ุทุจูุจ', 'ุฌุฑุงุญุฉ', 'ุนุธุงู', 'ูุณุฑ',
        'ุบุถุฑูู', 'ููุตู', 'ูุฑู', 'ุฑูุจุฉ', 'ุนููุฏ ููุฑู', 'ุฃุดุนุฉ', 'ุชุตููุฑ', 'ุชุดุฎูุต',
        'ููุฑุจูุฏุชู', 'ุฅุนุงูุฉ', 'ุฅุตุงุจุฉ', 'ูุณูุฌ', 'ุนุธู', 'ุฏู', 'ููุจ', 'ุดุฑุงููู',
        'medical', 'hospital', 'doctor', 'surgery', 'orthopedic', 'fracture',
        'bone', 'joint', 'hip', 'knee', 'spine', 'x-ray', 'diagnosis', 'patient',
        'treatment', 'disease', 'medicine', 'clinical', 'surgical', 'anatomy'
    ],
    'technical': [
        'ุจุฑูุฌุฉ', 'ููุฏ', 'ุณูุฑูุฑ', 'ููููุณ', 'ูููุฏูุฒ', 'ุดุจูุฉ', 'ูุงุนุฏุฉ ุจูุงูุงุช', 'ุณูุฑูุจุช',
        'ุชุซุจูุช', 'ุฅุนุฏุงุฏ', 'ุฃูุฑ', 'ูุญุทุฉ', 'ุญุฒูุฉ', 'ุชุญุฏูุซ', 'ุฎุทุฃ', 'ุฅุตูุงุญ',
        'ุจุงููุฌ', 'ุฃุฑุด', 'ูุงูุฌุงุฑู', 'ุบุงุฑูุฏุง', 'ูุฏู', 'ุฌููู', 'ุจุงูุซูู', 'ููุฏ',
        'programming', 'code', 'server', 'linux', 'windows', 'network', 'database',
        'script', 'install', 'setup', 'command', 'terminal', 'package', 'update',
        'error', 'fix', 'arch', 'manjaro', 'garuda', 'kde', 'gnome', 'python',
        'git', 'docker', 'api', 'config', 'bash', 'shell', 'kernel'
    ],
    'translation': [
        'ุชุฑุฌูุฉ', 'ูุชุฑุฌู', 'ูุบุฉ', 'ุฅูุฌููุฒู', 'ุนุฑุจู', 'ุชุนุฑูุจ', 'ูุต ูุชุฑุฌู',
        'ุซูุงุฆู ุงููุบุฉ', 'ูุนูู', 'ูุงููุณ', 'ูููุฉ', 'ุฌููุฉ',
        'translation', 'translate', 'language', 'english', 'arabic', 'bilingual',
        'meaning', 'dictionary', 'word', 'sentence', 'localized'
    ]
}

# ุฃููุงุท ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
TRANSLATION_PATTERNS = [
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*[-โโ:]\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*:\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*=\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    r'"?([A-Za-z][A-Za-z\s,\-\(\)]+?)"?\s*[,;]\s*"?([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+?)"?$',
]

# ูุงููุณ ุงูุชุตุญูุญ ุงูุฅููุงุฆู
ARABIC_SPELLING_CORRECTIONS = {
    # ุงูููุฒุงุช
    'ุงุฆูุง': 'ุฅูุง', 'ุงุฆู': 'ุฅู', 'ุฃูุดุงุก': 'ุฅูุดุงุก', 'ุงูุดุงุก': 'ุฅูุดุงุก',
    'ูุฃู': 'ูุฃู', 'ูุงู': 'ูุฃู',
    # ุงูุฃูู ุงููููุฉ
    'ุงูู': 'ุฅูู', 'ุงูู': 'ุฅูู', 'ุนูู': 'ุนูู', 'ุญุชู': 'ุญุชู',
    # ุงููููุงุช ุงูุดุงุฆุนุฉ
    'ุงูุถุง': 'ุฃูุถุงู', 'ุงูุธุง': 'ุฃูุถุงู', 'ุงูุถุข': 'ุฃูุถุงู',
    'ููุทุข': 'ููุท', 'ุฌุฏุง': 'ุฌุฏุงู', 'ุฌุฏุข': 'ุฌุฏุงู',
    'ูุซูุฑุง': 'ูุซูุฑุงู', 'ูุซูุฑุข': 'ูุซูุฑุงู',
    'ุฐุงูู': 'ุฐูู', 'ุงูุฐู': 'ุงูุฐู', 'ุงูุชู': 'ุงูุชู',
    'ุงููุฉ': 'ุงููู', 'ุงูุฃู': 'ุงูุขู', 'ุงูุงู': 'ุงูุขู',
    'ูุงูููู': 'ูุง ูููู', 'ูุงููุฌุฏ': 'ูุง ููุฌุฏ',
    'ูุงุชูุฌุฏ': 'ูุง ุชูุฌุฏ', 'ูุงููู': 'ูุง ููู',
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุงูุชุตููู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def detect_category(text):
    """ุชุตููู ุงููุต ุชููุงุฆูุงู"""
    text_lower = text.lower()
    scores = defaultdict(int)
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                scores[category] += 1
    
    return max(scores, key=scores.get) if scores else 'references'

def get_category_icon(category):
    """ุฅุฑุฌุงุน ุฃููููุฉ ุงูุชุตููู"""
    return {'medical': '๐ฅ', 'technical': '๐ป', 'translation': '๐', 'references': '๐'}.get(category, '๐')

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุงูุชุตุญูุญ ุงูุฅููุงุฆู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def correct_arabic_spelling(text):
    """ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ"""
    corrected = text
    for wrong, correct in ARABIC_SPELLING_CORRECTIONS.items():
        corrected = re.sub(r'\b' + wrong + r'\b', correct, corrected)
    corrected = re.sub(r'\s+', ' ', corrected)
    corrected = corrected.replace(' ,', ',').replace(' .', '.').replace('ุุ', 'ุ')
    return corrected.strip()

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def extract_translations_from_text(text):
    """ุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงููุชุฑุฌูุฉ ูู ูุต"""
    translations = []
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
        for pattern in TRANSLATION_PATTERNS:
            match = re.match(pattern, line)
            if match:
                english = match.group(1).strip()
                arabic = match.group(2).strip()
                if len(english) > 2 and len(arabic) > 2:
                    arabic_corrected = correct_arabic_spelling(arabic)
                    translations.append({
                        'english': english,
                        'arabic': arabic,
                        'arabic_corrected': arabic_corrected,
                        'needs_correction': arabic != arabic_corrected,
                        'source': 'text'
                    })
                break
    return translations

def extract_translations_from_excel(filepath):
    """ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ููู Excel"""
    translations = []
    if not PANDAS_AVAILABLE:
        return translations
    
    try:
        df_dict = pd.read_excel(filepath, sheet_name=None)
        for sheet_name, df in df_dict.items():
            english_col = arabic_col = None
            for col in df.columns:
                col_lower = str(col).lower()
                if 'english' in col_lower or 'ุฅูุฌููุฒู' in col_lower or 'ุงูุฅูุฌููุฒูุฉ' in col_lower:
                    english_col = col
                elif 'arabic' in col_lower or 'ุนุฑุจู' in col_lower or 'ุงูุนุฑุจูุฉ' in col_lower:
                    arabic_col = col
            
            if english_col and arabic_col:
                for _, row in df.iterrows():
                    english = str(row[english_col]).strip() if pd.notna(row[english_col]) else ''
                    arabic = str(row[arabic_col]).strip() if pd.notna(row[arabic_col]) else ''
                    if english and arabic and english != 'nan' and arabic != 'nan':
                        arabic_corrected = correct_arabic_spelling(arabic)
                        translations.append({
                            'english': english, 'arabic': arabic,
                            'arabic_corrected': arabic_corrected,
                            'needs_correction': arabic != arabic_corrected,
                            'source': f'excel:{sheet_name}'
                        })
    except Exception as e:
        print(f"  โ๏ธ ุฎุทุฃ ูู ูุฑุงุกุฉ Excel: {str(e)}")
    return translations

def extract_translations_from_csv(filepath):
    """ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ููู CSV"""
    translations = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if not header:
                return translations
            
            english_idx = arabic_idx = None
            for i, col in enumerate(header):
                col_lower = str(col).lower()
                if 'english' in col_lower or 'ุฅูุฌููุฒู' in col_lower:
                    english_idx = i
                elif 'arabic' in col_lower or 'ุนุฑุจู' in col_lower:
                    arabic_idx = i
            
            if english_idx is not None and arabic_idx is not None:
                for row in reader:
                    if len(row) > max(english_idx, arabic_idx):
                        english = row[english_idx].strip()
                        arabic = row[arabic_idx].strip()
                        if english and arabic:
                            arabic_corrected = correct_arabic_spelling(arabic)
                            translations.append({
                                'english': english, 'arabic': arabic,
                                'arabic_corrected': arabic_corrected,
                                'needs_correction': arabic != arabic_corrected,
                                'source': 'csv'
                            })
    except Exception as e:
        print(f"  โ๏ธ ุฎุทุฃ ูู ูุฑุงุกุฉ CSV: {str(e)}")
    return translations

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุนุงูุฌุฉ ุงููููุงุช ุงููุชุฎุตุตุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def convert_timestamp(value):
    """ุชุญููู ุงูุทูุงุจุน ุงูุฒูููุฉ ุฅูู ุชูุณูู ุนุฑุจู (ุต/ู)"""
    try:
        if PANDAS_AVAILABLE and pd.isna(value) or value is None:
            return "NULL"
        
        if isinstance(value, (int, float)):
            try:
                value = datetime.datetime.fromtimestamp(value)
            except:
                pass
        
        if isinstance(value, str):
            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                try:
                    value = datetime.datetime.strptime(value, fmt)
                    break
                except:
                    continue
        
        if isinstance(value, datetime.datetime):
            hour = value.hour
            hour_12 = 12 if hour == 0 else (12 if hour == 12 else (hour if hour < 12 else hour - 12))
            meridiem = "ุต" if hour < 12 else "ู"
            return f"{value.day:02d}/{value.month:02d}/{value.year} {hour_12}:{value.minute:02d}:{value.second:02d} {meridiem}"
        
        return str(value)
    except:
        return str(value)

def extract_db_to_text(db_path, output_file, via_excel=False):
    """ุงุณุชุฎุฑุงุฌ ูุงุนุฏุฉ ุจูุงูุงุช SQLite ุฅูู ูุต"""
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
        tables = [row[0] for row in cursor.fetchall()]
        
        if not tables:
            conn.close()
            return None, 0
        
        total_rows = 0
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ูุงุนุฏุฉ ุงูุจูุงูุงุช: {os.path.basename(db_path)}\n")
            f.write(f"ุชุงุฑูุฎ ุงูุงุณุชุฎุฑุงุฌ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            for table in tables:
                f.write(f"๐ ุฌุฏูู: {table}\n")
                f.write("-" * 40 + "\n")
                cursor.execute(f'SELECT * FROM "{table}"')
                rows = cursor.fetchall()
                cols = [d[0] for d in cursor.description]
                f.write("\t".join(cols) + "\n")
                
                for row in rows:
                    row_str = []
                    for val in row:
                        if val is None:
                            row_str.append("NULL")
                        elif isinstance(val, bytes):
                            row_str.append("<binary>")
                        else:
                            row_str.append(convert_timestamp(val) if via_excel else str(val))
                    f.write("\t".join(row_str) + "\n")
                f.write("\n")
                total_rows += len(rows)
                print(f"  โ ุฌุฏูู '{table}': {len(rows)} ุตู")
            
            f.write("=" * 80 + "\n")
            f.write(f"ุนุฏุฏ ุงูุฌุฏุงูู: {len(tables)} | ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
            f.write("=" * 80 + "\n")
        
        conn.close()
        return output_file, total_rows
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช: {str(e)}")
        return None, 0

def extract_excel_to_text(excel_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชูู Excel ููุต ููุธู"""
    if not PANDAS_AVAILABLE:
        print("  โ ููุชุจุฉ pandas ุบูุฑ ูุซุจุชุฉ")
        return None, 0
    
    try:
        excel_file = pd.ExcelFile(excel_path)
        total_rows = 0
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ููู Excel: {os.path.basename(excel_path)}\n")
            f.write("=" * 80 + "\n\n")
            
            for sheet in excel_file.sheet_names:
                df = pd.read_excel(excel_path, sheet_name=sheet)
                f.write(f"๐ ูุฑูุฉ: {sheet}\n")
                f.write("-" * 40 + "\n")
                f.write("\t".join(map(str, df.columns)) + "\n")
                
                for _, row in df.iterrows():
                    row_str = [str(v) if pd.notna(v) else "NULL" for v in row]
                    f.write("\t".join(row_str) + "\n")
                f.write("\n")
                total_rows += len(df)
                print(f"  โ ูุฑูุฉ '{sheet}': {len(df)} ุตู")
            
            f.write("=" * 80 + "\n")
            f.write(f"ุนุฏุฏ ุงูุฃูุฑุงู: {len(excel_file.sheet_names)} | ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
        
        return output_file, total_rows
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู Excel: {str(e)}")
        return None, 0

def extract_docx_to_text(docx_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู Word"""
    if not DOCX_AVAILABLE:
        print("  โ ููุชุจุฉ python-docx ุบูุฑ ูุซุจุชุฉ")
        return None, 0
    
    try:
        doc = Document(docx_path)
        paras_count = tables_count = 0
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ูุณุชูุฏ Word: {os.path.basename(docx_path)}\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("๐ ุงูููุฑุงุช:\n" + "-" * 40 + "\n")
            for para in doc.paragraphs:
                if para.text.strip():
                    f.write(para.text + "\n")
                    paras_count += 1
            
            if doc.tables:
                f.write("\n๐ ุงูุฌุฏุงูู:\n" + "-" * 40 + "\n")
                for i, table in enumerate(doc.tables, 1):
                    f.write(f"ุฌุฏูู {i}:\n")
                    for row in table.rows:
                        f.write("\t".join(cell.text.strip() for cell in row.cells) + "\n")
                    f.write("\n")
                    tables_count += 1
            
            f.write("=" * 80 + "\n")
            f.write(f"ุงูููุฑุงุช: {paras_count} | ุงูุฌุฏุงูู: {tables_count}\n")
        
        print(f"  โ ุงูููุฑุงุช: {paras_count} | ุงูุฌุฏุงูู: {tables_count}")
        return output_file, paras_count + tables_count
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู Word: {str(e)}")
        return None, 0

def extract_html_to_text(html_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู HTML"""
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if HTML_AVAILABLE:
            soup = BeautifulSoup(content, 'html.parser')
            for tag in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):
                tag.decompose()
            text = soup.get_text(separator='\n', strip=True)
        else:
            text = re.sub(r'<[^>]+>', '', content)
            text = re.sub(r'\s+', ' ', text).strip()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ูุญุชูู ุตูุญุฉ ุงูููุจ: {os.path.basename(html_path)}\n")
            f.write("=" * 80 + "\n\n")
            f.write(text)
        
        lines = len(text.split('\n'))
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {lines} ุณุทุฑ")
        return output_file, lines
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู HTML: {str(e)}")
        return None, 0

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุนุงูุฌุฉ ุงูุฃุฑุดููุงุช ูุงููุฌูุฏุงุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def should_ignore_path(path):
    """ุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงููุณุงุฑ"""
    for part in path.replace('\\', '/').split('/'):
        if part.startswith('.') or part in IGNORE_FOLDERS:
            return True
    return False

def is_text_content(content_bytes):
    """ูุญุต ูุง ุฅุฐุง ูุงู ุงููุญุชูู ูุตูุงู"""
    try:
        content_bytes.decode('utf-8')
        return True
    except UnicodeDecodeError:
        try:
            content_bytes.decode('latin-1')
            return True
        except:
            return False

def extract_archive_to_text(archive_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุฃุฑุดูู ุฅูู ููู ูุตู"""
    ext = pathlib.Path(archive_path).suffix.lower()
    
    if ext == '.zip':
        archive_type = 'zip'
    elif ext in TAR_EXTENSIONS or tarfile.is_tarfile(archive_path):
        archive_type = 'tar'
    elif RAR_AVAILABLE and ext == '.rar':
        archive_type = 'rar'
    else:
        print(f"  โ๏ธ ููุน ุงูุฃุฑุดูู ุบูุฑ ูุฏุนูู: {ext}")
        return None, 0, 0
    
    processed = skipped = 0
    
    try:
        with open(output_file, 'w', encoding='utf-8') as out:
            out.write("=" * 80 + "\n")
            out.write(f"ูุญุชูู ุงูุฃุฑุดูู: {os.path.basename(archive_path)}\n")
            out.write("=" * 80 + "\n\n")
            
            if archive_type == 'zip':
                with zipfile.ZipFile(archive_path, 'r') as archive:
                    for name in sorted(archive.namelist()):
                        if name.endswith('/') or should_ignore_path(name):
                            skipped += 1
                            continue
                        file_ext = pathlib.Path(name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS or file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                            skipped += 1
                            continue
                        try:
                            with archive.open(name, 'r') as f:
                                content = f.read()
                            if not is_text_content(content):
                                skipped += 1
                                continue
                            out.write(f"ููู: {name}\n" + "-" * 40 + "\n")
                            out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                            processed += 1
                        except:
                            skipped += 1
            
            elif archive_type == 'tar':
                with tarfile.open(archive_path, 'r:*') as archive:
                    for member in archive.getmembers():
                        if member.isdir() or should_ignore_path(member.name):
                            skipped += 1
                            continue
                        file_ext = pathlib.Path(member.name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS or file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                            skipped += 1
                            continue
                        try:
                            f = archive.extractfile(member)
                            if not f:
                                continue
                            content = f.read()
                            if not is_text_content(content):
                                skipped += 1
                                continue
                            out.write(f"ููู: {member.name}\n" + "-" * 40 + "\n")
                            out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                            processed += 1
                        except:
                            skipped += 1
            
            elif archive_type == 'rar':
                with rarfile.RarFile(archive_path, 'r') as archive:
                    for name in sorted(archive.namelist()):
                        if name.endswith('/') or should_ignore_path(name):
                            skipped += 1
                            continue
                        file_ext = pathlib.Path(name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS or file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                            skipped += 1
                            continue
                        try:
                            with archive.open(name, 'r') as f:
                                content = f.read()
                            if not is_text_content(content):
                                skipped += 1
                                continue
                            out.write(f"ููู: {name}\n" + "-" * 40 + "\n")
                            out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                            processed += 1
                        except:
                            skipped += 1
            
            out.write("=" * 80 + "\n")
            out.write(f"ููุฎุต: {processed} ูุนุงูุฌ | {skipped} ูุชุฌุงูู\n")
        
        print(f"  โ {processed} ููู ูุนุงูุฌ | {skipped} ูุชุฌุงูู")
        return output_file, processed, skipped
    except Exception as e:
        print(f"  โ ุฎุทุฃ: {str(e)}")
        return None, 0, 0

def extract_folder_to_text(folder_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฌูุฏ ุฅูู ููู ูุตู"""
    processed = skipped = 0
    
    try:
        with open(output_file, 'w', encoding='utf-8') as out:
            out.write("=" * 80 + "\n")
            out.write(f"ูุญุชูู ุงููุฌูุฏ: {os.path.basename(folder_path)}\n")
            out.write("=" * 80 + "\n\n")
            
            for root, dirs, files in os.walk(folder_path):
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in IGNORE_FOLDERS]
                
                for file in sorted(files):
                    path = os.path.join(root, file)
                    rel = os.path.relpath(path, folder_path)
                    
                    if should_ignore_path(rel):
                        skipped += 1
                        continue
                    
                    ext = pathlib.Path(file).suffix.lower()
                    if ext in BINARY_EXTENSIONS:
                        skipped += 1
                        continue
                    
                    try:
                        with open(path, 'rb') as f:
                            content = f.read()
                        if not is_text_content(content):
                            skipped += 1
                            continue
                        out.write(f"ููู: {rel}\n" + "-" * 40 + "\n")
                        out.write(content.decode('utf-8', errors='replace') + "\n" + "=" * 80 + "\n\n")
                        processed += 1
                    except:
                        skipped += 1
            
            out.write("=" * 80 + "\n")
            out.write(f"ููุฎุต: {processed} ูุนุงูุฌ | {skipped} ูุชุฌุงูู\n")
        
        print(f"  โ {processed} ููู ูุนุงูุฌ | {skipped} ูุชุฌุงูู")
        return output_file, processed, skipped
    except Exception as e:
        print(f"  โ ุฎุทุฃ: {str(e)}")
        return None, 0, 0

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงููุนุงูุฌ ุงูุฑุฆูุณู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class KnowledgeBaseProcessor:
    """ูุนุงูุฌ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุดุงูู"""
    
    def __init__(self, input_path, output_path, via_excel=False):
        self.input_path = input_path
        self.output_path = output_path
        self.via_excel = via_excel
        self.translations = []
        self.processed_files = []
        self.categories = defaultdict(list)
        self.setup_dirs()
    
    def setup_dirs(self):
        """ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช"""
        dirs = [
            self.output_path,
            os.path.join(self.output_path, 'technical', 'linux'),
            os.path.join(self.output_path, 'technical', 'windows'),
            os.path.join(self.output_path, 'medical', 'orthopedics'),
            os.path.join(self.output_path, 'translation'),
            os.path.join(self.output_path, 'scripts'),
            os.path.join(self.output_path, 'digests'),
            os.path.join(self.output_path, 'extracted')
        ]
        for d in dirs:
            os.makedirs(d, exist_ok=True)
    
    def process_file(self, filepath):
        """ูุนุงูุฌุฉ ููู ูุงุญุฏ"""
        filename = os.path.basename(filepath)
        ext = pathlib.Path(filepath).suffix.lower()
        base_name = os.path.splitext(filename)[0]
        
        print(f"\n๐ ูุนุงูุฌุฉ: {filename}")
        
        # ุชุญุฏูุฏ ููุน ุงููุนุงูุฌุฉ
        result = None
        
        if ext in DB_EXTENSIONS:
            output = os.path.join(self.output_path, 'extracted', f"{base_name}_db.txt")
            result = extract_db_to_text(filepath, output, self.via_excel)
        elif ext in EXCEL_EXTENSIONS:
            output = os.path.join(self.output_path, 'extracted', f"{base_name}_excel.txt")
            result = extract_excel_to_text(filepath, output)
            # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
            trans = extract_translations_from_excel(filepath)
            if trans:
                self.translations.extend(trans)
                print(f"  โ {len(trans)} ุชุฑุฌูุฉ ูุณุชุฎุฑุฌุฉ")
        elif ext in WORD_EXTENSIONS:
            output = os.path.join(self.output_path, 'extracted', f"{base_name}_word.txt")
            result = extract_docx_to_text(filepath, output)
        elif ext in HTML_EXTENSIONS:
            output = os.path.join(self.output_path, 'extracted', f"{base_name}_html.txt")
            result = extract_html_to_text(filepath, output)
        elif ext in ARCHIVE_EXTENSIONS or ext in TAR_EXTENSIONS:
            output = os.path.join(self.output_path, 'extracted', f"{base_name}_archive.txt")
            result = extract_archive_to_text(filepath, output)
        elif ext == '.csv':
            trans = extract_translations_from_csv(filepath)
            if trans:
                self.translations.extend(trans)
                print(f"  โ {len(trans)} ุชุฑุฌูุฉ ูุณุชุฎุฑุฌุฉ")
            # ูุณุฎ ุงูููู
            import shutil
            shutil.copy(filepath, os.path.join(self.output_path, 'translation', filename))
            result = (filepath, len(trans), 0)
        elif ext in TEXT_EXTENSIONS or ext not in BINARY_EXTENSIONS:
            try:
                with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                    content = f.read()
                
                # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
                trans = extract_translations_from_text(content)
                if trans:
                    self.translations.extend(trans)
                    print(f"  โ {len(trans)} ุชุฑุฌูุฉ ูุณุชุฎุฑุฌุฉ")
                
                # ุชุตููู ููุณุฎ
                category = detect_category(content[:5000])
                self.categories[category].append(filepath)
                
                if category == 'technical':
                    if any(kw in content.lower() for kw in ['linux', 'arch', 'garuda', 'ubuntu']):
                        dest_dir = os.path.join(self.output_path, 'technical', 'linux')
                    else:
                        dest_dir = os.path.join(self.output_path, 'technical', 'windows')
                else:
                    dest_dir = os.path.join(self.output_path, category)
                
                dest = os.path.join(dest_dir, filename)
                counter = 1
                while os.path.exists(dest):
                    dest = os.path.join(dest_dir, f"{base_name}_{counter}{ext}")
                    counter += 1
                
                with open(dest, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"  โ ูุตูู: {get_category_icon(category)} {category}")
                result = (dest, len(content.split('\n')), 0)
            except Exception as e:
                print(f"  โ๏ธ ุฎุทุฃ: {str(e)}")
                result = None
        
        if result and result[0]:
            self.processed_files.append(filepath)
    
    def process_folder(self, folderpath):
        """ูุนุงูุฌุฉ ูุฌูุฏ"""
        foldername = os.path.basename(folderpath)
        output = os.path.join(self.output_path, 'extracted', f"{foldername}_folder.txt")
        
        result = extract_folder_to_text(folderpath, output)
        
        if result and result[0]:
            # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
            with open(result[0], 'r', encoding='utf-8') as f:
                content = f.read()
            trans = extract_translations_from_text(content)
            if trans:
                self.translations.extend(trans)
                print(f"  โ {len(trans)} ุชุฑุฌูุฉ ูุณุชุฎุฑุฌุฉ")
            
            category = detect_category(content)
            self.categories[category].append(folderpath)
            self.processed_files.append(folderpath)
    
    def process_all(self):
        """ูุนุงูุฌุฉ ุฌููุน ุงูุนูุงุตุฑ"""
        print("\n" + "=" * 60)
        print("๐ ุจุฏุก ุงููุนุงูุฌุฉ")
        print("=" * 60)
        
        if os.path.isfile(self.input_path):
            ext = pathlib.Path(self.input_path).suffix.lower()
            if ext in ARCHIVE_EXTENSIONS or ext in TAR_EXTENSIONS:
                self.process_file(self.input_path)
            else:
                self.process_file(self.input_path)
        
        elif os.path.isdir(self.input_path):
            for item in sorted(os.listdir(self.input_path)):
                item_path = os.path.join(self.input_path, item)
                if os.path.isfile(item_path):
                    self.process_file(item_path)
                elif os.path.isdir(item_path):
                    self.process_folder(item_path)
        
        self.save_translations()
        self.save_report()
    
    def save_translations(self):
        """ุญูุธ ุงูุชุฑุฌูุงุช ูู CSV"""
        if not self.translations:
            print("\nโ๏ธ ูู ูุชู ุงูุนุซูุฑ ุนูู ุชุฑุฌูุงุช")
            return
        
        output = os.path.join(self.output_path, 'translation', 'translations_collected.csv')
        
        # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
        unique = []
        seen = set()
        for t in self.translations:
            key = (t['english'].lower(), t['arabic'])
            if key not in seen:
                seen.add(key)
                unique.append(t)
        
        with open(output, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['English', 'Arabic (Original)', 'Arabic (Corrected)', 'Needs Correction', 'Source'])
            for t in unique:
                writer.writerow([t['english'], t['arabic'], t['arabic_corrected'], 
                               'Yes' if t['needs_correction'] else 'No', t['source']])
        
        corrections = sum(1 for t in unique if t['needs_correction'])
        print(f"\n๐ ุชุฑุฌูุงุช ูุญููุธุฉ: {len(unique)} | ุชุญุชุงุฌ ุชุตุญูุญ: {corrections}")
        print(f"   ๐ {output}")
    
    def save_report(self):
        """ุญูุธ ุชูุฑูุฑ ุงููุนุงูุฌุฉ"""
        report = os.path.join(self.output_path, 'PROCESSING_REPORT.md')
        
        with open(report, 'w', encoding='utf-8') as f:
            f.write("# ุชูุฑูุฑ ูุนุงูุฌุฉ ุงููููุงุช\n\n")
            f.write(f"**ุงูุชุงุฑูุฎ:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ๐ ุฅุญุตุงุฆูุงุช\n\n")
            f.write(f"| ุงูุจูุฏ | ุงูุนุฏุฏ |\n|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงููุนุงูุฌุฉ | {len(self.processed_files)} |\n")
            f.write(f"| ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | {len(self.translations)} |\n\n")
            
            f.write("## ๐ ุงูุชุตููู\n\n")
            for cat, files in self.categories.items():
                f.write(f"### {get_category_icon(cat)} {cat.title()}\n\n")
                for file in files:
                    f.write(f"- {os.path.basename(file)}\n")
                f.write("\n")
            
            f.write("## โ ุงููููุงุช ุงููุนุงูุฌุฉ\n\n")
            for file in self.processed_files:
                f.write(f"- {file}\n")
        
        print(f"\n๐ ุชูุฑูุฑ: {report}")

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุทุฉ ุงูุฏุฎูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def print_dependencies():
    """ุนุฑุถ ุญุงูุฉ ุงูููุชุจุงุช"""
    print("\n๐ฆ ุญุงูุฉ ุงูููุชุจุงุช:")
    print(f"  {'โ' if PANDAS_AVAILABLE else 'โ'} pandas (Excel)")
    print(f"  {'โ' if DOCX_AVAILABLE else 'โ'} python-docx (Word)")
    print(f"  {'โ' if HTML_AVAILABLE else 'โ๏ธ'} beautifulsoup4 (HTML)")
    print(f"  {'โ' if RAR_AVAILABLE else 'โ๏ธ'} rarfile (RAR)")
    print()

def main():
    parser = argparse.ArgumentParser(
        description='ูุนุงูุฌ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุงูุดุงูู',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  python knowledge_base_processor.py -i ./files -o ./processed
  python knowledge_base_processor.py -i database.db -o ./output --via-excel
  python knowledge_base_processor.py -i archive.zip -o ./knowledge_base
        """
    )
    
    parser.add_argument('-i', '--input', required=True, help='ูุณุงุฑ ุงูููู ุฃู ุงููุฌูุฏ')
    parser.add_argument('-o', '--output', default='./knowledge_base', help='ูุณุงุฑ ุงูุฅุฎุฑุงุฌ')
    parser.add_argument('--via-excel', action='store_true', help='ุชุญููู DB ุนุจุฑ Excel')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"โ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {args.input}")
        sys.exit(1)
    
    print_dependencies()
    
    processor = KnowledgeBaseProcessor(args.input, args.output, args.via_excel)
    processor.process_all()
    
    print("\n" + "=" * 60)
    print("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
    print("=" * 60)
    print(f"\n๐ ุงูุฅุฎุฑุงุฌ: {args.output}")
    print("\n๐ก ุงูุฎุทูุฉ ุงูุชุงููุฉ: ุฑุงุฌุน ุงููููุงุช ุซู ุดุงุฑููุง ููุชุญูู ูุงูุฑูุน ุฅูู GitHub")

if __name__ == '__main__':
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/knowledge_processor.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Knowledge Processor - ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ
=====================================================
ูุธุงุฆู ุงูุณูุฑูุจุช:
1. ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงููููุงุช ุงููุถุบูุทุฉ (ZIP, RAR, TAR)
2. ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงููุฌูุฏุงุช ุฅูู ูููุงุช ูุตูุฉ
3. ุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงููุชุฑุฌูุฉ ุซูุงุฆูุฉ ุงููุบุฉ
4. ุชุตููู ุงููุญุชูู ุชููุงุฆูุงู (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ูุฑุงุฌุน)
5. ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ
6. ุฌูุน ุงูุชุฑุฌูุงุช ูู ููู CSV ููุญุฏ
7. ุฅูุดุงุก ุชูุฑูุฑ ููุฎุต

ุงูุงุณุชุฎุฏุงู:
    python knowledge_processor.py --input <ูุณุงุฑ_ุงููููุงุช> --output <ูุณุงุฑ_ุงูุฅุฎุฑุงุฌ>
    
ูุซุงู:
    python knowledge_processor.py --input ./my_files --output ./processed
"""

import os
import sys
import re
import csv
import json
import zipfile
import tarfile
import pathlib
import argparse
import datetime
import subprocess
from collections import defaultdict

# ูุญุงููุฉ ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

try:
    from openpyxl import load_workbook
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False

try:
    import rarfile
    RAR_SUPPORT = True
except ImportError:
    RAR_SUPPORT = False

# ============================================
# ุงูููุงููุณ ูุงูุฃููุงุท
# ============================================

# ุงููููุงุช ุงูููุชุงุญูุฉ ููุชุตููู
CATEGORY_KEYWORDS = {
    'medical': [
        # ุนุฑุจู
        'ุทุจู', 'ุทุจ', 'ูุฑุถ', 'ุนูุงุฌ', 'ุฏูุงุก', 'ูุณุชุดูู', 'ุทุจูุจ', 'ุฌุฑุงุญุฉ', 'ุนุธุงู', 'ูุณุฑ',
        'ุบุถุฑูู', 'ููุตู', 'ูุฑู', 'ุฑูุจุฉ', 'ุนููุฏ ููุฑู', 'ุฃุดุนุฉ', 'ุชุตููุฑ', 'ุชุดุฎูุต',
        'ููุฑุจูุฏุชู', 'ุฅุนุงูุฉ', 'ุฅุตุงุจุฉ', 'ูุณูุฌ', 'ุนุธู', 'ุฏู', 'ููุจ', 'ุดุฑุงููู',
        # ุฅูุฌููุฒู
        'medical', 'hospital', 'doctor', 'surgery', 'orthopedic', 'fracture',
        'bone', 'joint', 'hip', 'knee', 'spine', 'x-ray', 'diagnosis', 'patient',
        'treatment', 'disease', 'medicine', 'clinical', 'surgical', 'anatomy'
    ],
    'technical': [
        # ุนุฑุจู
        'ุจุฑูุฌุฉ', 'ููุฏ', 'ุณูุฑูุฑ', 'ููููุณ', 'ูููุฏูุฒ', 'ุดุจูุฉ', 'ูุงุนุฏุฉ ุจูุงูุงุช', 'ุณูุฑูุจุช',
        'ุชุซุจูุช', 'ุฅุนุฏุงุฏ', 'ุฃูุฑ', 'ูุญุทุฉ', 'ุญุฒูุฉ', 'ุชุญุฏูุซ', 'ุฎุทุฃ', 'ุฅุตูุงุญ',
        'ุจุงููุฌ', 'ุฃุฑุด', 'ูุงูุฌุงุฑู', 'ุบุงุฑูุฏุง', 'ูุฏู', 'ุฌููู', 'ุจุงูุซูู', 'ููุฏ',
        # ุฅูุฌููุฒู
        'programming', 'code', 'server', 'linux', 'windows', 'network', 'database',
        'script', 'install', 'setup', 'command', 'terminal', 'package', 'update',
        'error', 'fix', 'arch', 'manjaro', 'garuda', 'kde', 'gnome', 'python',
        'git', 'docker', 'api', 'config', 'bash', 'shell', 'kernel'
    ],
    'translation': [
        # ุนุฑุจู
        'ุชุฑุฌูุฉ', 'ูุชุฑุฌู', 'ูุบุฉ', 'ุฅูุฌููุฒู', 'ุนุฑุจู', 'ุชุนุฑูุจ', 'ูุต ูุชุฑุฌู',
        'ุซูุงุฆู ุงููุบุฉ', 'ูุนูู', 'ูุงููุณ', 'ูููุฉ', 'ุฌููุฉ',
        # ุฅูุฌููุฒู
        'translation', 'translate', 'language', 'english', 'arabic', 'bilingual',
        'meaning', 'dictionary', 'word', 'sentence', 'localized'
    ]
}

# ุฃููุงุท ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
TRANSLATION_PATTERNS = [
    # ููุท: English - Arabic
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*[-โโ:]\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    # ููุท: English: Arabic
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*:\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    # ููุท: English = Arabic
    r'^([A-Za-z][A-Za-z\s,\-\(\)]+?)\s*=\s*([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+)$',
    # ููุท ูู ูููุงุช CSV/Excel
    r'"?([A-Za-z][A-Za-z\s,\-\(\)]+?)"?\s*[,;]\s*"?([\u0600-\u06FF][\u0600-\u06FF\sุ,\-\(\)]+?)"?$',
]

# ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ ุงูุดุงุฆุนุฉ ูุฅุตูุงุญูุง
ARABIC_SPELLING_CORRECTIONS = {
    # ุงูููุฒุงุช
    'ุงุฆูุง': 'ุฅูุง',
    'ุงุฆู': 'ุฅู',
    'ุฃูุดุงุก': 'ุฅูุดุงุก',
    'ุงูุดุงุก': 'ุฅูุดุงุก',
    'ูุฃู': 'ูุฃู',
    'ูุงู': 'ูุฃู',
    # ุงูุชุงุก ุงููุฑุจูุทุฉ
    'ุฉ': 'ุฉ',  # ููููุงุฑูุฉ
    'ูุงุฆ': 'ูุง',  # ููุงูุฉ ุงููููุฉ
    # ุงูุฃูู ุงููููุฉ
    'ุงูู': 'ุฅูู',
    'ุงูู': 'ุฅูู',
    'ุนูู': 'ุนูู',
    'ุญุชู': 'ุญุชู',
    # ุงููููุงุช ุงูุดุงุฆุนุฉ
    'ุงูุถุง': 'ุฃูุถุงู',
    'ุงูุธุง': 'ุฃูุถุงู',
    'ุงูุถุข': 'ุฃูุถุงู',
    'ููุท': 'ููุท',
    'ููุทุข': 'ููุท',
    'ุฌุฏุง': 'ุฌุฏุงู',
    'ุฌุฏุข': 'ุฌุฏุงู',
    'ูุซูุฑุง': 'ูุซูุฑุงู',
    'ูุซูุฑุข': 'ูุซูุฑุงู',
    'ูุฐุง': 'ูุฐุง',
    'ูุฐู': 'ูุฐู',
    'ุฐุงูู': 'ุฐูู',
    'ุฐุงูู': 'ุฐูู',
    'ุงูุชู': 'ุงูุชู',
    'ุงูุฐู': 'ุงูุฐู',
    'ุงูุชู': 'ุงูุชู',
    'ุงููู': 'ุงููู',
    'ุงููุฉ': 'ุงููู',
    'ุงูุฃู': 'ุงูุขู',
    'ุงูุงู': 'ุงูุขู',
    'ุจุนุถ': 'ุจุนุถ',
    'ุฌุฒุก': 'ุฌุฒุก',
    'ุฌุฒุฆ': 'ุฌุฒุก',
    'ููุฏ': 'ููุฏ',
    'ูููุฏ': 'ูููุฏ',
    'ูููู': 'ูููู',
    'ูุงูููู': 'ูุง ูููู',
    'ูุงููุฌุฏ': 'ูุง ููุฌุฏ',
    'ูุงููู': 'ูุง ููู',
    'ูุงุชูุฌุฏ': 'ูุง ุชูุฌุฏ',
}

# ============================================
# ุฏูุงู ุงูุชุตููู
# ============================================

def detect_category(text):
    """ุชุตููู ุงููุต ุชููุงุฆูุงู ุจูุงุกู ุนูู ุงููููุงุช ุงูููุชุงุญูุฉ"""
    text_lower = text.lower()
    scores = defaultdict(int)
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                scores[category] += 1
    
    if not scores:
        return 'references'
    
    return max(scores, key=scores.get)

def get_category_icon(category):
    """ุฅุฑุฌุงุน ุฃููููุฉ ุงูุชุตููู"""
    icons = {
        'medical': '๐ฅ',
        'technical': '๐ป',
        'translation': '๐',
        'references': '๐'
    }
    return icons.get(category, '๐')

# ============================================
# ุฏูุงู ุงูุชุตุญูุญ ุงูุฅููุงุฆู
# ============================================

def correct_arabic_spelling(text):
    """ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ"""
    corrected = text
    
    for wrong, correct in ARABIC_SPELLING_CORRECTIONS.items():
        # ุชุตุญูุญ ูุน ูุฑุงุนุงุฉ ุญุฏูุฏ ุงููููุฉ
        pattern = r'\b' + wrong + r'\b'
        corrected = re.sub(pattern, correct, corrected)
    
    # ุฅุตูุงุญ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    corrected = re.sub(r'\s+', ' ', corrected)
    
    # ุฅุตูุงุญ ุนูุงูุงุช ุงูุชุฑููู
    corrected = corrected.replace(' ,', ',')
    corrected = corrected.replace(' .', '.')
    corrected = corrected.replace('ุุ', 'ุ')
    
    return corrected.strip()

def get_spelling_suggestions(text):
    """ุงูุญุตูู ุนูู ุงูุชุฑุงุญุงุช ุงูุชุตุญูุญ"""
    suggestions = []
    words = re.findall(r'[\u0600-\u06FF]+', text)
    
    for word in words:
        if word in ARABIC_SPELLING_CORRECTIONS:
            suggestions.append({
                'original': word,
                'corrected': ARABIC_SPELLING_CORRECTIONS[word]
            })
    
    return suggestions

# ============================================
# ุฏูุงู ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช
# ============================================

def extract_translations_from_text(text):
    """ุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงููุชุฑุฌูุฉ ูู ูุต"""
    translations = []
    lines = text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # ุชุฌุฑุจุฉ ูู ุงูุฃููุงุท
        for pattern in TRANSLATION_PATTERNS:
            match = re.match(pattern, line.strip())
            if match:
                english = match.group(1).strip()
                arabic = match.group(2).strip()
                
                # ุงูุชุญูู ูู ุตุญุฉ ุงูุงุณุชุฎุฑุงุฌ
                if len(english) > 2 and len(arabic) > 2:
                    # ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ
                    arabic_corrected = correct_arabic_spelling(arabic)
                    
                    translations.append({
                        'english': english,
                        'arabic': arabic,
                        'arabic_corrected': arabic_corrected,
                        'needs_correction': arabic != arabic_corrected,
                        'source': 'text'
                    })
                break
    
    return translations

def extract_translations_from_excel(filepath):
    """ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ููู Excel"""
    translations = []
    
    if not OPENPYXL_AVAILABLE and not PANDAS_AVAILABLE:
        print("โ๏ธ  openpyxl ุฃู pandas ูุทููุจ ููุฑุงุกุฉ ูููุงุช Excel")
        return translations
    
    try:
        if PANDAS_AVAILABLE:
            # ูุญุงููุฉ ูุฑุงุกุฉ ุงูููู
            df = pd.read_excel(filepath, sheet_name=None)
            
            for sheet_name, sheet_df in df.items():
                # ุงูุจุญุซ ุนู ุฃุนูุฏุฉ ุฅูุฌููุฒูุฉ ูุนุฑุจูุฉ
                english_col = None
                arabic_col = None
                
                for col in sheet_df.columns:
                    col_lower = str(col).lower()
                    if 'english' in col_lower or 'ุฅูุฌููุฒู' in col_lower or 'ุงูุฅูุฌููุฒูุฉ' in col_lower:
                        english_col = col
                    elif 'arabic' in col_lower or 'ุนุฑุจู' in col_lower or 'ุงูุนุฑุจูุฉ' in col_lower:
                        arabic_col = col
                
                if english_col and arabic_col:
                    for _, row in sheet_df.iterrows():
                        english = str(row[english_col]).strip() if pd.notna(row[english_col]) else ''
                        arabic = str(row[arabic_col]).strip() if pd.notna(row[arabic_col]) else ''
                        
                        if english and arabic and english != 'nan' and arabic != 'nan':
                            arabic_corrected = correct_arabic_spelling(arabic)
                            
                            translations.append({
                                'english': english,
                                'arabic': arabic,
                                'arabic_corrected': arabic_corrected,
                                'needs_correction': arabic != arabic_corrected,
                                'source': f'excel:{sheet_name}'
                            })
        
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {len(translations)} ุชุฑุฌูุฉ ูู Excel")
        
    except Exception as e:
        print(f"  โ๏ธ  ุฎุทุฃ ูู ูุฑุงุกุฉ Excel: {str(e)}")
    
    return translations

def extract_translations_from_csv(filepath):
    """ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ููู CSV"""
    translations = []
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            header = next(reader, None)
            
            if not header:
                return translations
            
            # ุงูุจุญุซ ุนู ุฃุนูุฏุฉ ุฅูุฌููุฒูุฉ ูุนุฑุจูุฉ
            english_idx = None
            arabic_idx = None
            
            for i, col in enumerate(header):
                col_lower = str(col).lower()
                if 'english' in col_lower or 'ุฅูุฌููุฒู' in col_lower:
                    english_idx = i
                elif 'arabic' in col_lower or 'ุนุฑุจู' in col_lower:
                    arabic_idx = i
            
            if english_idx is not None and arabic_idx is not None:
                for row in reader:
                    if len(row) > max(english_idx, arabic_idx):
                        english = row[english_idx].strip()
                        arabic = row[arabic_idx].strip()
                        
                        if english and arabic:
                            arabic_corrected = correct_arabic_spelling(arabic)
                            
                            translations.append({
                                'english': english,
                                'arabic': arabic,
                                'arabic_corrected': arabic_corrected,
                                'needs_correction': arabic != arabic_corrected,
                                'source': 'csv'
                            })
        
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {len(translations)} ุชุฑุฌูุฉ ูู CSV")
        
    except Exception as e:
        print(f"  โ๏ธ  ุฎุทุฃ ูู ูุฑุงุกุฉ CSV: {str(e)}")
    
    return translations

# ============================================
# ุฏูุงู ุงุณุชุฎุฑุงุฌ ุงูุฃุฑุดููุงุช
# ============================================

BINARY_EXTENSIONS = {
    '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin',
    '.obj', '.o', '.a', '.lib', '.dylib', '.class',
    '.jar', '.war', '.ear', '.apk', '.ipa', '.app', '.dmg',
    '.iso', '.img', '.raw', '.dat', '.db', '.sqlite',
    '.npy', '.npz', '.pt', '.pth', '.h5', '.pkl', '.pickle'
}

IGNORE_FOLDERS = {'venv', '__pycache__', '.git', '.idea', '.vscode', 'node_modules'}

def should_ignore_path(path):
    """ุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงููุณุงุฑ"""
    parts = path.replace('\\', '/').split('/')
    for part in parts:
        if part.startswith('.') or part in IGNORE_FOLDERS:
            return True
    return False

def is_text_content(content_bytes):
    """ูุญุต ูุง ุฅุฐุง ูุงู ุงููุญุชูู ูุตูุงู"""
    try:
        content_bytes.decode('utf-8')
        return True
    except UnicodeDecodeError:
        try:
            content_bytes.decode('latin-1')
            return True
        except UnicodeDecodeError:
            return False

def extract_archive_to_text(archive_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุฃุฑุดูู ุฅูู ููู ูุตู"""
    
    archive_type = None
    if archive_path.lower().endswith('.zip'):
        archive_type = 'zip'
    elif archive_path.lower().endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tar.xz')):
        archive_type = 'tar'
    elif RAR_SUPPORT and archive_path.lower().endswith('.rar'):
        archive_type = 'rar'
    else:
        print(f"  โ๏ธ  ููุน ุงูุฃุฑุดูู ุบูุฑ ูุฏุนูู: {archive_path}")
        return None
    
    files_processed = 0
    files_skipped = 0
    
    try:
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ุงูุฃุฑุดูู: {os.path.basename(archive_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุงุณุชุฎุฑุงุฌ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write("=" * 80 + "\n\n")
            
            if archive_type == 'zip':
                with zipfile.ZipFile(archive_path, 'r') as archive:
                    for file_name in sorted(archive.namelist()):
                        if file_name.endswith('/') or should_ignore_path(file_name):
                            files_skipped += 1
                            continue
                        
                        file_ext = pathlib.Path(file_name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS:
                            files_skipped += 1
                            continue
                        
                        try:
                            with archive.open(file_name, 'r') as f:
                                content_bytes = f.read()
                            
                            if not is_text_content(content_bytes):
                                files_skipped += 1
                                continue
                            
                            content = content_bytes.decode('utf-8', errors='replace')
                            
                            out_file.write(f"ุงุณู ุงูููู: {file_name}\n")
                            out_file.write("-" * 40 + "\n")
                            out_file.write(content)
                            if not content.endswith('\n'):
                                out_file.write('\n')
                            out_file.write("\n" + "=" * 80 + "\n\n")
                            files_processed += 1
                            
                        except Exception:
                            files_skipped += 1
            
            elif archive_type == 'tar':
                with tarfile.open(archive_path, 'r:*') as archive:
                    for member in archive.getmembers():
                        if member.isdir() or should_ignore_path(member.name):
                            files_skipped += 1
                            continue
                        
                        file_ext = pathlib.Path(member.name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS:
                            files_skipped += 1
                            continue
                        
                        try:
                            f = archive.extractfile(member)
                            if not f:
                                continue
                            content_bytes = f.read()
                            
                            if not is_text_content(content_bytes):
                                files_skipped += 1
                                continue
                            
                            content = content_bytes.decode('utf-8', errors='replace')
                            
                            out_file.write(f"ุงุณู ุงูููู: {member.name}\n")
                            out_file.write("-" * 40 + "\n")
                            out_file.write(content)
                            if not content.endswith('\n'):
                                out_file.write('\n')
                            out_file.write("\n" + "=" * 80 + "\n\n")
                            files_processed += 1
                            
                        except Exception:
                            files_skipped += 1
            
            elif archive_type == 'rar' and RAR_SUPPORT:
                with rarfile.RarFile(archive_path, 'r') as archive:
                    for file_name in sorted(archive.namelist()):
                        if file_name.endswith('/') or should_ignore_path(file_name):
                            files_skipped += 1
                            continue
                        
                        file_ext = pathlib.Path(file_name).suffix.lower()
                        if file_ext in BINARY_EXTENSIONS:
                            files_skipped += 1
                            continue
                        
                        try:
                            with archive.open(file_name, 'r') as f:
                                content_bytes = f.read()
                            
                            if not is_text_content(content_bytes):
                                files_skipped += 1
                                continue
                            
                            content = content_bytes.decode('utf-8', errors='replace')
                            
                            out_file.write(f"ุงุณู ุงูููู: {file_name}\n")
                            out_file.write("-" * 40 + "\n")
                            out_file.write(content)
                            if not content.endswith('\n'):
                                out_file.write('\n')
                            out_file.write("\n" + "=" * 80 + "\n\n")
                            files_processed += 1
                            
                        except Exception:
                            files_skipped += 1
            
            # ููุฎุต
            out_file.write("\n" + "=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุนุงูุฌุฉ: {files_processed}\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
            out_file.write("=" * 80 + "\n")
        
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {files_processed} ููู ุฅูู: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู ุงุณุชุฎุฑุงุฌ ุงูุฃุฑุดูู: {str(e)}")
        return None

def extract_folder_to_text(folder_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฌูุฏ ุฅูู ููู ูุตู"""
    
    files_processed = 0
    files_skipped = 0
    
    try:
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ุงููุฌูุฏ: {os.path.basename(folder_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุงุณุชุฎุฑุงุฌ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write("=" * 80 + "\n\n")
            
            for root, dirs, files in os.walk(folder_path):
                # ุชุฌุงูู ุงููุฌูุฏุงุช ุงููุฎููุฉ ูุงููุนูุงุฑูุฉ
                dirs[:] = [d for d in dirs if not d.startswith('.') and d not in IGNORE_FOLDERS]
                
                for file in sorted(files):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, folder_path)
                    
                    if should_ignore_path(rel_path):
                        files_skipped += 1
                        continue
                    
                    file_ext = pathlib.Path(file).suffix.lower()
                    if file_ext in BINARY_EXTENSIONS:
                        files_skipped += 1
                        continue
                    
                    try:
                        with open(file_path, 'rb') as f:
                            content_bytes = f.read()
                        
                        if not is_text_content(content_bytes):
                            files_skipped += 1
                            continue
                        
                        content = content_bytes.decode('utf-8', errors='replace')
                        
                        out_file.write(f"ุงุณู ุงูููู: {rel_path}\n")
                        out_file.write("-" * 40 + "\n")
                        out_file.write(content)
                        if not content.endswith('\n'):
                            out_file.write('\n')
                        out_file.write("\n" + "=" * 80 + "\n\n")
                        files_processed += 1
                        
                    except Exception:
                        files_skipped += 1
            
            # ููุฎุต
            out_file.write("\n" + "=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุนุงูุฌุฉ: {files_processed}\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
            out_file.write("=" * 80 + "\n")
        
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {files_processed} ููู ุฅูู: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"  โ ุฎุทุฃ ูู ุงุณุชุฎุฑุงุฌ ุงููุฌูุฏ: {str(e)}")
        return None

# ============================================
# ุงููุนุงูุฌ ุงูุฑุฆูุณู
# ============================================

class KnowledgeProcessor:
    """ูุนุงูุฌ ุงููููุงุช ุงููุนุฑููุฉ ุงูุฑุฆูุณู"""
    
    def __init__(self, input_path, output_path):
        self.input_path = input_path
        self.output_path = output_path
        self.translations = []
        self.processed_files = []
        self.categories = defaultdict(list)
        
        # ุฅูุดุงุก ูุฌูุฏุงุช ุงูุฅุฎุฑุงุฌ
        self.setup_output_dirs()
    
    def setup_output_dirs(self):
        """ุฅูุดุงุก ูููู ูุฌูุฏุงุช ุงูุฅุฎุฑุงุฌ"""
        dirs = [
            self.output_path,
            os.path.join(self.output_path, 'technical', 'linux'),
            os.path.join(self.output_path, 'technical', 'windows'),
            os.path.join(self.output_path, 'medical', 'orthopedics'),
            os.path.join(self.output_path, 'translation'),
            os.path.join(self.output_path, 'scripts'),
            os.path.join(self.output_path, 'digests'),
            os.path.join(self.output_path, 'extracted')
        ]
        
        for d in dirs:
            os.makedirs(d, exist_ok=True)
    
    def process_file(self, filepath):
        """ูุนุงูุฌุฉ ููู ูุงุญุฏ"""
        filename = os.path.basename(filepath)
        ext = pathlib.Path(filepath).suffix.lower()
        
        print(f"\n๐ ูุนุงูุฌุฉ: {filename}")
        
        # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ุญุณุจ ููุน ุงูููู
        translations = []
        
        if ext in ['.xlsx', '.xls']:
            translations = extract_translations_from_excel(filepath)
        elif ext == '.csv':
            translations = extract_translations_from_csv(filepath)
        elif ext in ['.txt', '.md', '.py', '.js', '.json', '.xml', '.html', '.css']:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            translations = extract_translations_from_text(content)
        
        if translations:
            self.translations.extend(translations)
            print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {len(translations)} ุชุฑุฌูุฉ")
        
        # ุชุตููู ุงูููู
        if ext not in ['.xlsx', '.xls', '.csv']:
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read(5000)  # ูุฑุงุกุฉ ุฃูู 5000 ุญุฑู ููุชุตููู
            
            category = detect_category(content)
            self.categories[category].append(filepath)
            
            # ูุณุฎ ุงูููู ุฅูู ุงููุฌูุฏ ุงูููุงุณุจ
            dest_dir = os.path.join(self.output_path, category)
            if category == 'technical':
                # ุชุญุฏูุฏ ููููุณ ุฃู ูููุฏูุฒ
                if 'linux' in content.lower() or 'arch' in content.lower() or 'garuda' in content.lower():
                    dest_dir = os.path.join(self.output_path, 'technical', 'linux')
                else:
                    dest_dir = os.path.join(self.output_path, 'technical', 'windows')
            
            dest_path = os.path.join(dest_dir, filename)
            
            # ุชุฌูุจ ุงููุชุงุจุฉ ููู ุงููููุงุช
            counter = 1
            while os.path.exists(dest_path):
                name = pathlib.Path(filename).stem
                dest_path = os.path.join(dest_dir, f"{name}_{counter}{ext}")
                counter += 1
            
            with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            with open(dest_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"  โ ุชู ุชุตููู ุงูููู: {get_category_icon(category)} {category}")
        
        self.processed_files.append(filepath)
    
    def process_archive(self, filepath):
        """ูุนุงูุฌุฉ ููู ูุถุบูุท"""
        filename = pathlib.Path(filepath).stem
        output_file = os.path.join(self.output_path, 'extracted', f"{filename}_extracted.txt")
        
        result = extract_archive_to_text(filepath, output_file)
        
        if result:
            # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ุงููุญุชูู ุงููุณุชุฎุฑุฌ
            with open(result, 'r', encoding='utf-8') as f:
                content = f.read()
            
            translations = extract_translations_from_text(content)
            if translations:
                self.translations.extend(translations)
                print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {len(translations)} ุชุฑุฌูุฉ ูู ุงูุฃุฑุดูู")
            
            # ุชุตููู ุงููุญุชูู
            category = detect_category(content)
            self.categories[category].append(filepath)
            
            self.processed_files.append(filepath)
    
    def process_folder(self, folderpath):
        """ูุนุงูุฌุฉ ูุฌูุฏ"""
        foldername = os.path.basename(folderpath)
        output_file = os.path.join(self.output_path, 'extracted', f"{foldername}_extracted.txt")
        
        result = extract_folder_to_text(folderpath, output_file)
        
        if result:
            # ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ุงููุญุชูู ุงููุณุชุฎุฑุฌ
            with open(result, 'r', encoding='utf-8') as f:
                content = f.read()
            
            translations = extract_translations_from_text(content)
            if translations:
                self.translations.extend(translations)
                print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ {len(translations)} ุชุฑุฌูุฉ ูู ุงููุฌูุฏ")
            
            # ุชุตููู ุงููุญุชูู
            category = detect_category(content)
            self.categories[category].append(folderpath)
            
            self.processed_files.append(folderpath)
    
    def process_all(self):
        """ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช ูู ุงููุณุงุฑ ุงููุฏุฎู"""
        print("\n" + "=" * 60)
        print("๐ ุจุฏุก ูุนุงูุฌุฉ ุงููููุงุช")
        print("=" * 60)
        
        if os.path.isfile(self.input_path):
            # ููู ูุงุญุฏ
            ext = pathlib.Path(self.input_path).suffix.lower()
            
            if ext in ['.zip', '.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tar.xz']:
                self.process_archive(self.input_path)
            elif ext == '.rar' and RAR_SUPPORT:
                self.process_archive(self.input_path)
            else:
                self.process_file(self.input_path)
        
        elif os.path.isdir(self.input_path):
            # ูุฌูุฏ ูุงูู
            for item in os.listdir(self.input_path):
                item_path = os.path.join(self.input_path, item)
                
                if os.path.isfile(item_path):
                    ext = pathlib.Path(item_path).suffix.lower()
                    
                    if ext in ['.zip', '.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tar.xz']:
                        self.process_archive(item_path)
                    elif ext == '.rar' and RAR_SUPPORT:
                        self.process_archive(item_path)
                    else:
                        self.process_file(item_path)
                
                elif os.path.isdir(item_path):
                    self.process_folder(item_path)
        
        # ุญูุธ ุงููุชุงุฆุฌ
        self.save_translations()
        self.save_report()
    
    def save_translations(self):
        """ุญูุธ ุงูุชุฑุฌูุงุช ูู ููู CSV"""
        if not self.translations:
            print("\nโ๏ธ  ูู ูุชู ุงูุนุซูุฑ ุนูู ุชุฑุฌูุงุช")
            return
        
        output_file = os.path.join(self.output_path, 'translation', 'translations_collected.csv')
        
        # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช
        unique_translations = []
        seen = set()
        
        for t in self.translations:
            key = (t['english'].lower(), t['arabic'])
            if key not in seen:
                seen.add(key)
                unique_translations.append(t)
        
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['English', 'Arabic (Original)', 'Arabic (Corrected)', 'Needs Correction', 'Source'])
            
            for t in unique_translations:
                writer.writerow([
                    t['english'],
                    t['arabic'],
                    t['arabic_corrected'],
                    'Yes' if t['needs_correction'] else 'No',
                    t['source']
                ])
        
        corrections_count = sum(1 for t in unique_translations if t['needs_correction'])
        
        print(f"\n" + "=" * 60)
        print(f"๐ ุชุฑุฌูุงุช ูุญููุธุฉ")
        print(f"=" * 60)
        print(f"  โ ุฅุฌูุงูู ุงูุชุฑุฌูุงุช: {len(unique_translations)}")
        print(f"  โ ุชุฑุฌูุงุช ุชุญุชุงุฌ ุชุตุญูุญ: {corrections_count}")
        print(f"  โ ุงูููู: {output_file}")
    
    def save_report(self):
        """ุญูุธ ุชูุฑูุฑ ุงููุนุงูุฌุฉ"""
        report_file = os.path.join(self.output_path, 'PROCESSING_REPORT.md')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ุชูุฑูุฑ ูุนุงูุฌุฉ ุงููููุงุช\n\n")
            f.write(f"**ุชุงุฑูุฎ ุงููุนุงูุฌุฉ:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ๐ ุฅุญุตุงุฆูุงุช\n\n")
            f.write(f"| ุงูุจูุฏ | ุงูุนุฏุฏ |\n")
            f.write(f"|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงููุนุงูุฌุฉ | {len(self.processed_files)} |\n")
            f.write(f"| ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ | {len(self.translations)} |\n\n")
            
            f.write("## ๐ ุงูุชุตููู\n\n")
            for category, files in self.categories.items():
                icon = get_category_icon(category)
                f.write(f"### {icon} {category.title()}\n\n")
                for file in files:
                    f.write(f"- {os.path.basename(file)}\n")
                f.write("\n")
            
            f.write("## โ ุงููููุงุช ุงููุนุงูุฌุฉ\n\n")
            for file in self.processed_files:
                f.write(f"- {file}\n")
        
        print(f"\n๐ ุชูุฑูุฑ ูุญููุธ: {report_file}")

# ============================================
# ููุทุฉ ุงูุฏุฎูู
# ============================================

def main():
    parser = argparse.ArgumentParser(
        description='ูุนุงูุฌ ุงููููุงุช ุงููุนุฑููุฉ - ุงุณุชุฎุฑุงุฌ ูุชุตููู ูุชุตุญูุญ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ุฃูุซูุฉ:
  python knowledge_processor.py --input ./files --output ./processed
  python knowledge_processor.py -i document.xlsx -o ./output
  python knowledge_processor.py -i ./project_folder -o ./knowledge_base
        """
    )
    
    parser.add_argument('-i', '--input', required=True, help='ูุณุงุฑ ุงูููู ุฃู ุงููุฌูุฏ ุงููุฏุฎู')
    parser.add_argument('-o', '--output', default='./knowledge_base', help='ูุณุงุฑ ุงูุฅุฎุฑุงุฌ (ุงูุชุฑุงุถู: ./knowledge_base)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"โ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {args.input}")
        sys.exit(1)
    
    processor = KnowledgeProcessor(args.input, args.output)
    processor.process_all()
    
    print("\n" + "=" * 60)
    print("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
    print("=" * 60)
    print(f"\n๐ ุงููููุงุช ูู: {args.output}")
    print("\n๐ก ุงูุฎุทูุฉ ุงูุชุงููุฉ: ุฑุงุฌุน ุงููููุงุช ุงููุงุชุฌุฉ ุซู ุดุงุฑููุง ููุชุญูู ูุงูุฑูุน ุฅูู GitHub")

if __name__ == '__main__':
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/text_processor_gui.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
  ูุนุงูุฌ ุงููุตูุต ุงูุฐูู - ูุงุฌูุฉ ุฑุณูููุฉ ูุน ุฏุนู Ollama
  Intelligent Text Processor - GUI with Ollama Support
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ุงููููุฒุงุช:
  - ูุงุฌูุฉ ุฑุณูููุฉ ุณููุฉ (tkinter)
  - ูุธุงู logging ูุชุทูุฑ ูุน ุฃููุงู
  - ุดุฑูุท ุชูุฏู ุชูุงุนูู
  - ุชูุงูู ูุน Ollama ููุชุตููู ูุงูุชุฑุฌูุฉ ุงูุฐููุฉ
  - ุฏุนู ุงูููุงุฐุฌ ุงููุญููุฉ (qwen, llama, mistral...)
  - ูุนุงูุฌุฉ ุฃูููุงูู ูุงููุฉ

ุงูุงุณุชุฎุฏุงู:
  python text_processor_gui.py

ุงููุชุทูุจุงุช:
  pip install datasketch tqdm chardet requests
  # ููุชุดุบูู Ollama:
  # ollama pull qwen2.5:7b
  # ollama serve
"""

import os
import re
import csv
import json
import pickle
import hashlib
import logging
import threading
import queue
import subprocess
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Set, Generator, Any
from collections import defaultdict
from dataclasses import dataclass, field, asdict
import tkinter as tk
from tkinter import ttk, scrolledtext, filedialog, messagebox
import tkinter.font as tkfont

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

try:
    from datasketch import MinHash, MinHashLSH
    HAS_DATASKETCH = True
except ImportError:
    HAS_DATASKETCH = False

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

try:
    import requests
    HAS_REQUESTS = True
except ImportError:
    HAS_REQUESTS = False

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงูุซูุงุจุช
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

KB = 1024
MB = 1024 * KB
GB = 1024 * MB

MAX_SEGMENT_SIZE = 50 * KB
MIN_SEGMENT_SIZE = 500
SIMILARITY_THRESHOLD = 0.85

OLLAMA_API_URL = "http://localhost:11434/api"

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

SPELLING_CORRECTIONS = {
    'ุงูุดุงุก': 'ุฅูุดุงุก', 'ุงูุงู': 'ุงูุขู', 'ุงูู': 'ุฅูู', 'ุงุฐุง': 'ุฅุฐุง',
    'ุงูุง': 'ุฅูุง', 'ุงู': 'ุฃู', 'ุงูู': 'ุฃูู', 'ุงูุถุง': 'ุฃูุถุงู',
    'ุฐุงูู': 'ุฐูู', 'ูุงุฐุง': 'ูุฐุง', 'ูุงุฐู': 'ูุฐู',
    'ุงููุฏ': 'ุฃููุฏ', 'ุงุณุชุทูุน': 'ุฃุณุชุทูุน', 'ุงุณุชุฎุฏู': 'ุฃุณุชุฎุฏู',
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูููุงุช ุงูุชุตููู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

CATEGORY_KEYWORDS = {
    'medical': {
        'ar': ['ุทุจู', 'ุทุจ', 'ุนุธุงู', 'ุฌุฑุงุญุฉ', 'ูุณุชุดูู', 'ุทุจูุจ', 'ุนูุงุฌ', 'ูุฑุถ',
               'ูุณุฑ', 'ุบุถุฑูู', 'ููุตู', 'ุฑูุจุฉ', 'ูุฎุฐ', 'ุนููุฏ ููุฑู', 'ุฃุดุนุฉ'],
        'en': ['medical', 'orthopedic', 'surgery', 'hospital', 'doctor', 'treatment',
               'fracture', 'bone', 'joint', 'knee', 'hip', 'spine', 'implant']
    },
    'technical': {
        'ar': ['ููููุณ', 'ูููุฏูุฒ', 'ุจุฑูุงูุฌ', 'ุชุซุจูุช', 'ุญุฒูุฉ', 'ูุธุงู', 'ุณูุฑูุฑ',
               'ููุฏ', 'ุจุฑูุฌุฉ', 'ุณูุฑูุจุช', 'ุฃูุฑ', 'ุทุฑููุฉ', 'git', 'python'],
        'en': ['linux', 'windows', 'software', 'install', 'package', 'system',
               'server', 'code', 'programming', 'script', 'terminal']
    },
    'translation': {
        'ar': ['ุชุฑุฌูุฉ', 'ูุชุฑุฌู', 'ูุบุฉ', 'ุนุฑุจู', 'ุฅูุฌููุฒู', 'ูุงููุณ', 'ูุตุทูุญ'],
        'en': ['translation', 'translator', 'language', 'arabic', 'english', 'dictionary']
    },
    'reference': {
        'ar': ['ููุฑุณ', 'ูุฑุฌุน', 'ุฏููู', 'ูุงุฆูุฉ', 'ุฌุฏูู', 'ุจูุงูุงุช', 'ุฅุญุตุงุฆูุงุช'],
        'en': ['index', 'reference', 'guide', 'list', 'table', 'data', 'statistics']
    }
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุธุงู Logging ุงููุชุทูุฑ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class ColoredFormatter(logging.Formatter):
    """ูููุณู ูููู ููู logging"""
    
    COLORS = {
        'DEBUG': '\033[36m',    # Cyan
        'INFO': '\033[32m',     # Green
        'WARNING': '\033[33m',  # Yellow
        'ERROR': '\033[31m',    # Red
        'CRITICAL': '\033[35m', # Magenta
        'RESET': '\033[0m'      # Reset
    }
    
    EMOJI = {
        'DEBUG': '๐',
        'INFO': 'โ',
        'WARNING': 'โ๏ธ',
        'ERROR': 'โ',
        'CRITICAL': '๐ฅ'
    }
    
    def format(self, record):
        emoji = self.EMOJI.get(record.levelname, '')
        timestamp = datetime.now().strftime('%H:%M:%S')
        
        # ุชูุณูู ูููุงุฌูุฉ ุงูุฑุณูููุฉ
        return f"[{timestamp}] {emoji} {record.getMessage()}"


class QueueHandler(logging.Handler):
    """ูุนุงูุฌ ูุฅุฑุณุงู ุงูู logs ุฅูู ุทุงุจูุฑ"""
    
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue
    
    def emit(self, record):
        self.log_queue.put(self.format(record))


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุนููู Ollama
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class OllamaClient:
    """ุนููู ููุชูุงุตู ูุน Ollama API"""
    
    def __init__(self, model: str = "qwen2.5:7b", logger=None):
        self.model = model
        self.base_url = OLLAMA_API_URL
        self.logger = logger or logging.getLogger(__name__)
        self.available = self._check_availability()
    
    def _check_availability(self) -> bool:
        """ุงูุชุญูู ูู ุชููุฑ Ollama"""
        if not HAS_REQUESTS:
            return False
        
        try:
            response = requests.get(f"{self.base_url}/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m.get('name', '') for m in models]
                self.logger.info(f"๐ฆ Ollama ูุชููุฑ. ุงูููุงุฐุฌ: {', '.join(model_names[:5])}")
                return True
        except Exception as e:
            self.logger.warning(f"โ๏ธ Ollama ุบูุฑ ูุชููุฑ: {e}")
        
        return False
    
    def get_models(self) -> List[str]:
        """ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงูููุงุฐุฌ ุงููุชููุฑุฉ"""
        if not self.available:
            return []
        
        try:
            response = requests.get(f"{self.base_url}/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return [m.get('name', '') for m in models]
        except:
            pass
        
        return []
    
    def classify_text(self, text: str) -> Tuple[str, float]:
        """ุชุตููู ุงููุต ุจุงุณุชุฎุฏุงู Ollama"""
        if not self.available:
            return 'misc', 0.0
        
        prompt = f"""ุตูู ุงููุต ุงูุชุงูู ุฅูู ูุงุญุฏุฉ ูู ุงููุฆุงุช: medical, technical, translation, reference, misc
        
ุงููุต:
{text[:500]}

ุฃุฌุจ ููุท ุจุงุณู ุงููุฆุฉ ูุฑูู ุงูุซูุฉ (0-1) ุจูุฐุง ุงูุดูู: category,confidence"""

        try:
            response = requests.post(
                f"{self.base_url}/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 50
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json().get('response', '')
                # ุชุญููู ุงููุชูุฌุฉ
                parts = result.strip().split(',')
                if len(parts) >= 2:
                    category = parts[0].strip().lower()
                    confidence = float(parts[1].strip())
                    return category, min(max(confidence, 0.0), 1.0)
        except Exception as e:
            self.logger.warning(f"โ๏ธ ุฎุทุฃ ูู ุชุตููู Ollama: {e}")
        
        return 'misc', 0.0
    
    def translate_text(self, text: str, target_lang: str = 'ar') -> str:
        """ุชุฑุฌูุฉ ุงููุต ุจุงุณุชุฎุฏุงู Ollama"""
        if not self.available:
            return text
        
        prompt = f"""ุชุฑุฌู ุงููุต ุงูุชุงูู ุฅูู ุงููุบุฉ {'ุงูุนุฑุจูุฉ' if target_lang == 'ar' else 'ุงูุฅูุฌููุฒูุฉ'}.
ุฃุฌุจ ุจุงูุชุฑุฌูุฉ ููุท ุจุฏูู ุดุฑุญ:

{text}"""

        try:
            response = requests.post(
                f"{self.base_url}/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 500
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json().get('response', text).strip()
        except Exception as e:
            self.logger.warning(f"โ๏ธ ุฎุทุฃ ูู ุชุฑุฌูุฉ Ollama: {e}")
        
        return text
    
    def correct_text(self, text: str) -> Tuple[str, List[str]]:
        """ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ูุงููุญููุฉ ุจุงุณุชุฎุฏุงู Ollama"""
        if not self.available:
            corrected = text
            corrections = []
            for wrong, right in SPELLING_CORRECTIONS.items():
                if wrong in corrected:
                    corrected = corrected.replace(wrong, right)
                    corrections.append(f"{wrong} โ {right}")
            return corrected, corrections
        
        prompt = f"""ุตุญุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ูุงููุญููุฉ ูู ุงููุต ุงูุนุฑุจู ุงูุชุงูู.
ุฃุฌุจ ุจุงููุต ุงููุตุญุญ ููุท:

{text}"""

        try:
            response = requests.post(
                f"{self.base_url}/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 1000
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                corrected = response.json().get('response', text).strip()
                # ุงูุชุดุงู ุงูุชุตุญูุญุงุช
                corrections = []
                if corrected != text:
                    corrections.append("ุชู ุงูุชุตุญูุญ ุจูุงุณุทุฉ Ollama")
                return corrected, corrections
        except Exception as e:
            self.logger.warning(f"โ๏ธ ุฎุทุฃ ูู ุชุตุญูุญ Ollama: {e}")
        
        return text, []
    
    def extract_translations(self, text: str) -> List[Dict]:
        """ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ุจุงุณุชุฎุฏุงู Ollama"""
        if not self.available or len(text) < 50:
            return []
        
        prompt = f"""ุงุณุชุฎุฑุฌ ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ (ุฅูุฌููุฒู-ุนุฑุจู) ูู ุงููุต ุงูุชุงูู.
ุฃุฌุจ ุจุตูุบุฉ JSON:
[{{"en": "English text", "ar": "ุงููุต ุงูุนุฑุจู"}}]

ุงููุต:
{text[:1000]}"""

        try:
            response = requests.post(
                f"{self.base_url}/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "num_predict": 500
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json().get('response', '')
                # ุงุณุชุฎุฑุงุฌ JSON
                json_match = re.search(r'\[.*\]', result, re.DOTALL)
                if json_match:
                    pairs = json.loads(json_match.group())
                    return [{'english': p['en'], 'arabic': p['ar']} for p in pairs if 'en' in p and 'ar' in p]
        except Exception as e:
            self.logger.warning(f"โ๏ธ ุฎุทุฃ ูู ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช: {e}")
        
        return []


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุณุงุนุฏุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def generate_id(content: str) -> str:
    return hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()[:16]


def detect_encoding(file_path: Path) -> str:
    if HAS_CHARDET:
        with open(file_path, 'rb') as f:
            raw = f.read(10000)
            result = chardet.detect(raw)
            return result.get('encoding', 'utf-8')
    return 'utf-8'


def detect_language(text: str) -> str:
    has_ar = bool(re.search(r'[\u0600-\u06FF]', text))
    has_en = bool(re.search(r'[A-Za-z]', text))
    
    if has_ar and has_en:
        return 'mixed'
    elif has_ar:
        return 'ar'
    elif has_en:
        return 'en'
    return 'other'


def calculate_quality_score(text: str) -> float:
    if not text or len(text) < 10:
        return 0.0
    
    score = 1.0
    
    # ูุณุจุฉ ุงูุฑููุฒ
    symbol_count = len(re.findall(r'[^\w\s\u0600-\u06FF]', text))
    symbol_ratio = symbol_count / max(len(text), 1)
    if symbol_ratio > 0.3:
        score *= (1 - symbol_ratio)
    
    # garbage characters
    garbage = len(re.findall(r'[๏ฟฝ\x00-\x08\x0B\x0C\x0E-\x1F]', text))
    if garbage > 0:
        score *= max(0.1, 1 - (garbage / 100))
    
    return max(0.0, min(1.0, round(score, 3)))


def classify_text_basic(text: str) -> Tuple[str, float]:
    """ุชุตููู ุฃุณุงุณู ุจุงููููุงุช ุงูููุชุงุญูุฉ"""
    text_lower = text.lower()
    scores = defaultdict(float)
    
    for category, langs in CATEGORY_KEYWORDS.items():
        for lang, keywords in langs.items():
            for kw in keywords:
                if kw.lower() in text_lower:
                    scores[category] += 1
    
    if not scores:
        return 'misc', 0.0
    
    best_category = max(scores, key=scores.get)
    confidence = min(scores[best_category] / 10, 1.0)
    
    return best_category, confidence


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุนุงูุฌ ุงููุตูุต
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class TextProcessor:
    """ูุนุงูุฌ ุงููุตูุต ุงูุฃุณุงุณู"""
    
    def __init__(self, input_dir: str, output_dir: str, use_ollama: bool = True,
                 ollama_model: str = "qwen2.5:7b", logger=None):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.logger = logger or logging.getLogger(__name__)
        
        # ุฅูุดุงุก ูุฌูุฏุงุช ุงูุฅุฎุฑุงุฌ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'segments').mkdir(exist_ok=True)
        (self.output_dir / 'translations').mkdir(exist_ok=True)
        
        # ุฅุญุตุงุฆูุงุช
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'total_segments': 0,
            'unique_segments': 0,
            'duplicates_found': 0,
            'translations_extracted': 0,
            'corrections_made': 0,
            'categories': defaultdict(int)
        }
        
        # ุงูุชูุฏู
        self.seen_hashes: Set[str] = set()
        self.minhash_lsh = None
        
        if HAS_DATASKETCH:
            self.minhash_lsh = MinHashLSH(threshold=SIMILARITY_THRESHOLD, num_perm=128)
        
        # Ollama
        self.use_ollama = use_ollama
        self.ollama = None
        if use_ollama and HAS_REQUESTS:
            self.ollama = OllamaClient(model=ollama_model, logger=self.logger)
    
    def get_files(self) -> List[Path]:
        extensions = {'.txt', '.md', '.csv', '.json', '.log', '.py', '.js', '.html'}
        files = []
        for ext in extensions:
            files.extend(self.input_dir.rglob(f'*{ext}'))
        return sorted(set(files))
    
    def segment_file(self, file_path: Path) -> Generator[Dict, None, None]:
        try:
            encoding = detect_encoding(file_path)
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read()
        except Exception as e:
            self.logger.error(f"ุฎุทุฃ ูู ูุฑุงุกุฉ {file_path.name}: {e}")
            return
        
        lines = content.split('\n')
        current_segment = []
        current_start = 1
        segment_idx = 0
        
        for i, line in enumerate(lines, 1):
            current_segment.append(line)
            current_content = '\n'.join(current_segment)
            
            should_split = (
                len(current_content) >= MAX_SEGMENT_SIZE or
                (len(current_content) >= MIN_SEGMENT_SIZE and
                 (line.strip() == '' or line.strip().startswith('#')))
            )
            
            if should_split and len(current_content) >= MIN_SEGMENT_SIZE:
                content_str = '\n'.join(current_segment).strip()
                if content_str:
                    yield {
                        'id': f"{file_path.stem}_{segment_idx}",
                        'content': content_str,
                        'source_file': file_path.name,
                        'start_line': current_start,
                        'end_line': i,
                        'language': detect_language(content_str)
                    }
                    segment_idx += 1
                
                current_segment = []
                current_start = i + 1
        
        if current_segment:
            content_str = '\n'.join(current_segment).strip()
            if content_str and len(content_str) >= MIN_SEGMENT_SIZE:
                yield {
                    'id': f"{file_path.stem}_{segment_idx}",
                    'content': content_str,
                    'source_file': file_path.name,
                    'start_line': current_start,
                    'end_line': len(lines),
                    'language': detect_language(content_str)
                }
    
    def process_segment(self, segment: Dict) -> Dict:
        content = segment['content']
        
        # ุงูุชุตููู
        if self.ollama and self.ollama.available and self.use_ollama:
            segment['category'], segment['confidence'] = self.ollama.classify_text(content)
        else:
            segment['category'], segment['confidence'] = classify_text_basic(content)
        
        # ุงูุฌูุฏุฉ
        segment['quality_score'] = calculate_quality_score(content)
        
        # ุงูุชุตุญูุญ
        if segment['language'] in ('ar', 'mixed'):
            if self.ollama and self.ollama.available and self.use_ollama:
                corrected, corrections = self.ollama.correct_text(content)
            else:
                corrected = content
                corrections = []
                for wrong, right in SPELLING_CORRECTIONS.items():
                    if wrong in corrected:
                        corrected = corrected.replace(wrong, right)
                        corrections.append(f"{wrong} โ {right}")
            
            segment['corrected_content'] = corrected
            segment['corrections'] = corrections
        
        # ูุญุต ุงูุชูุฑุงุฑ
        content_hash = generate_id(content)
        if content_hash in self.seen_hashes:
            segment['is_duplicate'] = True
        else:
            self.seen_hashes.add(content_hash)
            segment['is_duplicate'] = False
        
        return segment
    
    def save_segment(self, segment: Dict):
        if segment.get('is_duplicate', False) or segment.get('quality_score', 0) < 0.3:
            return
        
        category_dir = self.output_dir / 'segments' / segment['category']
        category_dir.mkdir(exist_ok=True)
        
        file_path = category_dir / f"{segment['id']}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# ูุตุฏุฑ: {segment['source_file']}\n")
            f.write(f"# ูุบุฉ: {segment['language']}\n")
            f.write(f"# ุฌูุฏุฉ: {segment.get('quality_score', 0)}\n")
            f.write(f"# ุชุตููู: {segment['category']} ({segment.get('confidence', 0)})\n")
            f.write("\n" + "=" * 60 + "\n\n")
            f.write(segment.get('corrected_content', segment['content']))


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูุงุฌูุฉ tkinter
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class TextProcessorGUI:
    """ุงููุงุฌูุฉ ุงูุฑุณูููุฉ"""
    
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("๐ง ูุนุงูุฌ ุงููุตูุต ุงูุฐูู - Intelligent Text Processor")
        self.root.geometry("1000x700")
        self.root.minsize(800, 600)
        
        # ูุชุบูุฑุงุช
        self.input_path = tk.StringVar()
        self.output_path = tk.StringVar()
        self.use_ollama = tk.BooleanVar(value=True)
        self.ollama_model = tk.StringVar(value="qwen2.5:7b")
        self.is_processing = False
        self.stop_requested = False
        
        # ุทุงุจูุฑ ุงูู logs
        self.log_queue = queue.Queue()
        
        # ุฅุนุฏุงุฏ ุงูู logging
        self.setup_logging()
        
        # ุจูุงุก ุงููุงุฌูุฉ
        self.build_ui()
        
        # ุจุฏุก ุชุญุฏูุซ ุงูู logs
        self.update_logs()
        
        # ุงูุชุญูู ูู Ollama
        self.check_ollama()
    
    def setup_logging(self):
        """ุฅุนุฏุงุฏ ูุธุงู ุงูู logging"""
        self.logger = logging.getLogger('TextProcessor')
        self.logger.setLevel(logging.DEBUG)
        
        # ุฅุถุงูุฉ QueueHandler
        queue_handler = QueueHandler(self.log_queue)
        queue_handler.setFormatter(ColoredFormatter())
        self.logger.addHandler(queue_handler)
    
    def build_ui(self):
        """ุจูุงุก ูุงุฌูุฉ ุงููุณุชุฎุฏู"""
        
        # โโโ ุงูุฅุทุงุฑ ุงูุฑุฆูุณู โโโ
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.pack(fill=tk.BOTH, expand=True)
        
        # โโโ ุดุฑูุท ุงูุนููุงู โโโ
        title_frame = ttk.Frame(main_frame)
        title_frame.pack(fill=tk.X, pady=(0, 10))
        
        title_label = ttk.Label(
            title_frame,
            text="๐ง ูุนุงูุฌ ุงููุตูุต ุงูุฐูู ุงูุดุงูู",
            font=('Arial', 16, 'bold')
        )
        title_label.pack(side=tk.LEFT)
        
        # โโโ ุฅุทุงุฑ ุงูุฅุนุฏุงุฏุงุช โโโ
        settings_frame = ttk.LabelFrame(main_frame, text="โ๏ธ ุงูุฅุนุฏุงุฏุงุช", padding="10")
        settings_frame.pack(fill=tk.X, pady=(0, 10))
        
        # ูุณุงุฑ ุงูุฅุฏุฎุงู
        input_frame = ttk.Frame(settings_frame)
        input_frame.pack(fill=tk.X, pady=5)
        
        ttk.Label(input_frame, text="๐ ูุฌูุฏ ุงูุฅุฏุฎุงู:").pack(side=tk.LEFT)
        ttk.Entry(input_frame, textvariable=self.input_path, width=50).pack(side=tk.LEFT, padx=5)
        ttk.Button(input_frame, text="ุงุณุชุนุฑุงุถ...", command=self.browse_input).pack(side=tk.LEFT)
        
        # ูุณุงุฑ ุงูุฅุฎุฑุงุฌ
        output_frame = ttk.Frame(settings_frame)
        output_frame.pack(fill=tk.X, pady=5)
        
        ttk.Label(output_frame, text="๐ ูุฌูุฏ ุงูุฅุฎุฑุงุฌ:").pack(side=tk.LEFT)
        ttk.Entry(output_frame, textvariable=self.output_path, width=50).pack(side=tk.LEFT, padx=5)
        ttk.Button(output_frame, text="ุงุณุชุนุฑุงุถ...", command=self.browse_output).pack(side=tk.LEFT)
        
        # ุฅุนุฏุงุฏุงุช Ollama
        ollama_frame = ttk.Frame(settings_frame)
        ollama_frame.pack(fill=tk.X, pady=5)
        
        ttk.Checkbutton(
            ollama_frame,
            text="๐ฆ ุงุณุชุฎุฏุงู Ollama ููุชุตููู ูุงูุชุฑุฌูุฉ ุงูุฐููุฉ",
            variable=self.use_ollama,
            command=self.toggle_ollama
        ).pack(side=tk.LEFT)
        
        ttk.Label(ollama_frame, text="  ุงููููุฐุฌ:").pack(side=tk.LEFT)
        self.model_combo = ttk.Combobox(ollama_frame, textvariable=self.ollama_model, width=20)
        self.model_combo['values'] = ["qwen2.5:7b", "llama3.2:3b", "mistral:7b", "gemma2:9b"]
        self.model_combo.pack(side=tk.LEFT, padx=5)
        
        self.ollama_status = ttk.Label(ollama_frame, text="โณ ุฌุงุฑู ุงูุชุญูู...", foreground='orange')
        self.ollama_status.pack(side=tk.LEFT, padx=10)
        
        # โโโ ุฅุทุงุฑ ุงูุชูุฏู โโโ
        progress_frame = ttk.LabelFrame(main_frame, text="๐ ุงูุชูุฏู", padding="10")
        progress_frame.pack(fill=tk.X, pady=(0, 10))
        
        # ุดุฑูุท ุงูุชูุฏู ุงูุฑุฆูุณู
        self.progress_var = tk.DoubleVar()
        self.progress_bar = ttk.Progressbar(
            progress_frame,
            variable=self.progress_var,
            maximum=100,
            mode='determinate',
            length=400
        )
        self.progress_bar.pack(fill=tk.X, pady=5)
        
        # ุงูุชูุฏู ุงูุญุงูู
        self.progress_label = ttk.Label(progress_frame, text="ุฌุงูุฒ ููุจุฏุก")
        self.progress_label.pack()
        
        # ุงูุฅุญุตุงุฆูุงุช
        stats_frame = ttk.Frame(progress_frame)
        stats_frame.pack(fill=tk.X, pady=5)
        
        self.stats_labels = {}
        stats_items = [
            ('files', '๐ ุงููููุงุช: 0'),
            ('segments', '๐ ุงูููุงุทุน: 0'),
            ('duplicates', '๐ ุงูุชูุฑุงุฑุงุช: 0'),
            ('translations', '๐ ุงูุชุฑุฌูุงุช: 0')
        ]
        
        for key, text in stats_items:
            label = ttk.Label(stats_frame, text=text)
            label.pack(side=tk.LEFT, padx=10)
            self.stats_labels[key] = label
        
        # โโโ ุฅุทุงุฑ ุงูุณุฌู โโโ
        log_frame = ttk.LabelFrame(main_frame, text="๐ ุณุฌู ุงูุนูููุงุช", padding="10")
        log_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        # ูุฑุจุน ุงููุต ููุณุฌู
        self.log_text = scrolledtext.ScrolledText(
            log_frame,
            wrap=tk.WORD,
            height=15,
            font=('Consolas', 10),
            bg='#1e1e1e',
            fg='#ffffff'
        )
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # ุชูููู ุงูุฃููุงู
        self.log_text.tag_configure('INFO', foreground='#4ec9b0')
        self.log_text.tag_configure('WARNING', foreground='#dcdcaa')
        self.log_text.tag_configure('ERROR', foreground='#f14c4c')
        self.log_text.tag_configure('DEBUG', foreground='#569cd6')
        
        # โโโ ุฅุทุงุฑ ุงูุฃุฒุฑุงุฑ โโโ
        button_frame = ttk.Frame(main_frame)
        button_frame.pack(fill=tk.X)
        
        self.start_button = ttk.Button(
            button_frame,
            text="โถ๏ธ ุจุฏุก ุงููุนุงูุฌุฉ",
            command=self.start_processing,
            style='Accent.TButton'
        )
        self.start_button.pack(side=tk.LEFT, padx=5)
        
        self.stop_button = ttk.Button(
            button_frame,
            text="โน๏ธ ุฅููุงู",
            command=self.stop_processing,
            state=tk.DISABLED
        )
        self.stop_button.pack(side=tk.LEFT, padx=5)
        
        ttk.Button(
            button_frame,
            text="๐๏ธ ูุณุญ ุงูุณุฌู",
            command=self.clear_log
        ).pack(side=tk.LEFT, padx=5)
        
        ttk.Button(
            button_frame,
            text="๐ ูุชุญ ูุฌูุฏ ุงูุฅุฎุฑุงุฌ",
            command=self.open_output
        ).pack(side=tk.LEFT, padx=5)
        
        ttk.Button(
            button_frame,
            text="โ ุฎุฑูุฌ",
            command=self.root.quit
        ).pack(side=tk.RIGHT, padx=5)
    
    def browse_input(self):
        path = filedialog.askdirectory(title="ุงุฎุชุฑ ูุฌูุฏ ุงูุฅุฏุฎุงู")
        if path:
            self.input_path.set(path)
            # ุชุนููู ูุฌูุฏ ุงูุฅุฎุฑุงุฌ ุชููุงุฆูุงู
            if not self.output_path.get():
                self.output_path.set(os.path.join(path, "processed_output"))
    
    def browse_output(self):
        path = filedialog.askdirectory(title="ุงุฎุชุฑ ูุฌูุฏ ุงูุฅุฎุฑุงุฌ")
        if path:
            self.output_path.set(path)
    
    def check_ollama(self):
        """ุงูุชุญูู ูู ุชููุฑ Ollama"""
        def check():
            if HAS_REQUESTS:
                try:
                    response = requests.get(f"{OLLAMA_API_URL}/tags", timeout=5)
                    if response.status_code == 200:
                        models = response.json().get('models', [])
                        model_names = [m.get('name', '') for m in models]
                        self.model_combo['values'] = model_names
                        
                        self.root.after(0, lambda: self.ollama_status.config(
                            text=f"โ ูุชููุฑ ({len(models)} ูููุฐุฌ)",
                            foreground='green'
                        ))
                        self.logger.info(f"๐ฆ Ollama ูุชููุฑ. ุงูููุงุฐุฌ: {', '.join(model_names[:5])}")
                        return
                except:
                    pass
            
            self.root.after(0, lambda: self.ollama_status.config(
                text="โ ุบูุฑ ูุชููุฑ (ุณูุนูู ุจุฏูู AI)",
                foreground='red'
            ))
            self.logger.warning("โ๏ธ Ollama ุบูุฑ ูุชููุฑ. ุณูุชู ุงุณุชุฎุฏุงู ุงูุชุตููู ุงูุฃุณุงุณู.")
        
        threading.Thread(target=check, daemon=True).start()
    
    def toggle_ollama(self):
        if self.use_ollama.get():
            self.check_ollama()
        else:
            self.ollama_status.config(text="โช ูุนุทู", foreground='gray')
    
    def update_logs(self):
        """ุชุญุฏูุซ ูุฑุจุน ุงูุณุฌู"""
        try:
            while True:
                log_msg = self.log_queue.get_nowait()
                self.log_text.insert(tk.END, log_msg + '\n')
                self.log_text.see(tk.END)
        except queue.Empty:
            pass
        
        self.root.after(100, self.update_logs)
    
    def update_stats(self, stats: Dict):
        """ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช"""
        self.stats_labels['files'].config(text=f"๐ ุงููููุงุช: {stats.get('processed_files', 0)}")
        self.stats_labels['segments'].config(text=f"๐ ุงูููุงุทุน: {stats.get('unique_segments', 0)}")
        self.stats_labels['duplicates'].config(text=f"๐ ุงูุชูุฑุงุฑุงุช: {stats.get('duplicates_found', 0)}")
        self.stats_labels['translations'].config(text=f"๐ ุงูุชุฑุฌูุงุช: {stats.get('translations_extracted', 0)}")
    
    def start_processing(self):
        """ุจุฏุก ุงููุนุงูุฌุฉ"""
        if not self.input_path.get():
            messagebox.showerror("ุฎุทุฃ", "ุงูุฑุฌุงุก ุงุฎุชูุงุฑ ูุฌูุฏ ุงูุฅุฏุฎุงู")
            return
        
        if not os.path.exists(self.input_path.get()):
            messagebox.showerror("ุฎุทุฃ", "ูุฌูุฏ ุงูุฅุฏุฎุงู ุบูุฑ ููุฌูุฏ")
            return
        
        self.is_processing = True
        self.stop_requested = False
        
        self.start_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        
        threading.Thread(target=self.process, daemon=True).start()
    
    def process(self):
        """ุชูููุฐ ุงููุนุงูุฌุฉ"""
        try:
            self.logger.info("=" * 50)
            self.logger.info("๐ ุจุฏุก ุงููุนุงูุฌุฉ...")
            self.logger.info(f"๐ ูุฌูุฏ ุงูุฅุฏุฎุงู: {self.input_path.get()}")
            self.logger.info(f"๐ ูุฌูุฏ ุงูุฅุฎุฑุงุฌ: {self.output_path.get()}")
            self.logger.info(f"๐ฆ ุงุณุชุฎุฏุงู Ollama: {'ูุนู' if self.use_ollama.get() else 'ูุง'}")
            self.logger.info(f"๐ค ุงููููุฐุฌ: {self.ollama_model.get()}")
            self.logger.info("=" * 50)
            
            # ุฅูุดุงุก ุงููุนุงูุฌ
            processor = TextProcessor(
                input_dir=self.input_path.get(),
                output_dir=self.output_path.get(),
                use_ollama=self.use_ollama.get(),
                ollama_model=self.ollama_model.get(),
                logger=self.logger
            )
            
            # ุงูุญุตูู ุนูู ุงููููุงุช
            files = processor.get_files()
            processor.stats['total_files'] = len(files)
            
            self.logger.info(f"๐ ูุฌุฏ {len(files)} ููู ูููุนุงูุฌุฉ")
            
            if not files:
                self.logger.warning("โ๏ธ ูุง ุชูุฌุฏ ูููุงุช ูููุนุงูุฌุฉ")
                return
            
            # ูุนุงูุฌุฉ ุงููููุงุช
            for i, file_path in enumerate(files):
                if self.stop_requested:
                    self.logger.warning("โ๏ธ ุชู ุทูุจ ุงูุฅููุงู...")
                    break
                
                # ุชุญุฏูุซ ุงูุชูุฏู
                progress = (i + 1) / len(files) * 100
                self.root.after(0, lambda p=progress: self.progress_var.set(p))
                self.root.after(0, lambda f=file_path: self.progress_label.config(
                    text=f"ูุนุงูุฌุฉ: {f.name} ({i+1}/{len(files)})"
                ))
                
                self.logger.info(f"๐ [{i+1}/{len(files)}] ูุนุงูุฌุฉ: {file_path.name}")
                
                try:
                    segments_count = 0
                    for segment in processor.segment_file(file_path):
                        if self.stop_requested:
                            break
                        
                        processed = processor.process_segment(segment)
                        processor.stats['total_segments'] += 1
                        
                        if processed.get('is_duplicate'):
                            processor.stats['duplicates_found'] += 1
                        else:
                            processor.stats['unique_segments'] += 1
                            cat = processed['category']
                            processor.stats['categories'][cat] = processor.stats['categories'].get(cat, 0) + 1
                            
                            if processed.get('corrections'):
                                processor.stats['corrections_made'] += 1
                            
                            processor.save_segment(processed)
                            segments_count += 1
                    
                    processor.stats['processed_files'] += 1
                    
                    # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
                    self.root.after(0, lambda s=processor.stats.copy(): self.update_stats(s))
                    
                except Exception as e:
                    self.logger.error(f"โ ุฎุทุฃ ูู {file_path.name}: {e}")
            
            # ุญูุธ ุงูุชูุฑูุฑ
            self.save_report(processor.stats)
            
            self.logger.info("=" * 50)
            self.logger.info("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
            self.logger.info(f"๐ ุงูููุฎุต:")
            self.logger.info(f"   - ุงููููุงุช: {processor.stats['processed_files']}")
            self.logger.info(f"   - ุงูููุงุทุน: {processor.stats['unique_segments']}")
            self.logger.info(f"   - ุงูุชูุฑุงุฑุงุช: {processor.stats['duplicates_found']}")
            self.logger.info("=" * 50)
            
            self.root.after(0, lambda: self.progress_label.config(text="โ ุงูุชููุช ุงููุนุงูุฌุฉ!"))
            messagebox.showinfo("ุชู", f"ุงูุชููุช ุงููุนุงูุฌุฉ!\n\nุงููููุงุช: {processor.stats['processed_files']}\nุงูููุงุทุน: {processor.stats['unique_segments']}")
            
        except Exception as e:
            self.logger.error(f"๐ฅ ุฎุทุฃ ุนุงู: {e}")
            messagebox.showerror("ุฎุทุฃ", f"ุญุฏุซ ุฎุทุฃ: {e}")
        
        finally:
            self.is_processing = False
            self.root.after(0, lambda: self.start_button.config(state=tk.NORMAL))
            self.root.after(0, lambda: self.stop_button.config(state=tk.DISABLED))
    
    def stop_processing(self):
        """ุฅููุงู ุงููุนุงูุฌุฉ"""
        self.stop_requested = True
        self.logger.warning("โ๏ธ ุชู ุทูุจ ุฅููุงู ุงููุนุงูุฌุฉ...")
    
    def clear_log(self):
        """ูุณุญ ุงูุณุฌู"""
        self.log_text.delete(1.0, tk.END)
    
    def open_output(self):
        """ูุชุญ ูุฌูุฏ ุงูุฅุฎุฑุงุฌ"""
        if self.output_path.get() and os.path.exists(self.output_path.get()):
            if sys.platform == 'win32':
                os.startfile(self.output_path.get())
            elif sys.platform == 'darwin':
                subprocess.run(['open', self.output_path.get()])
            else:
                subprocess.run(['xdg-open', self.output_path.get()])
        else:
            messagebox.showwarning("ุชูุจูู", "ูุฌูุฏ ุงูุฅุฎุฑุงุฌ ุบูุฑ ููุฌูุฏ ุจุนุฏ")
    
    def save_report(self, stats: Dict):
        """ุญูุธ ุงูุชูุฑูุฑ"""
        output_dir = Path(self.output_path.get())
        output_dir.mkdir(parents=True, exist_ok=True)
        
        report_path = output_dir / 'PROCESSING_REPORT.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ๐ ุชูุฑูุฑ ุงููุนุงูุฌุฉ ุงูุฐููุฉ\n\n")
            f.write(f"**ุงูุชุงุฑูุฎ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ๐ ุงูุฅุญุตุงุฆูุงุช\n\n")
            f.write("| ุงูุจูุฏ | ุงูุนุฏุฏ |\n")
            f.write("|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงููุนุงูุฌุฉ | {stats.get('processed_files', 0)} |\n")
            f.write(f"| ุงูููุงุทุน ุงููุฑูุฏุฉ | {stats.get('unique_segments', 0)} |\n")
            f.write(f"| ุงูุชูุฑุงุฑุงุช ุงูููุชุดูุฉ | {stats.get('duplicates_found', 0)} |\n")
            f.write(f"| ุงูุชุตุญูุญุงุช | {stats.get('corrections_made', 0)} |\n\n")
            
            f.write("## ๐ ุงูุชุตููู\n\n")
            f.write("| ุงููุฆุฉ | ุงูุนุฏุฏ |\n")
            f.write("|-------|-------|\n")
            
            for cat, count in stats.get('categories', {}).items():
                icons = {'medical': '๐ฅ', 'technical': '๐ป', 'translation': '๐', 'reference': '๐', 'misc': '๐'}
                icon = icons.get(cat, '๐')
                f.write(f"| {icon} {cat} | {count} |\n")
        
        self.logger.info(f"๐ ุชู ุญูุธ ุงูุชูุฑูุฑ: {report_path}")
    
    def run(self):
        """ุชุดุบูู ุงููุงุฌูุฉ"""
        self.logger.info("๐ง ุชู ุชุญููู ูุนุงูุฌ ุงููุตูุต ุงูุฐูู")
        self.logger.info("๐ ุงุฎุชุฑ ูุฌูุฏ ุงูุฅุฏุฎุงู ูุงูุฅุฎุฑุงุฌ ุซู ุงุถุบุท 'ุจุฏุก ุงููุนุงูุฌุฉ'")
        self.root.mainloop()


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุทุฉ ุงูุฏุฎูู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

if __name__ == "__main__":
    app = TextProcessorGUI()
    app.run()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/translation_extractor.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
ูุณุชุฎุฑุฌ ุงูุชุฑุฌูุงุช ุงูููุญุฏ - Translation Extractor
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
ุงููุฏู:
- ุงุณุชุฎุฑุงุฌ ุงูุฌูู ุงููุชุฑุฌูุฉ (ุฅูุฌููุฒูุฉ-ุนุฑุจูุฉ) ูู ุงููููุงุช
- ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ
- ุชุตููู ุงููุญุชูู (ุทุจู/ุชููู/ุชุฑุฌูุฉ/ูุฑุงุฌุน)
- ุฅูุดุงุก ููู CSV ููุญุฏ

ุงูุงุณุชุฎุฏุงู:
python translation_extractor.py --input /path/to/files --output /path/to/output
"""

import os
import re
import csv
import json
import hashlib
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import argparse

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ ุงูุดุงุฆุนุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

SPELLING_CORRECTIONS = {
    # ุฃุฎุทุงุก ุงูููุฒุฉ
    'ุงูุดุงุก': 'ุฅูุดุงุก',
    'ุงูุงู': 'ุงูุขู',
    'ููู': 'ููู',
    'ุงุฐุง': 'ุฅุฐุง',
    'ุงูู': 'ุฅูู',
    'ุนุจุงุฑุฉ': 'ุนุจุงุฑุฉ',
    
    # ุฃุฎุทุงุก ุงูุฃูู
    'ุงูุถุง': 'ุฃูุถุงู',
    'ุงูุถุงู': 'ุฃูุถุงู',
    'ุฐุงูู': 'ุฐูู',
    'ูุงุฐุง': 'ูุฐุง',
    'ูุงุฐู': 'ูุฐู',
    
    # ุฃุฎุทุงุก ุงูุชุงุก ุงููุฑุจูุทุฉ
    'ูููุงุช': 'ูููุงุช',
    'ูุดููุงุช': 'ูุดููุงุช',
    
    # ุฃุฎุทุงุก ุดุงุฆุนุฉ
    'ูููู': 'ูููู',
    'ุฌุทุง': 'ุฎุทุฃ',
    ' ุงุณุชุทูุน': 'ุฃุณุชุทูุน',
    ' ุงุณุชุฎุฏู': 'ุฃุณุชุฎุฏู',
    
    # ุฃุฎุทุงุก ุงููุงุก ูุงูุฃูู
    'ุงุญู': 'ุฃุญูู',
    'ูุญู': 'ูุญู',
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ูููุงุช ุงูุชุตููู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

CATEGORY_KEYWORDS = {
    'medical': [
        # ุนุฑุจู
        'ุทุจู', 'ุทุจ', 'ุนุธุงู', 'ุฌุฑุงุญุฉ', 'ูุณุชุดูู', 'ุทุจูุจ', 'ุนูุงุฌ', 'ูุฑุถ', 'ูุณุฑ',
        'ุบุถุฑูู', 'ููุตู', 'ุฑูุจุฉ', 'ูุฎุฐ', 'ุนููุฏ ููุฑู', 'ุชุตููุฑ', 'ุฃุดุนุฉ', 'dicom',
        'ูุณุชูุฒูุงุช', 'ุทุจูุฉ', 'ุนุธููุฉ', 'ุชูุชุงูููู', 'ุณุชุงููุณ', 'ุบุฑุณุงุช', 'ูุณุงููุฑ',
        'ุจุฑุงุบู', 'ููุงุตู', 'ุงุตุทูุงุนูุฉ', 'ุบุงูุงููู', 'ุณููุฏ', 'ุตูุงุฆุญ', 'ุจุฑูุดุงุช',
        # ุฅูุฌููุฒู
        'medical', 'orthopedic', 'surgery', 'hospital', 'doctor', 'treatment',
        'fracture', 'bone', 'joint', 'knee', 'hip', 'spine', 'implant', 'screw',
        'plate', 'titanium', 'stainless', 'prosthesis', 'dhs', 'pin', 'nail'
    ],
    'technical': [
        # ุนุฑุจู
        'ููููุณ', 'ูููุฏูุฒ', 'ุจุฑูุงูุฌ', 'ุชุซุจูุช', 'ุญุฒูุฉ', 'ูุธุงู', 'ุณูุฑูุฑ', 'ููุฏ',
        'ุจุฑูุฌุฉ', 'ุณูุฑูุจุช', 'ุฃูุฑ', 'ุทุฑููุฉ', 'ูุณุชุฎุฏู', 'ููู', 'ูุฌูุฏ', 'git',
        'github', 'python', 'bash', 'shell', 'kde', 'gnome', 'plasma', 'arch',
        'ูุงูุฌุงุฑู', 'ุบุงุฑูุฏุง', 'ุฒูุฑูู', 'ุฃูุจููุชู', 'ููุฏูุฑุง', 'docker', 'vm',
        'ุขูุฉ', 'ุงูุชุฑุงุถูุฉ', 'vpn', 'proxy', 'ุดุจูุฉ', 'ุฅูุชุฑูุช', 'ุฃูุงู',
        # ุฅูุฌููุฒู
        'linux', 'windows', 'software', 'install', 'package', 'system', 'server',
        'code', 'programming', 'script', 'command', 'terminal', 'user', 'file',
        'folder', 'directory', 'kernel', 'desktop', 'environment', 'window', 'manager'
    ],
    'translation': [
        # ุนุฑุจู
        'ุชุฑุฌูุฉ', 'ูุชุฑุฌู', 'ูุบุฉ', 'ุนุฑุจู', 'ุฅูุฌููุฒู', 'ูุงููุณ', 'ูุตุทูุญ', 'ูุนูู',
        'ูููุฉ', 'ุฌููุฉ', 'ูุต', 'ูุซููุฉ', 'ูุบูู', 'ุซูุงุฆู', 'ุงููุบุฉ', 'ุชุนุฑูุจ',
        # ุฅูุฌููุฒู
        'translation', 'translator', 'language', 'arabic', 'english', 'dictionary',
        'term', 'meaning', 'word', 'sentence', 'text', 'document', 'bilingual'
    ],
    'reference': [
        # ุนุฑุจู
        'ููุฑุณ', 'ูุฑุฌุน', 'ุฏููู', 'ูุงุฆูุฉ', 'ุฌุฏูู', 'ุจูุงูุงุช', 'ุฅุญุตุงุฆูุงุช', 'ุชูุฑูุฑ',
        'ููุฎุต', 'ููุฑุณ', 'ุชุตููู', 'ูููุน', 'ุฑุงุจุท', 'ูุตุฏุฑ',
        # ุฅูุฌููุฒู
        'index', 'reference', 'guide', 'list', 'table', 'data', 'statistics',
        'report', 'summary', 'classification', 'link', 'source', 'inventory'
    ]
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุณุงุนุฏุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

def is_arabic(text: str) -> bool:
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงููุต ูุญุชูู ุนูู ุฃุญุฑู ุนุฑุจูุฉ"""
    return bool(re.search(r'[\u0600-\u06FF]', text))


def is_english(text: str) -> bool:
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงููุต ูุญุชูู ุนูู ุฃุญุฑู ุฅูุฌููุฒูุฉ"""
    return bool(re.search(r'[a-zA-Z]', text))


def is_bilingual(text: str) -> bool:
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงููุต ุซูุงุฆู ุงููุบุฉ"""
    return is_arabic(text) and is_english(text)


def correct_spelling(text: str) -> Tuple[str, List[str]]:
    """ุชุตุญูุญ ุงูุฃุฎุทุงุก ุงูุฅููุงุฆูุฉ ุงูุนุฑุจูุฉ"""
    corrections = []
    corrected = text
    
    for wrong, right in SPELLING_CORRECTIONS.items():
        if wrong in corrected:
            corrected = corrected.replace(wrong, right)
            corrections.append(f"{wrong} โ {right}")
    
    return corrected, corrections


def normalize_text(text: str) -> str:
    """ุชุทุจูุน ุงููุต"""
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    text = re.sub(r'\s+', ' ', text.strip())
    # ุชูุญูุฏ ุนูุงูุงุช ุงูุชุฑููู
    text = text.replace('ุ', ',')
    text = text.replace('ุ', ';')
    return text


def classify_text(text: str) -> Tuple[str, float]:
    """ุชุตููู ุงููุต ุจูุงุกู ุนูู ุงููููุงุช ุงูููุชุงุญูุฉ"""
    text_lower = text.lower()
    scores = {}
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw.lower() in text_lower)
        scores[category] = score
    
    if max(scores.values()) == 0:
        return 'misc', 0.0
    
    best_category = max(scores, key=scores.get)
    confidence = scores[best_category] / max(len(CATEGORY_KEYWORDS[best_category]), 1)
    
    return best_category, min(confidence, 1.0)


def extract_translation_pairs(text: str) -> List[Dict]:
    """ุงุณุชุฎุฑุงุฌ ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ ูู ุงููุต"""
    pairs = []
    lines = text.split('\n')
    
    # ุฃููุงุท ุงูุจุญุซ ุนู ุงูุชุฑุฌูุงุช
    patterns = [
        # ููุท: English | Arabic ุฃู English - Arabic
        r'^([A-Za-z][A-Za-z0-9\s\-_,\.]+)\s*[\|\-โโ]\s*([\u0600-\u06FF][\u0600-\u06FF\s\-_,\.]+)$',
        # ููุท: "English": "Arabic"
        r'["\']([A-Za-z][^"\']+)["\']\s*:\s*["\']([\u0600-\u06FF][^"\']+)["\']',
        # ููุท ุฌุฏูู Markdown: | English | Arabic |
        r'\|\s*([A-Za-z][A-Za-z0-9\s\-_,\.]+)\s*\|\s*([\u0600-\u06FF][\u0600-\u06FF\s\-_,\.]+)\s*\|',
    ]
    
    for line in lines:
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        
        for pattern in patterns:
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                eng = normalize_text(match.group(1))
                ara = normalize_text(match.group(2))
                
                if len(eng) > 2 and len(ara) > 2:
                    corrected_ara, corrections = correct_spelling(ara)
                    category, confidence = classify_text(f"{eng} {ara}")
                    
                    pairs.append({
                        'english': eng,
                        'arabic_original': ara,
                        'arabic_corrected': corrected_ara,
                        'corrections': corrections,
                        'category': category,
                        'confidence': round(confidence, 2)
                    })
                break
    
    return pairs


def extract_from_csv_content(content: str) -> List[Dict]:
    """ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช ูู ูุญุชูู CSV"""
    pairs = []
    lines = content.strip().split('\n')
    
    for line in lines[1:]:  # ุชุฎุทู ุงูุนููุงู
        parts = line.split(',')
        if len(parts) >= 3:
            try:
                eng = parts[1].strip('"')
                ara = parts[2].strip('"')
                
                if eng and ara and len(eng) > 2 and len(ara) > 2:
                    corrected_ara, corrections = correct_spelling(ara)
                    category, confidence = classify_text(f"{eng} {ara}")
                    
                    pairs.append({
                        'english': eng,
                        'arabic_original': ara,
                        'arabic_corrected': corrected_ara,
                        'corrections': corrections,
                        'category': category,
                        'confidence': round(confidence, 2)
                    })
            except:
                continue
    
    return pairs


def hash_pair(eng: str, ara: str) -> str:
    """ุฅูุดุงุก hash ููุฒูุฌ ุงููุบูู"""
    return hashlib.md5(f"{eng}|{ara}".lower().encode()).hexdigest()[:16]


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงููุนุงูุฌ ุงูุฑุฆูุณู
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

class TranslationExtractor:
    """ูุณุชุฎุฑุฌ ุงูุชุฑุฌูุงุช ุงูุฑุฆูุณู"""
    
    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.all_pairs = []
        self.seen_hashes = set()
        self.stats = {
            'files_processed': 0,
            'pairs_extracted': 0,
            'duplicates_removed': 0,
            'corrections_made': 0,
            'categories': {}
        }
    
    def process_file(self, file_path: Path) -> List[Dict]:
        """ูุนุงูุฌุฉ ููู ูุงุญุฏ"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            print(f"  โ๏ธ ุฎุทุฃ ูู ูุฑุงุกุฉ {file_path.name}: {e}")
            return []
        
        pairs = []
        
        # ูุนุงูุฌุฉ ูููุงุช CSV
        if file_path.suffix.lower() == '.csv':
            pairs = extract_from_csv_content(content)
        else:
            pairs = extract_translation_pairs(content)
        
        # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุฅุถุงูุฉ ูุนูููุงุช ุงููุตุฏุฑ
        unique_pairs = []
        for pair in pairs:
            h = hash_pair(pair['english'], pair['arabic_corrected'])
            
            if h not in self.seen_hashes:
                self.seen_hashes.add(h)
                pair['hash'] = h
                pair['source_file'] = file_path.name
                unique_pairs.append(pair)
                
                # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
                if pair['corrections']:
                    self.stats['corrections_made'] += 1
                
                cat = pair['category']
                self.stats['categories'][cat] = self.stats['categories'].get(cat, 0) + 1
            else:
                self.stats['duplicates_removed'] += 1
        
        return unique_pairs
    
    def process_directory(self):
        """ูุนุงูุฌุฉ ุฌููุน ุงููููุงุช ูู ุงููุฌูุฏ"""
        print(f"๐ ูุญุต ุงููุฌูุฏ: {self.input_dir}")
        
        extensions = ['.txt', '.md', '.csv', '.json']
        files = [f for f in self.input_dir.rglob('*') 
                 if f.is_file() and f.suffix.lower() in extensions]
        
        print(f"๐ ูุฌุฏ {len(files)} ููู ูููุนุงูุฌุฉ")
        
        for i, file_path in enumerate(files, 1):
            print(f"  [{i}/{len(files)}] ูุนุงูุฌุฉ: {file_path.name}")
            
            pairs = self.process_file(file_path)
            self.all_pairs.extend(pairs)
            self.stats['files_processed'] += 1
            self.stats['pairs_extracted'] += len(pairs)
            
            if pairs:
                print(f"    โ ุงุณุชุฎุฑุฌ {len(pairs)} ุฒูุฌ ุชุฑุฌูุฉ")
    
    def save_results(self):
        """ุญูุธ ุงููุชุงุฆุฌ"""
        print("\n๐พ ุญูุธ ุงููุชุงุฆุฌ...")
        
        # ุญูุธ CSV
        csv_path = self.output_dir / 'translations_unified.csv'
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([
                '#', 'English', 'Arabic (Original)', 'Arabic (Corrected)',
                'Category', 'Confidence', 'Corrections', 'Source File', 'Hash'
            ])
            
            for i, pair in enumerate(self.all_pairs, 1):
                writer.writerow([
                    i,
                    pair['english'],
                    pair['arabic_original'],
                    pair['arabic_corrected'],
                    pair['category'],
                    pair['confidence'],
                    '; '.join(pair['corrections']) if pair['corrections'] else '',
                    pair['source_file'],
                    pair['hash']
                ])
        
        print(f"  โ ุญูุธ CSV: {csv_path}")
        
        # ุญูุธ TSV ููุชุฏุฑูุจ
        tsv_path = self.output_dir / 'translations_training.tsv'
        with open(tsv_path, 'w', encoding='utf-8') as f:
            for pair in self.all_pairs:
                f.write(f"{pair['english']}\t{pair['arabic_corrected']}\n")
        
        print(f"  โ ุญูุธ TSV: {tsv_path}")
        
        # ุญูุธ JSONL ูู HuggingFace
        jsonl_path = self.output_dir / 'translations_huggingface.jsonl'
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for pair in self.all_pairs:
                obj = {
                    'translation': {
                        'en': pair['english'],
                        'ar': pair['arabic_corrected']
                    },
                    'category': pair['category'],
                    'confidence': pair['confidence']
                }
                f.write(json.dumps(obj, ensure_ascii=False) + '\n')
        
        print(f"  โ ุญูุธ JSONL: {jsonl_path}")
        
        # ุญูุธ ุงูุชูุฑูุฑ
        self.save_report()
    
    def save_report(self):
        """ุญูุธ ุชูุฑูุฑ ุงููุนุงูุฌุฉ"""
        report_path = self.output_dir / 'PROCESSING_REPORT.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ๐ ุชูุฑูุฑ ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช\n\n")
            f.write(f"**ุงูุชุงุฑูุฎ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## ๐ ุงูุฅุญุตุงุฆูุงุช\n\n")
            f.write("| ุงูุจูุฏ | ุงูุนุฏุฏ |\n")
            f.write("|-------|-------|\n")
            f.write(f"| ุงููููุงุช ุงููุนุงูุฌุฉ | {self.stats['files_processed']} |\n")
            f.write(f"| ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ ุงููุณุชุฎุฑุฌุฉ | {self.stats['pairs_extracted']} |\n")
            f.write(f"| ุงูุชูุฑุงุฑุงุช ุงููุฒุงูุฉ | {self.stats['duplicates_removed']} |\n")
            f.write(f"| ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ | {self.stats['corrections_made']} |\n\n")
            
            f.write("## ๐ ุงูุชุตููู\n\n")
            f.write("| ุงููุฆุฉ | ุงูุนุฏุฏ | ุงููุณุจุฉ |\n")
            f.write("|-------|-------|--------|\n")
            
            total = sum(self.stats['categories'].values())
            for cat, count in sorted(self.stats['categories'].items(), 
                                    key=lambda x: x[1], reverse=True):
                pct = (count / total * 100) if total > 0 else 0
                icon = {'medical': '๐ฅ', 'technical': '๐ป', 'translation': '๐', 
                       'reference': '๐', 'misc': '๐'}.get(cat, '๐')
                f.write(f"| {icon} {cat} | {count} | {pct:.1f}% |\n")
            
            f.write("\n## ๐ ุงููููุงุช ุงููุงุชุฌุฉ\n\n")
            f.write("| ุงูููู | ุงููุตู |\n")
            f.write("|-------|-------|\n")
            f.write("| translations_unified.csv | ุฌููุน ุงูุชุฑุฌูุงุช ูู ููู CSV |\n")
            f.write("| translations_training.tsv | ุชูุณูู TSV ููุชุฏุฑูุจ |\n")
            f.write("| translations_huggingface.jsonl | ุชูุณูู JSONL ูู HuggingFace |\n")
            
            f.write("\n## โ ุฃูุซูุฉ ุนูู ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ\n\n")
            f.write("| English | Arabic | Category |\n")
            f.write("|---------|--------|----------|\n")
            
            for pair in self.all_pairs[:10]:
                f.write(f"| {pair['english'][:40]}... | {pair['arabic_corrected'][:40]}... | {pair['category']} |\n")
        
        print(f"  โ ุญูุธ ุงูุชูุฑูุฑ: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='ูุณุชุฎุฑุฌ ุงูุชุฑุฌูุงุช ุงูููุญุฏ')
    parser.add_argument('--input', '-i', required=True, help='ูุฌูุฏ ุงูุฅุฏุฎุงู')
    parser.add_argument('--output', '-o', required=True, help='ูุฌูุฏ ุงูุฅุฎุฑุงุฌ')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("๐ ูุณุชุฎุฑุฌ ุงูุชุฑุฌูุงุช ุงูููุญุฏ")
    print("=" * 60)
    
    extractor = TranslationExtractor(args.input, args.output)
    extractor.process_directory()
    extractor.save_results()
    
    print("\n" + "=" * 60)
    print("โ ุงูุชููุช ุงููุนุงูุฌุฉ!")
    print("=" * 60)
    print(f"๐ ุงูููุฎุต:")
    print(f"  - ุงููููุงุช: {extractor.stats['files_processed']}")
    print(f"  - ุงูุชุฑุฌูุงุช: {extractor.stats['pairs_extracted']}")
    print(f"  - ุงูุชูุฑุงุฑุงุช ุงููุฒุงูุฉ: {extractor.stats['duplicates_removed']}")
    print(f"  - ุงูุชุตุญูุญุงุช: {extractor.stats['corrections_made']}")


if __name__ == "__main__":
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/zip_rar_folder2txt.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import zipfile
import os
import sys
import pathlib
import mimetypes
import datetime
import glob
import subprocess
import tempfile

# ูุญุงููุฉ ุงุณุชูุฑุงุฏ ููุชุจุฉ rarfile
RAR_SUPPORT = False
rarfile = None
try:
    import rarfile
    RAR_SUPPORT = True
except ImportError:
    pass

# ูุญุงููุฉ ุงุณุชูุฑุงุฏ ููุชุจุฉ tarfile (ุฌุฒุก ูู ููุชุจุงุช ุจุงูุซูู ุงูููุงุณูุฉ)
try:
    import tarfile
    TAR_SUPPORT = True
except ImportError:
    TAR_SUPPORT = False

def is_split_archive_extension(extension):
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูุงูุชุฏุงุฏ ุฌุฒุกูุง ูู ุฃุฑุดูู ูุถุบูุท ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก"""
    ext_lower = extension.lower()
    if ext_lower.startswith('.z'):
        # ุชุญูู ููุง ุฅุฐุง ูุงู ุงูุฌุฒุก ุงููุชุจูู ุจุนุฏ '.z' ูู ุฑูู
        remaining = ext_lower[2:]  # ุจุนุฏ '.z'
        if remaining.isdigit():
            return True
    # ุฃูุถูุงุ ุชุฌุงูู ุงููููุงุช ุฐุงุช ุงูุงูุชุฏุงุฏุงุช ุงูุฃุฎุฑู ุงููุดุงุจูุฉ
    if ext_lower in ['.001', '.002', '.003', '.004', '.005', 
                     '.006', '.007', '.008', '.009', '.010',
                     '.r00', '.r01', '.r02', '.r03', '.r04',
                     '.part1.rar', '.part2.rar', '.part3.rar']:
        return True
    return False

def install_unrar_windows():
    """ูุญุงููุฉ ุชุซุจูุช unrar ุนูู Windows ุฅุฐุง ูู ููู ููุฌูุฏูุง"""
    try:
        # ุชุญููู unrar ูู ุงูุฅูุชุฑูุช
        import urllib.request
        import shutil
        
        print("  ๐ง ูุญุงููุฉ ุชุซุจูุช unrar ุชููุงุฆููุง...")
        
        # ุฑุงุจุท ูุชูุฒูู unrar ูู ูููุน rarlab
        unrar_url = "https://www.rarlab.com/rar/unrarw32.exe"
        temp_dir = tempfile.gettempdir()
        unrar_exe = os.path.join(temp_dir, "unrar.exe")
        
        # ุชูุฒูู unrar
        print(f"  ๐ฅ ุฌุงุฑู ุชูุฒูู unrar...")
        urllib.request.urlretrieve(unrar_url, unrar_exe)
        
        # ุงุณุชุฎุฑุงุฌ unrar.exe ูู ุงูููู ุงูุชูููุฐู
        print(f"  ๐ฆ ุฌุงุฑู ุงุณุชุฎุฑุงุฌ unrar...")
        
        # ุฅูุดุงุก ูุฌูุฏ unrar
        unrar_dir = os.path.join(temp_dir, "unrar")
        os.makedirs(unrar_dir, exist_ok=True)
        
        # ุชุดุบูู ุงูููู ุงูุชูููุฐู ูุงุณุชุฎุฑุงุฌ ุงููููุงุช
        try:
            subprocess.run([unrar_exe, f"-o{unrar_dir}"], capture_output=True, check=True)
        except:
            # ุฅุฐุง ูุดูุ ูุณุชุฎุฏู 7zip ูุงุณุชุฎุฑุงุฌู
            try:
                import py7zr
                with py7zr.SevenZipFile(unrar_exe, mode='r') as z:
                    z.extractall(path=unrar_dir)
            except:
                pass
        
        # ุงูุจุญุซ ุนู unrar.exe ูู ุงููุฌูุฏ ุงููุณุชุฎุฑุฌ
        for root, dirs, files in os.walk(unrar_dir):
            if "unrar.exe" in files:
                unrar_path = os.path.join(root, "unrar.exe")
                # ุชุนููู ุงููุณุงุฑ ูู rarfile
                rarfile.UNRAR_TOOL = unrar_path
                print(f"  โ ุชู ุงูุนุซูุฑ ุนูู unrar ูู: {unrar_path}")
                return True
        
        print("  โ๏ธ  ูู ูุชู ุงูุนุซูุฑ ุนูู unrar.exe ูู ุงูููู ุงููุณุชุฎุฑุฌ")
        return False
        
    except Exception as e:
        print(f"  โ ูุดู ุชุซุจูุช unrar ุชููุงุฆููุง: {str(e)}")
        print(f"  โน๏ธ  ููููู ุชุซุจูุช WinRAR ูุฏูููุง ูู: https://www.win-rar.com/")
        return False

def get_unique_filename(base_name, extension=".txt"):
    """ุฅูุดุงุก ุงุณู ููู ูุฑูุฏ ุฅุฐุง ูุงู ุงูููู ููุฌูุฏูุง ุจุงููุนู"""
    counter = 1
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ุงููุญุงููุฉ ุงูุฃููู: ุงูุงุณู ุงูุฃุณุงุณู ููุท
    if not os.path.exists(f"{base_name}{extension}"):
        return f"{base_name}{extension}"
    
    # ุงููุญุงููุฉ ุงูุซุงููุฉ: ุงูุงุณู ูุน ุงูุชุงุฑูุฎ ูุงูููุช
    new_name = f"{base_name}_{timestamp}{extension}"
    if not os.path.exists(new_name):
        return new_name
    
    # ุงููุญุงููุฉ ุงูุซุงูุซุฉ: ุงูุงุณู ูุน ุงูุชุงุฑูุฎ ูุงูููุช ูุฑูู ุนุดูุงุฆู
    import random
    random_suffix = random.randint(1000, 9999)
    new_name = f"{base_name}_{timestamp}_{random_suffix}{extension}"
    if not os.path.exists(new_name):
        return new_name
    
    # ุฅุฐุง ูุดูุช ุฌููุน ุงููุญุงููุงุชุ ุฅุถุงูุฉ ุนุฏุงุฏ
    while True:
        new_name = f"{base_name}_{timestamp}_{counter}{extension}"
        if not os.path.exists(new_name):
            return new_name
        counter += 1

def is_text_file(content_bytes):
    """ูุญุต ูุง ุฅุฐุง ูุงู ุงูููู ูุตููุง ุฃู ุซูุงุฆููุง"""
    if not content_bytes:
        return False
    
    # ูุญุงููุฉ ูู ุงูุชุฑููุฒ ููุต UTF-8
    try:
        content_bytes.decode('utf-8')
        return True
    except UnicodeDecodeError:
        # ุฅุฐุง ูุดู UTF-8ุ ุฌุฑุจ Latin-1
        try:
            content_bytes.decode('latin-1')
            return True
        except UnicodeDecodeError:
            return False

def should_ignore_file(file_path):
    """ุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงูููู/ุงููุฌูุฏ"""
    # ุชุฌุงูู ุฃู ูุณุงุฑ ูุญุชูู ุนูู ูุฌูุฏ ูุจุฏุฃ ุจููุทุฉ
    parts = file_path.split('/')
    for part in parts:
        if part.startswith('.') and part != '.' and part != '..':
            return True
    
    # ุชุฌุงูู ูุฌูุฏ __pycache__
    if '__pycache__' in parts:
        return True
    
    # ุชุฌุงูู ูููุงุช PYC (ุจุงูุช ููุฏ ุจุงูุซูู)
    if file_path.endswith('.pyc'):
        return True
    
    # ุชุฌุงูู ุฃู ููู ูู ูุฌูุฏ venv/ (ุจุฏูู ููุทุฉ ูู ุงูุจุฏุงูุฉ)
    for i, part in enumerate(parts):
        if part == 'venv':
            # ุชุญูู ููุง ุฅุฐุง ูุงู 'venv' ูู ุงุณู ูุฌูุฏ ูููุณ ุฌุฒุกูุง ูู ุงุณู ููู
            if i < len(parts) - 1:  # ุฅุฐุง ูู ููู ุขุฎุฑ ุฌุฒุก ูู ุงููุณุงุฑ
                return True
    
    # ุทุฑููุฉ ุจุฏููุฉ ููุชุญูู ูู ูุณุงุฑุงุช venv/
    if '/venv/' in file_path:
        return True
    
    # ุชุฌุงูู ุฃู ูุณุงุฑ ูุจุฏุฃ ุจู venv/
    if file_path.startswith('venv/'):
        return True
    
    return False

def is_model_file(file_path):
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูููู ูู ูุฌูุฏ models"""
    # ุงูุชุญูู ูู ูุฌูุฏุงุช models (ุจุฃุญุฑู ูุจูุฑุฉ ูุตุบูุฑุฉ)
    file_path_lower = file_path.lower()
    
    # ุชุญูู ูู ููุงุฐุฌ ูุฎุชููุฉ ููุณุงุฑุงุช models
    patterns = [
        'models/',
        'models\\',
        '/models/',
        '\\models\\',
    ]
    
    for pattern in patterns:
        if pattern in file_path_lower:
            return True
    
    # ุชุญูู ูู ุจุฏุงูุฉ ุงููุณุงุฑ
    if file_path_lower.startswith('models/'):
        return True
    
    return False

def get_file_type(file_path, content_bytes):
    """ุชุญุฏูุฏ ููุน ุงูููู ุจูุงุกู ุนูู ุงููุญุชูู"""
    # ุฃููุงูุ ูุญุต ุงูุจุงูุชุงุช ุงููููุฒุฉ ููุฃููุงุน ุงูุซูุงุฆูุฉ ุงูุดุงุฆุนุฉ
    if len(content_bytes) >= 4:
        # ูููุงุช PYC (ุจุงูุช ููุฏ ุจุงูุซูู)
        if content_bytes[:4] == b'\x63\x00\x00\x00':  # PYC magic number
            return "Python Compiled (.pyc)"
        
        # ูููุงุช ELF (ุชูููุฐูุงุช Linux)
        if content_bytes[:4] == b'\x7f\x45\x4c\x46':
            return "ELF Executable"
        
        # ูููุงุช PE (ุชูููุฐูุงุช Windows)
        if content_bytes[:2] == b'MZ':
            return "Windows Executable"
        
        # ูููุงุช PNG
        if content_bytes[:8] == b'\x89\x50\x4e\x47\x0d\x0a\x1a\x0a':
            return "PNG Image"
        
        # ูููุงุช JPEG
        if content_bytes[:3] == b'\xff\xd8\xff':
            return "JPEG Image"
        
        # ูููุงุช PDF
        if content_bytes[:4] == b'%PDF':
            return "PDF Document"
        
        # ูููุงุช ZIP
        if content_bytes[:2] == b'PK':
            return "ZIP Archive"
        
        # ูููุงุช GZIP
        if content_bytes[:2] == b'\x1f\x8b':
            return "GZIP Compressed"
    
    # ุฅุฐุง ูู ููู ุซูุงุฆููุง ูุงุถุญูุงุ ูุญุต ูุง ุฅุฐุง ูุงู ูุตููุง
    if is_text_file(content_bytes):
        return "Text"
    
    return "Binary"

def process_file_content(file_path, content_bytes, is_model_file_flag=False):
    """ูุนุงูุฌุฉ ูุญุชูู ุงูููู ูุฅุฑุฌุงุนู ููุต"""
    # ุฅุฐุง ูุงู ุงูููู ูู ูุฌูุฏ modelsุ ููุชูู ุจุงุณู ุงูููู ููุท
    if is_model_file_flag:
        # ูุนูุฏ ุงุณู ุงูููู ููุท ุจุฏูู ูุญุชูู
        return f"[ููู ูู ูุฌูุฏ models - ุชู ุชุณุฌูู ุงูุงุณู ููุท]\nุงุณู ุงูููู: {file_path}\n", "Model File"
    
    # ุชุญุฏูุฏ ููุน ุงูููู
    file_type = get_file_type(file_path, content_bytes)
    
    # ุฅุฐุง ูุงู ุงูููู ุซูุงุฆููุงุ ุชุฌุงููู
    if file_type != "Text":
        return None, file_type
    
    # ูุญุงููุฉ ูุฑุงุกุฉ ุงูููู ููุต UTF-8
    try:
        content = content_bytes.decode('utf-8')
        return content, "Text"
    except UnicodeDecodeError:
        # ุฅุฐุง ูุดู UTF-8ุ ุฌุฑุจ Latin-1
        try:
            content = content_bytes.decode('latin-1')
            return content, "Text"
        except UnicodeDecodeError:
            # ุฅุฐุง ูุดู ูููููุงุ ููู ุซูุงุฆู
            return None, "Binary"

def extract_archive_to_text(archive_path, output_file, archive_type="zip"):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงูุฃุฑุดูู (ZIP ุฃู RAR ุฃู TAR) ุฅูู ููู ูุตู"""
    
    if not os.path.exists(archive_path):
        print(f"ุงูููู {archive_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    # ูุงุฆูุฉ ุงูุงูุชุฏุงุฏุงุช ุงููุตูุฉ ุงููุนุฑููุฉ
    text_extensions = {
        '.txt', '.py', '.js', '.html', '.css', '.json', '.xml', 
        '.csv', '.md', '.yml', '.yaml', '.ini', '.cfg', '.conf',
        '.java', '.c', '.cpp', '.h', '.cs', '.php', '.rb', '.go',
        '.rs', '.swift', '.kt', '.sql', '.sh', '.bat', '.ps1',
        '.r', '.m', '.f', '.for', '.f90', '.f95', '.properties',
        '.toml', '.lock', '.log', '.tex', '.rst', '.adoc', '.asm',
        '.v', '.vhdl', '.verilog', '.ps', '.svg', '.ts', '.tsx',
        '.jsx', '.vue', '.svelte', '.elm', '.clj', '.scala', '.hs',
        '.lhs', '.erl', '.ex', '.exs', '.ml', '.mli'
    }
    
    # ูุงุฆูุฉ ุงูุงูุชุฏุงุฏุงุช ุงูุซูุงุฆูุฉ ุงููุนุฑููุฉ (ููุชุฌุงูู)
    binary_extensions = {
        '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin',
        '.obj', '.o', '.a', '.lib', '.dylib', '.bundle', '.class',
        '.jar', '.war', '.ear', '.apk', '.ipa', '.app', '.dmg',
        '.iso', '.img', '.raw', '.dat', '.db', '.sqlite', '.mdb',
        '.accdb', '.odb', '.hdf5', '.nc', '.mat', '.pkl', '.pickle',
        '.npy', '.npz', '.pt', '.pth', '.h5', '.hdf', '.fits',
        '.parquet', '.feather', '.orc', '.avro', '.proto', '.pb'
    }
    
    try:
        if archive_type == "zip":
            if not zipfile.is_zipfile(archive_path):
                print(f"  โ {os.path.basename(archive_path)} ููุณ ููู ZIP ุตุงูุญ!")
                return None, 0, 0
            
            archive = zipfile.ZipFile(archive_path, 'r')
            use_unrar = False
            
        elif archive_type == "rar":
            if not RAR_SUPPORT:
                print(f"  โ ููุชุจุฉ rarfile ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช RAR.")
                print(f"      ูู ุจุชุซุจูุชูุง ุจุงุณุชุฎุฏุงู: pip install rarfile")
                return None, 0, 0
            
            # ูุญุงููุฉ ุงูุนุซูุฑ ุนูู unrar
            unrar_path = None
            possible_paths = [
                'unrar',
                'C:\\Program Files\\WinRAR\\UnRAR.exe',
                'C:\\Program Files (x86)\\WinRAR\\UnRAR.exe',
                'C:\\Program Files\\7-Zip\\7z.exe',  # 7-Zip ููููู ูุชุญ RAR ุฃูุถูุง
                '/usr/bin/unrar',
                '/usr/local/bin/unrar',
                '/usr/bin/7z',  # 7z ุนูู Linux
            ]
            
            # ุชุญูู ูู ูุฌูุฏ unrar ุฃู 7z
            for path in possible_paths:
                try:
                    result = subprocess.run([path, '--version'], capture_output=True, timeout=2)
                    if result.returncode == 0:
                        unrar_path = path
                        break
                except:
                    continue
            
            use_unrar = unrar_path is not None
            
            if not use_unrar:
                print(f"  โ๏ธ  ูู ูุชู ุงูุนุซูุฑ ุนูู unrar ุฃู 7z. ุณูุชู ุงุณุชุฎุฏุงู ุงูููุฒุน ุงููุฏูุฌ.")
                print(f"      ูุฏ ูุง ุชุชููู ูู ูุฑุงุกุฉ ุจุนุถ ูููุงุช RAR.")
                
                # ูุญุงููุฉ ุชุซุจูุช unrar ุชููุงุฆููุง ุนูู Windows
                if sys.platform == "win32":
                    if install_unrar_windows():
                        # ุฅุนุงุฏุฉ ุงูุชุญูู ุจุนุฏ ุงูุชุซุจูุช
                        for path in possible_paths:
                            try:
                                result = subprocess.run([path, '--version'], capture_output=True, timeout=2)
                                if result.returncode == 0:
                                    unrar_path = path
                                    use_unrar = True
                                    break
                            except:
                                continue
            
            try:
                if not rarfile.is_rarfile(archive_path):
                    print(f"  โ {os.path.basename(archive_path)} ููุณ ููู RAR ุตุงูุญ!")
                    return None, 0, 0
                
                # ุฅุฐุง ูุงู unrar ูุชุงุญูุงุ ุงุณุชุฎุฏูู
                if use_unrar and unrar_path:
                    rarfile.UNRAR_TOOL = unrar_path
                    print(f"  โน๏ธ  ุงุณุชุฎุฏุงู {os.path.basename(unrar_path)} ููุชุญ ููู RAR")
                
                archive = rarfile.RarFile(archive_path, 'r')
                
            except rarfile.NeedFirstVolume:
                print(f"  โ {os.path.basename(archive_path)} ูุญุชุงุฌ ุฅูู ูููุงุช RAR ุฃุฎุฑู (ููู ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก).")
                return None, 0, 0
            except Exception as e:
                print(f"  โ ุฎุทุฃ ูู ูุชุญ ููู RAR: {str(e)}")
                return None, 0, 0
        
        elif archive_type == "tar":
            if not TAR_SUPPORT:
                print(f"  โ ููุชุจุฉ tarfile ุบูุฑ ูุชููุฑุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช TAR.")
                print(f"      ููุชุจุฉ tarfile ุฌุฒุก ูู ููุชุจุงุช ุจุงูุซูู ุงูููุงุณูุฉ.")
                return None, 0, 0
            
            try:
                # ูุญุงููุฉ ูุชุญ ููู TAR (ูุฏุนู .tar, .tar.gz, .tar.bz2, .tar.xz)
                archive = tarfile.open(archive_path, 'r:*')
            except Exception as e:
                print(f"  โ {os.path.basename(archive_path)} ููุณ ููู TAR ุตุงูุญ!")
                print(f"      ุงูุฎุทุฃ: {str(e)}")
                return None, 0, 0
        
        with archive:
            # ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงููููุงุช ูู ุงูุฃุฑุดูู
            if archive_type in ["zip", "rar"]:
                file_list = archive.namelist()
            elif archive_type == "tar":
                file_list = [member.name for member in archive.getmembers() 
                           if not member.isdir()]
            
            with open(output_file, 'w', encoding='utf-8') as out_file:
                out_file.write("=" * 80 + "\n")
                out_file.write(f"ูุญุชูู ุงูุฃุฑุดูู ({archive_type.upper()}): {os.path.basename(archive_path)}\n")
                if archive_type == "rar":
                    out_file.write(f"ุฃุฏุงุฉ ุงูุงุณุชุฎุฑุงุฌ: {'unrar' if use_unrar else 'ุงูููุฒุน ุงููุฏูุฌ'}\n")
                out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
                out_file.write("=" * 80 + "\n\n")
                
                files_processed = 0
                files_skipped = 0
                model_files_count = 0
                ignored_folders = set()
                ignored_venv_files = []
                binary_files = []
                archive_read_errors = 0
                
                for file_name in sorted(file_list):
                    # ุชุฎุทู ุงููุฌูุฏุงุช (ุงูุชู ุชูุชูู ุจ /)
                    if file_name.endswith('/'):
                        continue
                    
                    # ุงูุชุญูู ููุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงูููู
                    if should_ignore_file(file_name):
                        files_skipped += 1
                        
                        # ุชุณุฌูู ุฃููุงุน ุงููุฌูุฏุงุช/ุงููููุงุช ุงููุชุฌุงููุฉ
                        if '/venv/' in file_name or file_name.startswith('venv/'):
                            ignored_venv_files.append(file_name)
                            # ุฅุถุงูุฉ venv ุฅูู ูุงุฆูุฉ ุงููุฌูุฏุงุช ุงููุชุฌุงููุฉ
                            ignored_folders.add('venv')
                        
                        # ุชุณุฌูู ุงููุฌูุฏุงุช ุงูุฃุฎุฑู ุงููุชุฌุงููุฉ
                        folder = '/'.join(file_name.split('/')[:-1])
                        if folder:
                            for part in folder.split('/'):
                                if part.startswith('.') and part != '.' and part != '..':
                                    ignored_folders.add(part)
                                if part == '__pycache__':
                                    ignored_folders.add('__pycache__')
                        continue
                    
                    # ุงูุชุญูู ูู ุงูุงูุชุฏุงุฏ
                    file_ext = pathlib.Path(file_name).suffix.lower()
                    
                    # ุชุฎุทู ุงููููุงุช ุงูุซูุงุฆูุฉ ุจูุงุกู ุนูู ุงูุงูุชุฏุงุฏ
                    if file_ext in binary_extensions:
                        binary_files.append(file_name)
                        files_skipped += 1
                        continue
                    
                    try:
                        # ูุฑุงุกุฉ ูุญุชูู ุงูููู
                        if archive_type == "zip":
                            with archive.open(file_name, 'r') as file_in_archive:
                                content_bytes = file_in_archive.read()
                        elif archive_type == "rar":
                            try:
                                with archive.open(file_name, 'r') as file_in_archive:
                                    content_bytes = file_in_archive.read()
                            except Exception as e:
                                # ุฅุฐุง ูุดู ูุชุญ ููู ุฏุงุฎู RARุ ูุชุฎุทุงู
                                archive_read_errors += 1
                                # ุฅุฐุง ูุงูุช ุฃุฎุทุงุก ูุซูุฑุฉ ููู ูุณุชุฎุฏู unrarุ ููุตุญ ุจุชุซุจูุชู
                                if archive_read_errors <= 3:
                                    print(f"    โ๏ธ  ูุง ูููู ูุฑุงุกุฉ ุงูููู {file_name}: {str(e)}")
                                elif archive_read_errors == 4:
                                    print(f"    โน๏ธ  ... ูุฃุฎุทุงุก ุฃุฎุฑู ูู ูุฑุงุกุฉ ูููุงุช RAR")
                                files_skipped += 1
                                continue
                        elif archive_type == "tar":
                            try:
                                member = archive.getmember(file_name)
                                if member.isdir():
                                    continue
                                file_obj = archive.extractfile(member)
                                if file_obj:
                                    content_bytes = file_obj.read()
                                else:
                                    files_skipped += 1
                                    continue
                            except Exception as e:
                                archive_read_errors += 1
                                if archive_read_errors <= 3:
                                    print(f"    โ๏ธ  ูุง ูููู ูุฑุงุกุฉ ุงูููู {file_name}: {str(e)}")
                                elif archive_read_errors == 4:
                                    print(f"    โน๏ธ  ... ูุฃุฎุทุงุก ุฃุฎุฑู ูู ูุฑุงุกุฉ ูููุงุช TAR")
                                files_skipped += 1
                                continue
                        
                        # ุชุฎุทู ุงููููุงุช ุงููุงุฑุบุฉ
                        if len(content_bytes) == 0:
                            continue
                        
                        # ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูููู ูู ูุฌูุฏ models
                        is_model = is_model_file(file_name)
                        
                        # ูุนุงูุฌุฉ ูุญุชูู ุงูููู
                        content, file_type = process_file_content(file_name, content_bytes, is_model)
                        
                        # ุฅุฐุง ูุงู ุงูููู ุซูุงุฆููุงุ ุชุฌุงููู
                        if file_type != "Text" and file_type != "Model File":
                            binary_files.append(f"{file_name} ({file_type})")
                            files_skipped += 1
                            continue
                        
                        # ูุชุงุจุฉ ุงุณู ุงูููู ููุญุชูุงู
                        out_file.write(f"ุงุณู ุงูููู: {file_name}\n")
                        out_file.write("-" * 40 + "\n")
                        
                        if is_model:
                            # ูููููุงุช ูู ูุฌูุฏ modelsุ ููุชูู ุจูุชุงุจุฉ ุงุณู ุงูููู ููุท
                            model_files_count += 1
                            out_file.write(content)
                        elif content:
                            out_file.write(content)
                        
                        # ุฅุถุงูุฉ ุณุทุฑ ุฌุฏูุฏ ุฅุฐุง ูู ููู ููุฌูุฏูุง ูู ุงูููุงูุฉ
                        if content and not content.endswith('\n'):
                            out_file.write('\n')
                        
                        out_file.write("\n" + "=" * 80 + "\n\n")
                        files_processed += 1
                        
                    except Exception as e:
                        print(f"    โ๏ธ  ุฎุทุฃ ูู ูุนุงูุฌุฉ {file_name}: {str(e)}")
                        files_skipped += 1
                        continue
                
                # ููุฎุต ุงููุนุงูุฌุฉ
                out_file.write("\n" + "=" * 80 + "\n")
                out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {files_processed}\n")
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
                out_file.write(f"- ุนุฏุฏ ูููุงุช models (ุชู ุชุณุฌูู ุงูุฃุณูุงุก ููุท): {model_files_count}\n")
                out_file.write(f"- ุฅุฌูุงูู ุงููููุงุช ูู ุงูุฃุฑุดูู: {len(file_list)}\n")
                
                if archive_type == "rar" and archive_read_errors > 0:
                    out_file.write(f"- ุฃุฎุทุงุก ูุฑุงุกุฉ ูููุงุช RAR: {archive_read_errors}\n")
                    if not use_unrar:
                        out_file.write("  (ูุชุญุณูู ูุฑุงุกุฉ ูููุงุช RARุ ูู ุจุชุซุจูุช WinRAR ุฃู 7-Zip)\n")
                elif archive_type == "tar" and archive_read_errors > 0:
                    out_file.write(f"- ุฃุฎุทุงุก ูุฑุงุกุฉ ูููุงุช TAR: {archive_read_errors}\n")
                
                if ignored_folders:
                    out_file.write(f"- ุงููุฌูุฏุงุช/ุงูุฃููุงุน ุงููุชุฌุงููุฉ: {', '.join(sorted(ignored_folders))}\n")
                
                if ignored_venv_files:
                    out_file.write(f"- ุนุฏุฏ ุงููููุงุช ูู ูุฌูุฏุงุช venv/ ุงููุชุฌุงููุฉ: {len(ignored_venv_files)}\n")
                    if len(ignored_venv_files) <= 5:
                        out_file.write("  ุฃูุซูุฉ ุนูู ูููุงุช venv/ ุงููุชุฌุงููุฉ:\n")
                        for vf in ignored_venv_files[:5]:
                            out_file.write(f"    - {vf}\n")
                    else:
                        out_file.write(f"  (ุฃูู 5 ูู {len(ignored_venv_files)} ููู ูู venv/):\n")
                        for vf in ignored_venv_files[:5]:
                            out_file.write(f"    - {vf}\n")
                
                if binary_files:
                    out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ: {len(binary_files)}\n")
                    if len(binary_files) <= 5:
                        out_file.write("  ุฃูุซูุฉ ุนูู ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ:\n")
                        for bf in binary_files[:5]:
                            out_file.write(f"    - {bf}\n")
                    else:
                        out_file.write(f"  (ุฃูู 5 ูู {len(binary_files)} ููู ุซูุงุฆู):\n")
                        for bf in binary_files[:5]:
                            out_file.write(f"    - {bf}\n")
                
                if model_files_count > 0:
                    out_file.write(f"โน๏ธ  ููุงุญุธุฉ: ุชู ุชุณุฌูู ุฃุณูุงุก ููุท ูู {model_files_count} ููู ูู ูุฌูุฏุงุช models/\n")
                    out_file.write("   ููู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชูุงูุง ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ.\n")
                
                out_file.write("=" * 80 + "\n")
        
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช {archive_type.upper()} ุฅูู: {output_file}")
        print(f"  โ ุชูุช ูุนุงูุฌุฉ {files_processed} ููููุง ูุตููุง")
        print(f"  โ ุชู ุชุณุฌูู ุฃุณูุงุก {model_files_count} ููู ูู ูุฌูุฏุงุช models/")
        print(f"  โ ุชู ุชุฌุงูู {files_skipped} ููููุง")
        
        if archive_type == "rar" and archive_read_errors > 0:
            print(f"  โ๏ธ  ูุงูุช ููุงู {archive_read_errors} ุฃุฎุทุงุก ูู ูุฑุงุกุฉ ูููุงุช RAR")
            if not use_unrar:
                print(f"  ๐ก ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ูู ุจุชุซุจูุช WinRAR ุฃู 7-Zip")
        elif archive_type == "tar" and archive_read_errors > 0:
            print(f"  โ๏ธ  ูุงูุช ููุงู {archive_read_errors} ุฃุฎุทุงุก ูู ูุฑุงุกุฉ ูููุงุช TAR")
        
        return output_file, files_processed, files_skipped
        
    except Exception as e:
        print(f"  โ ุญุฏุซ ุฎุทุฃ ูู ูุนุงูุฌุฉ {archive_path}: {str(e)}")
        return None, 0, 0

def extract_folder_to_text(folder_path, output_file="extracted_contents.txt"):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฌูุฏ ุฅูู ููู ูุตู"""
    
    if not os.path.exists(folder_path):
        print(f"ุงููุฌูุฏ {folder_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    if not os.path.isdir(folder_path):
        print(f"{folder_path} ููุณ ูุฌูุฏูุง ุตุงูุญูุง!")
        return None, 0, 0
    
    # ูุงุฆูุฉ ุงูุงูุชุฏุงุฏุงุช ุงูุซูุงุฆูุฉ ุงููุนุฑููุฉ (ููุชุฌุงูู)
    binary_extensions = {
        '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin',
        '.obj', '.o', '.a', '.lib', '.dylib', '.bundle', '.class',
        '.jar', '.war', '.ear', '.apk', '.ipa', '.app', '.dmg',
        '.iso', '.img', '.raw', '.dat', '.db', '.sqlite', '.mdb',
        '.accdb', '.odb', '.hdf5', '.nc', '.mat', '.pkl', '.pickle',
        '.npy', '.npz', '.pt', '.pth', '.h5', '.hdf', '.fits',
        '.parquet', '.feather', '.orc', '.avro', '.proto', '.pb'
    }
    
    try:
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ุงููุฌูุฏ: {os.path.basename(folder_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n\n")
            
            files_processed = 0
            files_skipped = 0
            model_files_count = 0
            ignored_folders = set()
            ignored_venv_files = []
            binary_files = []
            total_files_count = 0
            
            # ุงุฌุชูุงุฒ ุฌููุน ุงููููุงุช ูู ุงููุฌูุฏ
            for root, dirs, files in os.walk(folder_path):
                # ุชุฌุงูู ุงููุฌูุฏุงุช ุงูุชู ุชุจุฏุฃ ุจููุทุฉ
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                
                # ุชุฌุงูู ูุฌูุฏุงุช __pycache__
                dirs[:] = [d for d in dirs if d != '__pycache__']
                
                # ุชุฌุงูู ูุฌูุฏุงุช venv
                dirs[:] = [d for d in dirs if d != 'venv']
                
                for file in files:
                    total_files_count += 1
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, folder_path)
                    
                    # ุชุญููู ูุณุงุฑ ุงููุธุงู ุฅูู ูุณุงุฑ Unix-style ููุชูุงุณู
                    rel_path_unix = rel_path.replace('\\', '/')
                    
                    # ุงูุชุญูู ููุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงูููู
                    if should_ignore_file(rel_path_unix):
                        files_skipped += 1
                        
                        # ุชุณุฌูู ุฃููุงุน ุงููุฌูุฏุงุช/ุงููููุงุช ุงููุชุฌุงููุฉ
                        if '/venv/' in rel_path_unix or rel_path_unix.startswith('venv/'):
                            ignored_venv_files.append(rel_path_unix)
                            ignored_folders.add('venv')
                        
                        # ุชุณุฌูู ุงููุฌูุฏุงุช ุงูุฃุฎุฑู ุงููุชุฌุงููุฉ
                        folder = '/'.join(rel_path_unix.split('/')[:-1])
                        if folder:
                            for part in folder.split('/'):
                                if part.startswith('.') and part != '.' and part != '..':
                                    ignored_folders.add(part)
                                if part == '__pycache__':
                                    ignored_folders.add('__pycache__')
                        continue
                    
                    # ุงูุชุญูู ูู ุงูุงูุชุฏุงุฏ
                    file_ext = pathlib.Path(file).suffix.lower()
                    
                    # ุชุฎุทู ุงููููุงุช ุงูุซูุงุฆูุฉ ุจูุงุกู ุนูู ุงูุงูุชุฏุงุฏ
                    if file_ext in binary_extensions:
                        binary_files.append(rel_path_unix)
                        files_skipped += 1
                        continue
                    
                    try:
                        # ูุฑุงุกุฉ ูุญุชูู ุงูููู
                        with open(file_path, 'rb') as f:
                            content_bytes = f.read()
                        
                        # ุชุฎุทู ุงููููุงุช ุงููุงุฑุบุฉ
                        if len(content_bytes) == 0:
                            continue
                        
                        # ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูููู ูู ูุฌูุฏ models
                        is_model = is_model_file(rel_path_unix)
                        
                        # ูุนุงูุฌุฉ ูุญุชูู ุงูููู
                        content, file_type = process_file_content(rel_path_unix, content_bytes, is_model)
                        
                        # ุฅุฐุง ูุงู ุงูููู ุซูุงุฆููุงุ ุชุฌุงููู
                        if file_type != "Text" and file_type != "Model File":
                            binary_files.append(f"{rel_path_unix} ({file_type})")
                            files_skipped += 1
                            continue
                        
                        # ูุชุงุจุฉ ุงุณู ุงูููู ููุญุชูุงู
                        out_file.write(f"ุงุณู ุงูููู: {rel_path_unix}\n")
                        out_file.write("-" * 40 + "\n")
                        
                        if is_model:
                            # ูููููุงุช ูู ูุฌูุฏ modelsุ ููุชูู ุจูุชุงุจุฉ ุงุณู ุงูููู ููุท
                            model_files_count += 1
                            out_file.write(content)
                        elif content:
                            out_file.write(content)
                        
                        # ุฅุถุงูุฉ ุณุทุฑ ุฌุฏูุฏ ุฅุฐุง ูู ููู ููุฌูุฏูุง ูู ุงูููุงูุฉ
                        if content and not content.endswith('\n'):
                            out_file.write('\n')
                        
                        out_file.write("\n" + "=" * 80 + "\n\n")
                        files_processed += 1
                        
                    except Exception as e:
                        print(f"  โ๏ธ  ุฎุทุฃ ูู ูุนุงูุฌุฉ {rel_path_unix}: {str(e)}")
                        files_skipped += 1
                        continue
            
            # ููุฎุต ุงููุนุงูุฌุฉ
            out_file.write("\n" + "=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {files_processed}\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
            out_file.write(f"- ุนุฏุฏ ูููุงุช models (ุชู ุชุณุฌูู ุงูุฃุณูุงุก ููุท): {model_files_count}\n")
            out_file.write(f"- ุฅุฌูุงูู ุงููููุงุช ุงูููุญูุตุฉ: {total_files_count}\n")
            
            if ignored_folders:
                out_file.write(f"- ุงููุฌูุฏุงุช/ุงูุฃููุงุน ุงููุชุฌุงููุฉ: {', '.join(sorted(ignored_folders))}\n")
            
            if ignored_venv_files:
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ูู ูุฌูุฏุงุช venv/ ุงููุชุฌุงููุฉ: {len(ignored_venv_files)}\n")
                if len(ignored_venv_files) <= 5:
                    out_file.write("  ุฃูุซูุฉ ุนูู ูููุงุช venv/ ุงููุชุฌุงููุฉ:\n")
                    for vf in ignored_venv_files[:5]:
                        out_file.write(f"    - {vf}\n")
                else:
                    out_file.write(f"  (ุฃูู 5 ูู {len(ignored_venv_files)} ููู ูู venv/):\n")
                    for vf in ignored_venv_files[:5]:
                        out_file.write(f"    - {vf}\n")
            
            if binary_files:
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ: {len(binary_files)}\n")
                if len(binary_files) <= 5:
                    out_file.write("  ุฃูุซูุฉ ุนูู ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ:\n")
                    for bf in binary_files[:5]:
                        out_file.write(f"    - {bf}\n")
                else:
                    out_file.write(f"  (ุฃูู 5 ูู {len(binary_files)} ููู ุซูุงุฆู):\n")
                    for bf in binary_files[:5]:
                        out_file.write(f"    - {bf}\n")
            
            if model_files_count > 0:
                out_file.write(f"โน๏ธ  ููุงุญุธุฉ: ุชู ุชุณุฌูู ุฃุณูุงุก ููุท ูู {model_files_count} ููู ูู ูุฌูุฏุงุช models/\n")
                out_file.write("   ููู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชูุงูุง ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ.\n")
            
            out_file.write("=" * 80 + "\n")
        
        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงููุฌูุฏ ุฅูู: {output_file}")
        print(f"  โ ุชูุช ูุนุงูุฌุฉ {files_processed} ููููุง ูุตููุง")
        print(f"  โ ุชู ุชุณุฌูู ุฃุณูุงุก {model_files_count} ููู ูู ูุฌูุฏุงุช models/")
        print(f"  โ ุชู ุชุฌุงูู {files_skipped} ููููุง")
        
        return output_file, files_processed, files_skipped
        
    except Exception as e:
        print(f"โ ุญุฏุซ ุฎุทุฃ ูู ูุนุงูุฌุฉ {folder_path}: {str(e)}")
        return None, 0, 0

def find_archives_in_folder(folder_path):
    """ุงูุนุซูุฑ ุนูู ุฌููุน ุงููููุงุช ุงููุถุบูุทุฉ ุฏุงุฎู ูุฌูุฏ"""
    archives = []
    
    # ุงูุชุฏุงุฏุงุช ุงููููุงุช ุงููุถุบูุทุฉ ุงููุฏุนููุฉ
    archive_extensions = ['.zip']
    if RAR_SUPPORT:
        archive_extensions.append('.rar')
    if TAR_SUPPORT:
        archive_extensions.extend(['.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz'])
    
    for root, dirs, files in os.walk(folder_path):
        # ุชุฌุงูู ุงููุฌูุฏุงุช ุงูุชู ุชุจุฏุฃ ุจููุทุฉ
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        
        for file in files:
            file_ext = pathlib.Path(file).suffix.lower()
            
            # ุชุฌุงูู ุฃุฌุฒุงุก ุงูุฃุฑุดูู ุงููุชุนุฏุฏ
            if is_split_archive_extension(file_ext):
                continue
                
            # ุงูุชุญูู ูู ุฌููุน ุงูุชุฏุงุฏุงุช TAR ุงููุญุชููุฉ
            is_tar_file = False
            if TAR_SUPPORT:
                tar_extensions = ['.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz']
                for tar_ext in tar_extensions:
                    if file.lower().endswith(tar_ext):
                        is_tar_file = True
                        break
            
            if file_ext in archive_extensions or is_tar_file:
                archive_path = os.path.join(root, file)
                archives.append(archive_path)
    
    return archives

def process_single_item(item_path):
    """ูุนุงูุฌุฉ ุนูุตุฑ ูุงุญุฏ (ููู ุฃู ูุฌูุฏ)"""
    results = []
    
    # ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูููู ุฌุฒุกูุง ูู ุฃุฑุดูู ูุชุนุฏุฏ
    if os.path.isfile(item_path):
        file_ext = pathlib.Path(item_path).suffix.lower()
        if is_split_archive_extension(file_ext):
            print(f"\n๐ ููู ุฌุฒุก ูู ุฃุฑุดูู ูุชุนุฏุฏ: {os.path.basename(item_path)}")
            print(f"   โ๏ธ  ูุฐุง ุงูููู ุฌุฒุก ูู ุฃุฑุดูู ูุถุบูุท ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก. ุณูุชู ุชุฌุงููู.")
            return results
    
    if os.path.isdir(item_path):
        print(f"\n๐ ูุนุงูุฌุฉ ุงููุฌูุฏ: {os.path.basename(item_path)}")
        
        # 1. ูุนุงูุฌุฉ ุงููุฌูุฏ ููุณู
        base_name = os.path.splitext(os.path.basename(item_path))[0]
        output_file = get_unique_filename(base_name + "_folder_contents", ".txt")
        result = extract_folder_to_text(item_path, output_file)
        if result[0]:
            results.append(result)
        
        # 2. ุงูุจุญุซ ุนู ุงููููุงุช ุงููุถุบูุทุฉ ุฏุงุฎู ุงููุฌูุฏ ููุนุงูุฌุชูุง
        archives = find_archives_in_folder(item_path)
        if archives:
            print(f"  ๐ฆ ูุฌุฏ {len(archives)} ููููุง ูุถุบูุทูุง ุฏุงุฎู ุงููุฌูุฏ:")
            for archive_path in archives:
                archive_name = os.path.basename(archive_path)
                rel_path = os.path.relpath(archive_path, item_path)
                print(f"    - {rel_path}")
                
                # ุชุญุฏูุฏ ููุน ุงูุฃุฑุดูู
                if archive_path.lower().endswith('.zip'):
                    base_name = os.path.splitext(archive_name)[0]
                    output_file = get_unique_filename(base_name + "_zip_contents", ".txt")
                    result = extract_archive_to_text(archive_path, output_file, "zip")
                elif archive_path.lower().endswith('.rar') and RAR_SUPPORT:
                    base_name = os.path.splitext(archive_name)[0]
                    output_file = get_unique_filename(base_name + "_rar_contents", ".txt")
                    result = extract_archive_to_text(archive_path, output_file, "rar")
                elif TAR_SUPPORT and tarfile and (tarfile.is_tarfile(archive_path) or 
                                                  archive_path.endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz'))):
                    base_name = os.path.splitext(archive_name)[0]
                    # ุฅุฒุงูุฉ ุงูุชุฏุงุฏุงุช ูุถุงุนูุฉ ูุซู .tar.gz
                    if base_name.endswith('.tar'):
                        base_name = base_name[:-4]
                    output_file = get_unique_filename(base_name + "_tar_contents", ".txt")
                    result = extract_archive_to_text(archive_path, output_file, "tar")
                else:
                    continue
                
                if result[0]:
                    results.append(result)
    
    elif zipfile.is_zipfile(item_path):
        print(f"\n๐ฆ ูุนุงูุฌุฉ ููู ZIP: {os.path.basename(item_path)}")
        base_name = os.path.splitext(os.path.basename(item_path))[0]
        output_file = get_unique_filename(base_name + "_zip_contents", ".txt")
        result = extract_archive_to_text(item_path, output_file, "zip")
        if result[0]:
            results.append(result)
    
    elif RAR_SUPPORT and rarfile and rarfile.is_rarfile(item_path):
        print(f"\n๐ฆ ูุนุงูุฌุฉ ููู RAR: {os.path.basename(item_path)}")
        base_name = os.path.splitext(os.path.basename(item_path))[0]
        output_file = get_unique_filename(base_name + "_rar_contents", ".txt")
        result = extract_archive_to_text(item_path, output_file, "rar")
        if result[0]:
            results.append(result)
    elif not RAR_SUPPORT and item_path.lower().endswith('.rar'):
        print(f"\nโ ููู RAR: {os.path.basename(item_path)}")
        print(f"   โ๏ธ  ููุชุจุฉ rarfile ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช RAR.")
        print(f"   โน๏ธ  ูู ุจุชุซุจูุชูุง ุจุงุณุชุฎุฏุงู: pip install rarfile")
    
    elif TAR_SUPPORT and tarfile and (tarfile.is_tarfile(item_path) or 
                                      item_path.endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz'))):
        print(f"\n๐ฆ ูุนุงูุฌุฉ ููู TAR: {os.path.basename(item_path)}")
        base_name = os.path.splitext(os.path.basename(item_path))[0]
        # ุฅุฒุงูุฉ ุงูุชุฏุงุฏุงุช ูุถุงุนูุฉ ูุซู .tar.gz
        if base_name.endswith('.tar'):
            base_name = base_name[:-4]
        output_file = get_unique_filename(base_name + "_tar_contents", ".txt")
        result = extract_archive_to_text(item_path, output_file, "tar")
        if result[0]:
            results.append(result)
    
    else:
        print(f"\nโ ููุน ุงูููู ุบูุฑ ูุนุฑูู ุฃู ุบูุฑ ูุฏุนูู: {item_path}")
        print("   ูุฌุจ ุฃู ูููู ุฅูุง ูุฌูุฏูุง ุฃู ููู ZIP ุฃู ููู RAR ุฃู ููู TAR.")
    
    return results

def main():
    """ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ"""
    
    # ุนุฑุถ ูุนูููุงุช ุญูู ุงูููุชุจุงุช ุงููุซุจุชุฉ
    print("๐ ูุญุต ุงูููุชุจุงุช ุงููุซุจุชุฉ:")
    print(f"   โ zipfile: ูุซุจุช (ุฏุนู ูููุงุช ZIP)")
    
    if RAR_SUPPORT:
        print(f"   โ rarfile: ูุซุจุช (ุฏุนู ูููุงุช RAR)")
        
        # ุงูุชุญูู ูู ูุฌูุฏ unrar ุฃู 7z
        unrar_found = False
        for tool in ['unrar', '7z']:
            try:
                subprocess.run([tool, '--version'], capture_output=True, timeout=2)
                print(f"   โ {tool}: ูุซุจุช ูู ุงููุธุงู")
                unrar_found = True
            except:
                continue
        
        if not unrar_found and sys.platform == "win32":
            # ูููููุฏูุฒุ ุชุญูู ูู ูุณุงุฑุงุช WinRAR ู7-Zip
            for path in ['C:\\Program Files\\WinRAR\\UnRAR.exe', 
                        'C:\\Program Files\\7-Zip\\7z.exe']:
                if os.path.exists(path):
                    print(f"   โ {os.path.basename(path)}: ูุซุจุช")
                    unrar_found = True
        
        if not unrar_found:
            print(f"   โ๏ธ  unrar/7z: ุบูุฑ ูุซุจุช (ูุทููุจ ููููุงุช RAR ุงููุนูุฏุฉ)")
            print(f"   โน๏ธ  ุณูุชู ุงุณุชุฎุฏุงู ุงูููุฒุน ุงููุฏูุฌ ูู rarfile")
            print(f"   ๐ก ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ูู ุจุชุซุจูุช WinRAR ุฃู 7-Zip")
    else:
        print(f"   โ rarfile: ุบูุฑ ูุซุจุช (ูุง ุฏุนู ููููุงุช RAR)")
        print(f"   โน๏ธ  ูู ุจุชุซุจูุชูุง ุจุงุณุชุฎุฏุงู: pip install rarfile")
    
    if TAR_SUPPORT:
        print(f"   โ tarfile: ูุซุจุช (ุฏุนู ูููุงุช TAR/TAR.GZ/TAR.BZ2)")
    else:
        print(f"   โ tarfile: ุบูุฑ ูุชููุฑ (ููุชุจุฉ ุจุงูุซูู ููุงุณูุฉุ ูุฌุจ ุฃู ุชููู ูุชููุฑุฉ)")
    
    print()
    
    if len(sys.argv) < 2:
        print("ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูููุงุช ZIP ุฃู RAR ุฃู TAR ุฃู ูุฌูุฏุงุช ุฅูู ูููุงุช ูุตูุฉ")
        print("=" * 60)
        print("ุงูุงุณุชุฎุฏุงู:")
        print("1. ุงุณุญุจ ุนุฏุฉ ูููุงุช ZIP/RAR/TAR ุฃู ูุฌูุฏุงุช ูุฃููุชูุง ููู ูุฐุง ุงูุณูุฑูุจุช")
        print("2. ุฃู ุงุณุชุฎุฏู: python script.py ููู1.zip ููู2.rar ููู3.tar.gz ูุฌูุฏ1 ...")
        print("\nููุงุญุธุงุช:")
        print("- ููููู ุณุญุจ ูุฅููุงุช ุนุฏุฉ ูููุงุช ููุฌูุฏุงุช ูู ููุณ ุงูููุช")
        print("- ุฅุฐุง ูุงู ุงููุฌูุฏ ูุญุชูู ุนูู ูููุงุช ูุถุบูุทุฉุ ุณูุชู ูุนุงูุฌุชูุง ุฃูุถูุง")
        print("- ุณูุชู ุชุฌุงูู ุฌููุน ุงููุฌูุฏุงุช ุงูุชู ุชุจุฏุฃ ุจููุทุฉ (ูุซู .venv, .git)")
        print("- ุณูุชู ุชุฌุงูู ูุฌูุฏุงุช __pycache__ ููููุงุช .pyc")
        print("- ุณูุชู ุชุฌุงูู ูุฌูุฏุงุช venv/ (ุจุฏูู ููุทุฉ ูู ุงูุจุฏุงูุฉ)")
        print("- ุณูุชู ุชุฌุงูู ุงููููุงุช ุงูุซูุงุฆูุฉ (ุตูุฑุ ุชูููุฐูุงุชุ ุฅูุฎ)")
        print("- ุณูุชู ุชุฌุงูู ูููุงุช ุงูุฃุฑุดูู ุงููุชุนุฏุฏุฉ ุงูุฃุฌุฒุงุก (ูุซู .z01, .z02, .r00, .part1.rar)")
        print("- ูุชู ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงููุตูุฉ ููุท")
        print("- ุณูุชู ุฅูุดุงุก ููู ูุตู ูููุตู ููู ููู/ูุฌูุฏ ูุนุงูุฌ")
        print("\n๐ ุฎุงุตูุฉ ุฌุฏูุฏุฉ: ูููุงุช ูุฌูุฏุงุช models/")
        print("- ุณูุชู ุชุณุฌูู ุฃุณูุงุก ุงููููุงุช ูู ูุฌูุฏุงุช models/ ููุท")
        print("- ูู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฐู ุงููููุงุช (ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ)")
        print("\nููุงุญุธุงุช ุญูู ูููุงุช RAR:")
        print("- ุฅุฐุง ูุงู unrar ุฃู 7-Zip ูุซุจุชูุงุ ุณูุชู ุงุณุชุฎุฏุงูู ูุชุญุณูู ุงููุชุงุฆุฌ")
        print("- ุนูู Windowsุ ูุฏ ูุญุงูู ุงูุจุฑูุงูุฌ ุชุซุจูุช unrar ุชููุงุฆููุง")
        print("\nุงุถุบุท Enter ููุฎุฑูุฌ...")
        input()
        return
    
    # ูุนุงูุฌุฉ ุฌููุน ุงูุนูุงุตุฑ ุงูููุฑุฒุฉ
    total_items = len(sys.argv) - 1
    all_results = []
    
    print(f"๐ฏ ุชู ุณุญุจ {total_items} ุนูุตุฑูุง ูููุนุงูุฌุฉ:")
    for i, item_path in enumerate(sys.argv[1:], 1):
        print(f"\n[{i}/{total_items}] ูุนุงูุฌุฉ: {item_path}")
        results = process_single_item(item_path)
        all_results.extend(results)
    
    # ุนุฑุถ ููุฎุต ููุงุฆู
    print("\n" + "=" * 60)
    print("๐ ููุฎุต ุงููุนุงูุฌุฉ ุงูููุงุฆู:")
    print("=" * 60)
    
    total_files_processed = sum(r[1] for r in all_results if r)
    total_files_skipped = sum(r[2] for r in all_results if r)
    total_output_files = len(all_results)
    
    print(f"๐ ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงูููุดุฃุฉ: {total_output_files}")
    print(f"๐ ุฅุฌูุงูู ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {total_files_processed:,}")
    print(f"๐ซ ุฅุฌูุงูู ุงููููุงุช ุงููุชุฌุงููุฉ: {total_files_skipped:,}")
    
    if all_results:
        print(f"\n๐ ูุงุฆูุฉ ุงููููุงุช ุงููุงุชุฌุฉ:")
        for i, (output_file, processed, skipped) in enumerate(all_results, 1):
            if output_file and os.path.exists(output_file):
                try:
                    file_size = os.path.getsize(output_file)
                    size_str = f"{file_size:,} ุจุงูุช"
                    if file_size > 1024*1024*1024:  # ุฃูุจุฑ ูู 1GB
                        size_str = f"{file_size/(1024*1024*1024):.1f} GB"
                    elif file_size > 1024*1024:  # ุฃูุจุฑ ูู 1MB
                        size_str = f"{file_size/(1024*1024):.1f} MB"
                    elif file_size > 1024:  # ุฃูุจุฑ ูู 1KB
                        size_str = f"{file_size/1024:.1f} KB"
                    
                    print(f"  {i:2d}. {os.path.basename(output_file)} ({size_str}) - {processed:,} ููููุง ูุนุงูุฌูุง")
                except:
                    print(f"  {i:2d}. {os.path.basename(output_file)} - {processed:,} ููููุง ูุนุงูุฌูุง")
    
    print("\nโ ุงูุชููุช ุงููุนุงูุฌุฉ!")
    print("๐ ุงูุชุงุฑูุฎ: " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    
    # ุชูุฏูู ูุตุงุฆุญ ูุชุญุณูู ูุชุงุฆุฌ RAR
    if RAR_SUPPORT:
        rar_files = [r for r in all_results if r[0] and '_rar_contents' in r[0]]
        if rar_files:
            print("\n๐ก ูุตุงุฆุญ ูุชุญุณูู ูุชุงุฆุฌ ูููุงุช RAR:")
            print("- ูู ุจุชุซุจูุช WinRAR ูู: https://www.win-rar.com/")
            print("- ุฃู ูู ุจุชุซุจูุช 7-Zip ูู: https://www.7-zip.org/")
            print("- ุจุนุฏ ุงูุชุซุจูุชุ ุฃุนุฏ ุชุดุบูู ุงูุณูุฑูุจุช ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถู")
    
    print("\n๐ ููุงุญุธุฉ: ูููุงุช ูุฌูุฏุงุช models/")
    print("- ุชู ุชุณุฌูู ุฃุณูุงุก ุงููููุงุช ูู ูุฌูุฏุงุช models/ ููุท")
    print("- ูู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฐู ุงููููุงุช (ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ)")
    
    print("\nุงุถุบุท Enter ููุฎุฑูุฌ...")
    input()

if __name__ == "__main__":
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/zip_rar_folder2txt_merged.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
ZIP/RAR/Text Folder Extractor - ูุณุชุฎุฑุฌ ูุญุชููุงุช ุงูุฃุฑุดููุงุช ูุงููุฌูุฏุงุช
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
ุงููุฏู:
- ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูููุงุช ZIP ู RAR ุฅูู ูููุงุช ูุตูุฉ
- ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงููุฌูุฏุงุช ุฅูู ูููุงุช ูุตูุฉ
- ูุนุงูุฌุฉ ุงููููุงุช ุงููุตูุฉ ุงููุฑุฏูุฉ
- ุชุฌุงูู ุงููููุงุช ุงูุซูุงุฆูุฉ ูุงููุฌูุฏุงุช ุบูุฑ ุงููุฑุบูุจ ูููุง
- ุชุชุจุน ุงููููุงุช ุงููุชุฌุงููุฉ ูุน ุฃุณุจุงุจ ุงูุชุฌุงูู

ุงูุงุณุชุฎุฏุงู:
python script.py ููู1.zip ููู2.rar ูุฌูุฏ1 ููู.txt ...
ุฃู ุงุณุญุจ ุงููููุงุช ูุฃููุชูุง ููู ุงูุณูุฑูุจุช
"""
import zipfile
import os
import sys
import pathlib
import mimetypes
import datetime
import glob
import subprocess
import tempfile
import random

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏุนู ูููุงุช RAR
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
RAR_SUPPORT = False
rarfile = None
try:
    import rarfile
    RAR_SUPPORT = True
except ImportError:
    pass

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ููุงุฆู ุงูุงูุชุฏุงุฏุงุช (ูู py_txt_zip_rar_folder2txt.py)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
TEXT_EXTENSIONS = {
    '.txt', '.py', '.js', '.html', '.css', '.json', '.xml',
    '.csv', '.md', '.yml', '.yaml', '.ini', '.cfg', '.conf',
    '.java', '.c', '.cpp', '.h', '.cs', '.php', '.rb', '.go',
    '.rs', '.swift', '.kt', '.sql', '.sh', '.bat', '.ps1',
    '.r', '.m', '.f', '.for', '.f90', '.f95', '.properties',
    '.toml', '.lock', '.log', '.tex', '.rst', '.adoc', '.asm',
    '.v', '.vhdl', '.verilog', '.ps', '.svg', '.ts', '.tsx',
    '.jsx', '.vue', '.svelte', '.elm', '.clj', '.scala', '.hs',
    '.lhs', '.erl', '.ex', '.exs', '.ml', '.mli'
}

BINARY_EXTENSIONS = {
    '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin',
    '.obj', '.o', '.a', '.lib', '.dylib', '.bundle', '.class',
    '.jar', '.war', '.ear', '.apk', '.ipa', '.app', '.dmg',
    '.iso', '.img', '.raw', '.dat', '.db', '.sqlite', '.mdb',
    '.accdb', '.odb', '.hdf5', '.nc', '.mat', '.pkl', '.pickle',
    '.npy', '.npz', '.pt', '.pth', '.h5', '.hdf', '.fits',
    '.parquet', '.feather', '.orc', '.avro', '.proto', '.pb'
}

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุณุงุนุฏุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def is_split_archive_extension(extension):
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูุงูุชุฏุงุฏ ุฌุฒุกูุง ูู ุฃุฑุดูู ูุถุบูุท ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก"""
    ext_lower = extension.lower()
    if ext_lower.startswith('.z'):
        remaining = ext_lower[2:]
        if remaining.isdigit():
            return True
    if ext_lower in ['.001', '.002', '.003', '.004', '.005',
                     '.006', '.007', '.008', '.009', '.010',
                     '.r00', '.r01', '.r02', '.r03', '.r04',
                     '.part1.rar', '.part2.rar', '.part3.rar']:
        return True
    return False

def install_unrar_windows():
    """ูุญุงููุฉ ุชุซุจูุช unrar ุนูู Windows ุฅุฐุง ูู ููู ููุฌูุฏูุง"""
    try:
        import urllib.request
        import shutil
        print("  ๐ง ูุญุงููุฉ ุชุซุจูุช unrar ุชููุงุฆููุง...")
        unrar_url = "https://www.rarlab.com/rar/unrarw32.exe"
        temp_dir = tempfile.gettempdir()
        unrar_exe = os.path.join(temp_dir, "unrar.exe")
        print(f"  ๐ฅ ุฌุงุฑู ุชูุฒูู unrar...")
        urllib.request.urlretrieve(unrar_url, unrar_exe)
        print(f"  ๐ฆ ุฌุงุฑู ุงุณุชุฎุฑุงุฌ unrar...")
        unrar_dir = os.path.join(temp_dir, "unrar")
        os.makedirs(unrar_dir, exist_ok=True)
        try:
            subprocess.run([unrar_exe, f"-o{unrar_dir}"], capture_output=True, check=True)
        except:
            try:
                import py7zr
                with py7zr.SevenZipFile(unrar_exe, mode='r') as z:
                    z.extractall(path=unrar_dir)
            except:
                pass
        for root, dirs, files in os.walk(unrar_dir):
            if "unrar.exe" in files:
                unrar_path = os.path.join(root, "unrar.exe")
                rarfile.UNRAR_TOOL = unrar_path
                print(f"  โ ุชู ุงูุนุซูุฑ ุนูู unrar ูู: {unrar_path}")
                return True
        print("  โ๏ธ  ูู ูุชู ุงูุนุซูุฑ ุนูู unrar.exe ูู ุงูููู ุงููุณุชุฎุฑุฌ")
        return False
    except Exception as e:
        print(f"  โ ูุดู ุชุซุจูุช unrar ุชููุงุฆููุง: {str(e)}")
        print(f"  โน๏ธ  ููููู ุชุซุจูุช WinRAR ูุฏูููุง ูู: https://www.win-rar.com/")
        return False

def get_unique_filename(target_dir, base_name, extension=".txt"):
    """ุฅูุดุงุก ุงุณู ููู ูุฑูุฏ ูู ุงููุณุงุฑ target_dir (ูู deepseek)"""
    counter = 1
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    base_path = os.path.join(target_dir, base_name)
    if not os.path.exists(base_path + extension):
        return base_path + extension
    new_path = os.path.join(target_dir, f"{base_name}_{timestamp}{extension}")
    if not os.path.exists(new_path):
        return new_path
    random_suffix = random.randint(1000, 9999)
    new_path = os.path.join(target_dir, f"{base_name}_{timestamp}_{random_suffix}{extension}")
    if not os.path.exists(new_path):
        return new_path
    while True:
        new_path = os.path.join(target_dir, f"{base_name}_{timestamp}_{counter}{extension}")
        if not os.path.exists(new_path):
            return new_path
        counter += 1

def is_text_file(content_bytes):
    """ูุญุต ูุง ุฅุฐุง ูุงู ุงูููู ูุตููุง ุฃู ุซูุงุฆููุง"""
    if not content_bytes:
        return False
    try:
        content_bytes.decode('utf-8')
        return True
    except UnicodeDecodeError:
        try:
            content_bytes.decode('latin-1')
            return True
        except UnicodeDecodeError:
            return False

def should_ignore_file(file_path):
    """ุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงูููู/ุงููุฌูุฏ"""
    parts = file_path.split('/')
    for part in parts:
        if part.startswith('.') and part != '.' and part != '..':
            return True
        if '__pycache__' in parts:
            return True
        if file_path.endswith('.pyc'):
            return True
        for i, part in enumerate(parts):
            if part == 'venv':
                if i < len(parts) - 1:
                    return True
                if '/venv/' in file_path:
                    return True
                if file_path.startswith('venv/'):
                    return True
    return False

def is_model_file(file_path):
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูููู ูู ูุฌูุฏ models"""
    file_path_lower = file_path.lower()
    patterns = ['models/', 'models\\', '/models/', '\\models\\']
    for pattern in patterns:
        if pattern in file_path_lower:
            return True
    if file_path_lower.startswith('models/'):
        return True
    return False

def get_file_type(file_path, content_bytes):
    """ุชุญุฏูุฏ ููุน ุงูููู ุจูุงุกู ุนูู ุงููุญุชูู"""
    if len(content_bytes) >= 4:
        if content_bytes[:4] == b'\x63\x00\x00\x00':
            return "Python Compiled (.pyc)"
        if content_bytes[:4] == b'\x7f\x45\x4c\x46':
            return "ELF Executable"
        if content_bytes[:2] == b'MZ':
            return "Windows Executable"
        if content_bytes[:8] == b'\x89\x50\x4e\x47\x0d\x0a\x1a\x0a':
            return "PNG Image"
        if content_bytes[:3] == b'\xff\xd8\xff':
            return "JPEG Image"
        if content_bytes[:4] == b'%PDF':
            return "PDF Document"
        if content_bytes[:2] == b'PK':
            return "ZIP Archive"
        if content_bytes[:2] == b'\x1f\x8b':
            return "GZIP Compressed"
    if is_text_file(content_bytes):
        return "Text"
    return "Binary"

def process_file_content(file_path, content_bytes, is_model_file_flag=False):
    """ูุนุงูุฌุฉ ูุญุชูู ุงูููู ูุฅุฑุฌุงุนู ููุต"""
    if is_model_file_flag:
        return f"[ููู ูู ูุฌูุฏ models - ุชู ุชุณุฌูู ุงูุงุณู ููุท]\nุงุณู ุงูููู: {file_path}\n", "Model File"
    file_type = get_file_type(file_path, content_bytes)
    if file_type != "Text":
        return None, file_type
    try:
        content = content_bytes.decode('utf-8')
        return content, "Text"
    except UnicodeDecodeError:
        try:
            content = content_bytes.decode('latin-1')
            return content, "Text"
        except UnicodeDecodeError:
            return None, "Binary"

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงุณุชุฎุฑุงุฌ ููู ูุฑุฏู (ูู deepseek ูุน ุชุญุณููุงุช)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def extract_single_file_to_text(file_path, output_file):
    """ูุนุงูุฌุฉ ููู ูุงุญุฏ (ูุตู) ูุงุณุชุฎุฑุงุฌ ูุญุชูุงู ุฅูู ููู ูุตู"""
    if not os.path.exists(file_path):
        print(f"ุงูููู {file_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0, []
    if not os.path.isfile(file_path):
        print(f"{file_path} ููุณ ููููุง!")
        return None, 0, 0, []

    ext = pathlib.Path(file_path).suffix.lower()
    ignored_files = []

    if ext in BINARY_EXTENSIONS:
        reason = f"ุงูุชุฏุงุฏ ุซูุงุฆู ูุนุฑูู {ext}"
        ignored_files.append((os.path.basename(file_path), reason))
        print(f"  โ๏ธ ุชุฌุงูู ({reason}): {os.path.basename(file_path)}")
        return None, 0, 1, ignored_files

    try:
        with open(file_path, 'rb') as f:
            content_bytes = f.read()

        if len(content_bytes) == 0:
            reason = "ููู ูุงุฑุบ"
            ignored_files.append((os.path.basename(file_path), reason))
            print(f"  โ๏ธ ุชุฌุงูู ({reason}): {os.path.basename(file_path)}")
            return None, 0, 1, ignored_files

        is_model = is_model_file(file_path)
        content, file_type = process_file_content(file_path, content_bytes, is_model)

        if file_type != "Text" and file_type != "Model File":
            reason = f"ููุน ุงูููู: {file_type}"
            ignored_files.append((os.path.basename(file_path), reason))
            print(f"  โ๏ธ ุชุฌุงูู ({reason}): {os.path.basename(file_path)}")
            return None, 0, 1, ignored_files

        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ุงูููู: {os.path.basename(file_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ุงุณู ุงูููู: {os.path.basename(file_path)}\n")
            out_file.write("-" * 40 + "\n")

            if is_model:
                out_file.write(content)
            elif content:
                out_file.write(content)
                if content and not content.endswith('\n'):
                    out_file.write('\n')

            out_file.write("\n" + "=" * 80 + "\n")

            # ูุชุงุจุฉ ุงููููุงุช ุงููุชุฌุงููุฉ (ูู deepseek)
            out_file.write("\n" + "=" * 80 + "\n")
            out_file.write("ุงููููุงุช ุงููุชุฌุงููุฉ:\n")
            out_file.write("=" * 80 + "\n")
            if ignored_files:
                for fname, reason in sorted(ignored_files):
                    out_file.write(f"- {fname} : {reason}\n")
            else:
                out_file.write("ูู ูุชู ุชุฌุงูู ุฃู ููู.\n")

        print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูููู ุฅูู: {output_file}")
        return output_file, 1, 0, ignored_files

    except Exception as e:
        reason = f"ุฎุทุฃ ูู ุงููุฑุงุกุฉ: {str(e)}"
        ignored_files.append((os.path.basename(file_path), reason))
        print(f"  โ๏ธ ุชุฌุงูู ({reason}): {os.path.basename(file_path)}")
        return None, 0, 1, ignored_files

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงุณุชุฎุฑุงุฌ ุฃุฑุดูู (ุฏูุฌ ุงูููุฒุชูู ูู ุงูููููู)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def extract_archive_to_text(archive_path, output_file, archive_type="zip"):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงูุฃุฑุดูู (ZIP ุฃู RAR) ุฅูู ููู ูุตู"""
    if not os.path.exists(archive_path):
        print(f"ุงูููู {archive_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0, []

    ignored_files = []

    try:
        if archive_type == "zip":
            if not zipfile.is_zipfile(archive_path):
                print(f"  โ {os.path.basename(archive_path)} ููุณ ููู ZIP ุตุงูุญ!")
                return None, 0, 0, []
            archive = zipfile.ZipFile(archive_path, 'r')
            use_unrar = False

        elif archive_type == "rar":
            if not RAR_SUPPORT:
                print(f"  โ ููุชุจุฉ rarfile ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช RAR.")
                return None, 0, 0, []

            unrar_path = None
            possible_paths = [
                'unrar',
                'C:\\Program Files\\WinRAR\\UnRAR.exe',
                'C:\\Program Files (x86)\\WinRAR\\UnRAR.exe',
                'C:\\Program Files\\7-Zip\\7z.exe',
                '/usr/bin/unrar',
                '/usr/local/bin/unrar',
                '/usr/bin/7z',
            ]

            for path in possible_paths:
                try:
                    result = subprocess.run([path, '--version'], capture_output=True, timeout=2)
                    if result.returncode == 0:
                        unrar_path = path
                        break
                except:
                    continue

            use_unrar = unrar_path is not None

            if not use_unrar and sys.platform == "win32":
                if install_unrar_windows():
                    for path in possible_paths:
                        try:
                            result = subprocess.run([path, '--version'], capture_output=True, timeout=2)
                            if result.returncode == 0:
                                unrar_path = path
                                use_unrar = True
                                break
                        except:
                            continue

            try:
                if not rarfile.is_rarfile(archive_path):
                    print(f"  โ {os.path.basename(archive_path)} ููุณ ููู RAR ุตุงูุญ!")
                    return None, 0, 0, []

                if use_unrar and unrar_path:
                    rarfile.UNRAR_TOOL = unrar_path
                    print(f"  โน๏ธ  ุงุณุชุฎุฏุงู {os.path.basename(unrar_path)} ููุชุญ ููู RAR")

                archive = rarfile.RarFile(archive_path, 'r')
            except rarfile.NeedFirstVolume:
                print(f"  โ {os.path.basename(archive_path)} ูุญุชุงุฌ ุฅูู ูููุงุช RAR ุฃุฎุฑู (ููู ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก).")
                return None, 0, 0, []
            except Exception as e:
                print(f"  โ ุฎุทุฃ ูู ูุชุญ ููู RAR: {str(e)}")
                return None, 0, 0, []

        with archive:
            file_list = archive.namelist()

            with open(output_file, 'w', encoding='utf-8') as out_file:
                out_file.write("=" * 80 + "\n")
                out_file.write(f"ูุญุชูู ุงูุฃุฑุดูู ({archive_type.upper()}): {os.path.basename(archive_path)}\n")
                if archive_type == "rar":
                    out_file.write(f"ุฃุฏุงุฉ ุงูุงุณุชุฎุฑุงุฌ: {'unrar' if use_unrar else 'ุงูููุฒุน ุงููุฏูุฌ'}\n")
                out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
                out_file.write("=" * 80 + "\n")

                files_processed = 0
                files_skipped = 0
                model_files_count = 0
                ignored_folders = set()
                ignored_venv_files = []
                binary_files = []
                rar_read_errors = 0

                for file_name in sorted(file_list):
                    if file_name.endswith('/'):
                        continue

                    if should_ignore_file(file_name):
                        files_skipped += 1
                        if '/venv/' in file_name or file_name.startswith('venv/'):
                            ignored_venv_files.append(file_name)
                            ignored_folders.add('venv')
                            reason = "ูุฌูุฏ venv/"
                        else:
                            folder = '/'.join(file_name.split('/')[:-1])
                            reason = "ูุฌูุฏ ูุฎูู ุฃู __pycache__"
                            for part in folder.split('/'):
                                if part.startswith('.') and part != '.' and part != '..':
                                    reason = f"ูุฌูุฏ ูุฎูู: {part}"
                                    ignored_folders.add(part)
                                    break
                                if part == '__pycache__':
                                    reason = "ูุฌูุฏ __pycache__"
                                    ignored_folders.add('__pycache__')
                                    break

                        ignored_files.append((file_name, reason))
                        print(f"    โ๏ธ ุชุฌุงูู ({reason}): {file_name}")
                        continue

                    file_ext = pathlib.Path(file_name).suffix.lower()

                    if file_ext in BINARY_EXTENSIONS:
                        binary_files.append(file_name)
                        files_skipped += 1
                        reason = f"ุงูุชุฏุงุฏ ุซูุงุฆู {file_ext}"
                        ignored_files.append((file_name, reason))
                        print(f"    โ๏ธ ุชุฌุงูู ({reason}): {file_name}")
                        continue

                    try:
                        if archive_type == "zip":
                            with archive.open(file_name, 'r') as file_in_archive:
                                content_bytes = file_in_archive.read()
                        elif archive_type == "rar":
                            try:
                                with archive.open(file_name, 'r') as file_in_archive:
                                    content_bytes = file_in_archive.read()
                            except Exception as e:
                                rar_read_errors += 1
                                files_skipped += 1
                                reason = f"ุฎุทุฃ ูู ูุฑุงุกุฉ RAR: {str(e)}"
                                ignored_files.append((file_name, reason))
                                print(f"    โ๏ธ ุชุฌุงูู ({reason}): {file_name}")
                                if rar_read_errors == 4:
                                    print(f"    โน๏ธ  ... ูุฃุฎุทุงุก ุฃุฎุฑู ูู ูุฑุงุกุฉ ูููุงุช RAR")
                                continue

                        if len(content_bytes) == 0:
                            files_skipped += 1
                            reason = "ููู ูุงุฑุบ"
                            ignored_files.append((file_name, reason))
                            print(f"    โ๏ธ ุชุฌุงูู ({reason}): {file_name}")
                            continue

                        is_model = is_model_file(file_name)
                        content, file_type = process_file_content(file_name, content_bytes, is_model)

                        if file_type != "Text" and file_type != "Model File":
                            binary_files.append(f"{file_name} ({file_type})")
                            files_skipped += 1
                            reason = f"ููุน ุงูููู: {file_type}"
                            ignored_files.append((file_name, reason))
                            print(f"    โ๏ธ ุชุฌุงูู ({reason}): {file_name}")
                            continue

                        out_file.write(f"ุงุณู ุงูููู: {file_name}\n")
                        out_file.write("-" * 40 + "\n")

                        if is_model:
                            model_files_count += 1
                            out_file.write(content)
                        elif content:
                            out_file.write(content)
                            if content and not content.endswith('\n'):
                                out_file.write('\n')

                        out_file.write("\n" + "=" * 80 + "\n")
                        files_processed += 1

                    except Exception as e:
                        print(f"    โ๏ธ ุฎุทุฃ ูู ูุนุงูุฌุฉ {file_name}: {str(e)}")
                        files_skipped += 1
                        reason = f"ุฎุทุฃ ูู ุงููุนุงูุฌุฉ: {str(e)}"
                        ignored_files.append((file_name, reason))
                        print(f"    โ๏ธ ุชุฌุงูู ({reason}): {file_name}")
                        continue

                # ูุชุงุจุฉ ุงูููุฎุต (ูู py_txt)
                out_file.write("\n" + "=" * 80 + "\n")
                out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
                out_file.write("=" * 80 + "\n")
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {files_processed}\n")
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
                out_file.write(f"- ุนุฏุฏ ูููุงุช models (ุชู ุชุณุฌูู ุงูุฃุณูุงุก ููุท): {model_files_count}\n")
                out_file.write(f"- ุฅุฌูุงูู ุงููููุงุช ูู ุงูุฃุฑุดูู: {len(file_list)}\n")

                if archive_type == "rar" and rar_read_errors > 0:
                    out_file.write(f"- ุฃุฎุทุงุก ูุฑุงุกุฉ ูููุงุช RAR: {rar_read_errors}\n")
                if not use_unrar:
                    out_file.write("  (ูุชุญุณูู ูุฑุงุกุฉ ูููุงุช RARุ ูู ุจุชุซุจูุช WinRAR ุฃู 7-Zip)\n")
                if ignored_folders:
                    out_file.write(f"- ุงููุฌูุฏุงุช/ุงูุฃููุงุน ุงููุชุฌุงููุฉ: {', '.join(sorted(ignored_folders))}\n")
                if ignored_venv_files:
                    out_file.write(f"- ุนุฏุฏ ุงููููุงุช ูู ูุฌูุฏุงุช venv/ ุงููุชุฌุงููุฉ: {len(ignored_venv_files)}\n")
                if binary_files:
                    out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ: {len(binary_files)}\n")
                if model_files_count > 0:
                    out_file.write(f"โน๏ธ  ููุงุญุธุฉ: ุชู ุชุณุฌูู ุฃุณูุงุก ููุท ูู {model_files_count} ููู ูู ูุฌูุฏุงุช models/\n")
                    out_file.write("   ููู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชูุงูุง ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ.\n")

                # ูุชุงุจุฉ ูุงุฆูุฉ ุงููููุงุช ุงููุชุฌุงููุฉ (ูู deepseek)
                out_file.write("\n" + "=" * 80 + "\n")
                out_file.write("ุงููููุงุช ุงููุชุฌุงููุฉ (ูุน ุณุจุจ ุงูุชุฌุงูู):\n")
                out_file.write("=" * 80 + "\n")
                if ignored_files:
                    for fname, reason in sorted(ignored_files):
                        out_file.write(f"- {fname} : {reason}\n")
                else:
                    out_file.write("ูู ูุชู ุชุฌุงูู ุฃู ููู.\n")
                out_file.write("=" * 80 + "\n")

                print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช {archive_type.upper()} ุฅูู: {output_file}")
                print(f"  โ ุชูุช ูุนุงูุฌุฉ {files_processed} ููููุง ูุตููุง")
                print(f"  โ ุชู ุชุณุฌูู ุฃุณูุงุก {model_files_count} ููู ูู ูุฌูุฏุงุช models/")
                print(f"  โ ุชู ุชุฌุงูู {files_skipped} ููููุง")
                print(f"  โ ุชู ุชุณุฌูู ุฃุณูุงุก ุงููููุงุช ุงููุชุฌุงููุฉ ูู ุงูููู ุงููุงุชุฌ ูุทุจุงุนุชูุง ุนูู ุงูุดุงุดุฉ.")

                if archive_type == "rar" and rar_read_errors > 0:
                    print(f"  โ๏ธ  ูุงูุช ููุงู {rar_read_errors} ุฃุฎุทุงุก ูู ูุฑุงุกุฉ ูููุงุช RAR")
                if not use_unrar:
                    print(f"  ๐ก ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ูู ุจุชุซุจูุช WinRAR ุฃู 7-Zip")

                return output_file, files_processed, files_skipped, ignored_files

    except Exception as e:
        print(f"  โ ุญุฏุซ ุฎุทุฃ ูู ูุนุงูุฌุฉ {archive_path}: {str(e)}")
        return None, 0, 0, []

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงุณุชุฎุฑุงุฌ ูุฌูุฏ (ุฏูุฌ ุงูููุฒุชูู ูู ุงูููููู)
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def extract_folder_to_text(folder_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฌูุฏ ุฅูู ููู ูุตู"""
    if not os.path.exists(folder_path):
        print(f"ุงููุฌูุฏ {folder_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0, []
    if not os.path.isdir(folder_path):
        print(f"{folder_path} ููุณ ูุฌูุฏูุง ุตุงูุญูุง!")
        return None, 0, 0, []

    ignored_files = []

    try:
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ุงููุฌูุฏ: {os.path.basename(folder_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n")

            files_processed = 0
            files_skipped = 0
            model_files_count = 0
            ignored_folders = set()
            ignored_venv_files = []
            binary_files = []
            total_files_count = 0

            for root, dirs, files in os.walk(folder_path):
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                dirs[:] = [d for d in dirs if d != '__pycache__']
                dirs[:] = [d for d in dirs if d != 'venv']

                for file in files:
                    total_files_count += 1
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, folder_path)
                    rel_path_unix = rel_path.replace('\\', '/')

                    if should_ignore_file(rel_path_unix):
                        files_skipped += 1
                        if '/venv/' in rel_path_unix or rel_path_unix.startswith('venv/'):
                            ignored_venv_files.append(rel_path_unix)
                            ignored_folders.add('venv')
                            reason = "ูุฌูุฏ venv/"
                        else:
                            folder = '/'.join(rel_path_unix.split('/')[:-1])
                            reason = "ูุฌูุฏ ูุฎูู ุฃู __pycache__"
                            for part in folder.split('/'):
                                if part.startswith('.') and part != '.' and part != '..':
                                    reason = f"ูุฌูุฏ ูุฎูู: {part}"
                                    ignored_folders.add(part)
                                    break
                                if part == '__pycache__':
                                    reason = "ูุฌูุฏ __pycache__"
                                    ignored_folders.add('__pycache__')
                                    break

                        ignored_files.append((rel_path_unix, reason))
                        print(f"  โ๏ธ ุชุฌุงูู ({reason}): {rel_path_unix}")
                        continue

                    file_ext = pathlib.Path(file).suffix.lower()

                    if file_ext in BINARY_EXTENSIONS:
                        binary_files.append(rel_path_unix)
                        files_skipped += 1
                        reason = f"ุงูุชุฏุงุฏ ุซูุงุฆู {file_ext}"
                        ignored_files.append((rel_path_unix, reason))
                        print(f"  โ๏ธ ุชุฌุงูู ({reason}): {rel_path_unix}")
                        continue

                    try:
                        with open(file_path, 'rb') as f:
                            content_bytes = f.read()

                        if len(content_bytes) == 0:
                            files_skipped += 1
                            reason = "ููู ูุงุฑุบ"
                            ignored_files.append((rel_path_unix, reason))
                            print(f"  โ๏ธ ุชุฌุงูู ({reason}): {rel_path_unix}")
                            continue

                        is_model = is_model_file(rel_path_unix)
                        content, file_type = process_file_content(rel_path_unix, content_bytes, is_model)

                        if file_type != "Text" and file_type != "Model File":
                            binary_files.append(f"{rel_path_unix} ({file_type})")
                            files_skipped += 1
                            reason = f"ููุน ุงูููู: {file_type}"
                            ignored_files.append((rel_path_unix, reason))
                            print(f"  โ๏ธ ุชุฌุงูู ({reason}): {rel_path_unix}")
                            continue

                        out_file.write(f"ุงุณู ุงูููู: {rel_path_unix}\n")
                        out_file.write("-" * 40 + "\n")

                        if is_model:
                            model_files_count += 1
                            out_file.write(content)
                        elif content:
                            out_file.write(content)
                            if content and not content.endswith('\n'):
                                out_file.write('\n')

                        out_file.write("\n" + "=" * 80 + "\n")
                        files_processed += 1

                    except Exception as e:
                        print(f"  โ๏ธ ุฎุทุฃ ูู ูุนุงูุฌุฉ {rel_path_unix}: {str(e)}")
                        files_skipped += 1
                        reason = f"ุฎุทุฃ ูู ุงููุนุงูุฌุฉ: {str(e)}"
                        ignored_files.append((rel_path_unix, reason))
                        print(f"  โ๏ธ ุชุฌุงูู ({reason}): {rel_path_unix}")
                        continue

            # ูุชุงุจุฉ ุงูููุฎุต
            out_file.write("\n" + "=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write("=" * 80 + "\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {files_processed}\n")
            out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
            out_file.write(f"- ุนุฏุฏ ูููุงุช models (ุชู ุชุณุฌูู ุงูุฃุณูุงุก ููุท): {model_files_count}\n")
            out_file.write(f"- ุฅุฌูุงูู ุงููููุงุช ุงูููุญูุตุฉ: {total_files_count}\n")

            if ignored_folders:
                out_file.write(f"- ุงููุฌูุฏุงุช/ุงูุฃููุงุน ุงููุชุฌุงููุฉ: {', '.join(sorted(ignored_folders))}\n")
            if ignored_venv_files:
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ูู ูุฌูุฏุงุช venv/ ุงููุชุฌุงููุฉ: {len(ignored_venv_files)}\n")
            if binary_files:
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ: {len(binary_files)}\n")
            if model_files_count > 0:
                out_file.write(f"โน๏ธ  ููุงุญุธุฉ: ุชู ุชุณุฌูู ุฃุณูุงุก ููุท ูู {model_files_count} ููู ูู ูุฌูุฏุงุช models/\n")
                out_file.write("   ููู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชูุงูุง ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ.\n")

            # ูุชุงุจุฉ ูุงุฆูุฉ ุงููููุงุช ุงููุชุฌุงููุฉ
            out_file.write("\n" + "=" * 80 + "\n")
            out_file.write("ุงููููุงุช ุงููุชุฌุงููุฉ (ูุน ุณุจุจ ุงูุชุฌุงูู):\n")
            out_file.write("=" * 80 + "\n")
            if ignored_files:
                for fname, reason in sorted(ignored_files):
                    out_file.write(f"- {fname} : {reason}\n")
            else:
                out_file.write("ูู ูุชู ุชุฌุงูู ุฃู ููู.\n")
            out_file.write("=" * 80 + "\n")

            print(f"  โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงููุฌูุฏ ุฅูู: {output_file}")
            print(f"  โ ุชูุช ูุนุงูุฌุฉ {files_processed} ููููุง ูุตููุง")
            print(f"  โ ุชู ุชุณุฌูู ุฃุณูุงุก {model_files_count} ููู ูู ูุฌูุฏุงุช models/")
            print(f"  โ ุชู ุชุฌุงูู {files_skipped} ููููุง")
            print(f"  โ ุชู ุชุณุฌูู ุฃุณูุงุก ุงููููุงุช ุงููุชุฌุงููุฉ ูู ุงูููู ุงููุงุชุฌ ูุทุจุงุนุชูุง ุนูู ุงูุดุงุดุฉ.")

            return output_file, files_processed, files_skipped, ignored_files

    except Exception as e:
        print(f"โ ุญุฏุซ ุฎุทุฃ ูู ูุนุงูุฌุฉ {folder_path}: {str(e)}")
        return None, 0, 0, []

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุฏูุงู ูุณุงุนุฏุฉ ุฅุถุงููุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def find_archives_in_folder(folder_path):
    """ุงูุนุซูุฑ ุนูู ุฌููุน ุงููููุงุช ุงููุถุบูุทุฉ ุฏุงุฎู ูุฌูุฏ"""
    archives = []
    archive_extensions = ['.zip']
    if RAR_SUPPORT:
        archive_extensions.append('.rar')

    for root, dirs, files in os.walk(folder_path):
        dirs[:] = [d for d in dirs if not d.startswith('.')]
        for file in files:
            file_ext = pathlib.Path(file).suffix.lower()
            if is_split_archive_extension(file_ext):
                continue
            if file_ext in archive_extensions:
                archive_path = os.path.join(root, file)
                archives.append(archive_path)

    return archives

def process_single_item(item_path):
    """ูุนุงูุฌุฉ ุนูุตุฑ ูุงุญุฏ (ููู ุฃู ูุฌูุฏ)"""
    results = []
    output_dir = os.path.dirname(item_path)
    if not output_dir:
        output_dir = '/'

    if os.path.isfile(item_path):
        file_ext = pathlib.Path(item_path).suffix.lower()

        if is_split_archive_extension(file_ext):
            print(f"\n๐ ููู ุฌุฒุก ูู ุฃุฑุดูู ูุชุนุฏุฏ: {os.path.basename(item_path)}")
            print(f"   โ๏ธ  ูุฐุง ุงูููู ุฌุฒุก ูู ุฃุฑุดูู ูุถุบูุท ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก. ุณูุชู ุชุฌุงููู.")
            return results

        if zipfile.is_zipfile(item_path):
            print(f"\n๐ฆ ูุนุงูุฌุฉ ููู ZIP: {os.path.basename(item_path)}")
            base_name = os.path.splitext(os.path.basename(item_path))[0]
            output_file = get_unique_filename(output_dir, base_name + "_zip_contents", ".txt")
            result = extract_archive_to_text(item_path, output_file, "zip")
            if result[0]:
                results.append((result[0], result[1], result[2]))

        elif RAR_SUPPORT and rarfile and rarfile.is_rarfile(item_path):
            print(f"\n๐ฆ ูุนุงูุฌุฉ ููู RAR: {os.path.basename(item_path)}")
            base_name = os.path.splitext(os.path.basename(item_path))[0]
            output_file = get_unique_filename(output_dir, base_name + "_rar_contents", ".txt")
            result = extract_archive_to_text(item_path, output_file, "rar")
            if result[0]:
                results.append((result[0], result[1], result[2]))

        else:
            if file_ext in TEXT_EXTENSIONS or file_ext not in BINARY_EXTENSIONS:
                print(f"\n๐ ูุนุงูุฌุฉ ููู: {os.path.basename(item_path)}")
                base_name = os.path.splitext(os.path.basename(item_path))[0]
                output_file = get_unique_filename(output_dir, base_name + "_file_contents", ".txt")
                result = extract_single_file_to_text(item_path, output_file)
                if result[0]:
                    results.append((result[0], result[1], result[2]))
            else:
                print(f"\nโ ููุน ุงูููู ุบูุฑ ูุฏุนูู ุฃู ุซูุงุฆู: {os.path.basename(item_path)}")
                print(f"   ูุฌุจ ุฃู ูููู ููู ูุตู (ูุซู .txt, .py, .md) ุฃู ููู ZIP/RAR ุฃู ูุฌูุฏ.")

    elif os.path.isdir(item_path):
        print(f"\n๐ ูุนุงูุฌุฉ ุงููุฌูุฏ: {os.path.basename(item_path)}")
        base_name = os.path.basename(item_path)
        output_file = get_unique_filename(output_dir, base_name + "_folder_contents", ".txt")
        result = extract_folder_to_text(item_path, output_file)
        if result[0]:
            results.append((result[0], result[1], result[2]))

        archives = find_archives_in_folder(item_path)
        if archives:
            print(f"  ๐ฆ ูุฌุฏ {len(archives)} ููููุง ูุถุบูุทูุง ุฏุงุฎู ุงููุฌูุฏ:")
            for archive_path in archives:
                archive_name = os.path.basename(archive_path)
                rel_path = os.path.relpath(archive_path, item_path)
                print(f"    - {rel_path}")

                if archive_path.lower().endswith('.zip'):
                    base_name = os.path.splitext(archive_name)[0]
                    output_file = get_unique_filename(output_dir, base_name + "_zip_contents", ".txt")
                    result = extract_archive_to_text(archive_path, output_file, "zip")
                elif archive_path.lower().endswith('.rar') and RAR_SUPPORT:
                    base_name = os.path.splitext(archive_name)[0]
                    output_file = get_unique_filename(output_dir, base_name + "_rar_contents", ".txt")
                    result = extract_archive_to_text(archive_path, output_file, "rar")
                else:
                    continue

                if result[0]:
                    results.append((result[0], result[1], result[2]))

    else:
        print(f"\nโ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {item_path}")

    return results

# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ุงูุฏุงูุฉ ุงูุฑุฆูุณูุฉ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
def main():
    print("๐ ูุญุต ุงูููุชุจุงุช ุงููุซุจุชุฉ:")
    print(f"   โ zipfile: ูุซุจุช (ุฏุนู ูููุงุช ZIP)")
    if RAR_SUPPORT:
        print(f"   โ rarfile: ูุซุจุช (ุฏุนู ูููุงุช RAR)")

    unrar_found = False
    for tool in ['unrar', '7z']:
        try:
            subprocess.run([tool, '--version'], capture_output=True, timeout=2)
            print(f"   โ {tool}: ูุซุจุช ูู ุงููุธุงู")
            unrar_found = True
        except:
            continue

    if not unrar_found and sys.platform == "win32":
        for path in ['C:\\Program Files\\WinRAR\\UnRAR.exe',
                     'C:\\Program Files\\7-Zip\\7z.exe']:
            if os.path.exists(path):
                print(f"   โ {os.path.basename(path)}: ูุซุจุช")
                unrar_found = True

    if not unrar_found:
        print(f"   โ๏ธ  unrar/7z: ุบูุฑ ูุซุจุช (ูุทููุจ ููููุงุช RAR ุงููุนูุฏุฉ)")
        print(f"   โน๏ธ  ุณูุชู ุงุณุชุฎุฏุงู ุงูููุฒุน ุงููุฏูุฌ ูู rarfile")
        print(f"   ๐ก ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ูู ุจุชุซุจูุช WinRAR ุฃู 7-Zip")
    else:
        print(f"   โ ุฃุฏูุงุช ูู ุงูุถุบุท: ูุซุจุชุฉ")

    if not RAR_SUPPORT:
        print(f"   โ rarfile: ุบูุฑ ูุซุจุช (ูุง ุฏุนู ููููุงุช RAR)")
        print(f"   โน๏ธ  ูู ุจุชุซุจูุชูุง ุจุงุณุชุฎุฏุงู: pip install rarfile")

    print()

    if len(sys.argv) < 2:
        print("ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูููุงุช ZIP ุฃู RAR ุฃู ูุฌูุฏุงุช ุฃู ูููุงุช ูุตูุฉ ุฅูู ูููุงุช ูุตูุฉ")
        print("=" * 60)
        print("ุงูุงุณุชุฎุฏุงู:")
        print("1. ุงุณุญุจ ุนุฏุฉ ูููุงุช ZIP/RAR ุฃู ูุฌูุฏุงุช ุฃู ูููุงุช ูุตูุฉ ูุฃููุชูุง ููู ูุฐุง ุงูุณูุฑูุจุช")
        print("2. ุฃู ุงุณุชุฎุฏู: python script.py ููู1.txt ููู2.py ูุฌูุฏ1 ููู.zip ...")
        print("\nููุงุญุธุงุช:")
        print("- ููููู ุณุญุจ ูุฅููุงุช ุนุฏุฉ ูููุงุช ููุฌูุฏุงุช ูู ููุณ ุงูููุช")
        print("- ุฅุฐุง ูุงู ุงููุฌูุฏ ูุญุชูู ุนูู ูููุงุช ูุถุบูุทุฉุ ุณูุชู ูุนุงูุฌุชูุง ุฃูุถูุง")
        print("- ุณูุชู ุชุฌุงูู ุฌููุน ุงููุฌูุฏุงุช ุงูุชู ุชุจุฏุฃ ุจููุทุฉ (ูุซู .venv, .git)")
        print("- ุณูุชู ุชุฌุงูู ูุฌูุฏุงุช __pycache__ ููููุงุช .pyc")
        print("- ุณูุชู ุชุฌุงูู ูุฌูุฏุงุช venv/ (ุจุฏูู ููุทุฉ ูู ุงูุจุฏุงูุฉ)")
        print("- ุณูุชู ุชุฌุงูู ุงููููุงุช ุงูุซูุงุฆูุฉ (ุตูุฑุ ุชูููุฐูุงุชุ ุฅูุฎ)")
        print("- ุณูุชู ุชุฌุงูู ูููุงุช ุงูุฃุฑุดูู ุงููุชุนุฏุฏุฉ ุงูุฃุฌุฒุงุก (ูุซู .z01, .z02, .r00, .part1.rar)")
        print("- ูุชู ุงุณุชุฎุฑุงุฌ ุงููููุงุช ุงููุตูุฉ ููุท")
        print("- ุณูุชู ุฅูุดุงุก ููู ูุตู ูููุตู ููู ููู/ูุฌูุฏ ูุนุงูุฌ")
        print("\n๐ ุฎุงุตูุฉ ุฌุฏูุฏุฉ: ูููุงุช ูุฌูุฏุงุช models/")
        print("- ุณูุชู ุชุณุฌูู ุฃุณูุงุก ุงููููุงุช ูู ูุฌูุฏุงุช models/ ููุท")
        print("- ูู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชูุงูุง ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ.")
        print("\n๐ ุฎุงุตูุฉ ุงููููุงุช ุงููุชุฌุงููุฉ:")
        print("- ุณูุชู ุทุจุงุนุฉ ุฃุณูุงุก ุงููููุงุช ุงููุชุฌุงููุฉ ุนูู ุงูุดุงุดุฉ ููุฑูุง ูุน ุณุจุจ ุงูุชุฌุงูู.")
        print("- ููุง ูุชู ุชุณุฌูููุง ูู ููู ุงูุฅุฎุฑุงุฌ ุงูููุงุฆู.")
        print("\nููุงุญุธุงุช ุญูู ูููุงุช RAR:")
        print("- ุฅุฐุง ูุงู unrar ุฃู 7-Zip ูุซุจุชูุงุ ุณูุชู ุงุณุชุฎุฏุงูู ูุชุญุณูู ุงููุชุงุฆุฌ")
        print("- ุนูู Windowsุ ูุฏ ูุญุงูู ุงูุจุฑูุงูุฌ ุชุซุจูุช unrar ุชููุงุฆููุง")
        print("\nุงุถุบุท Enter ููุฎุฑูุฌ...")
        input()
        return

    total_items = len(sys.argv) - 1
    all_results = []

    print(f"๐ฏ ุชู ุณุญุจ {total_items} ุนูุตุฑูุง ูููุนุงูุฌุฉ:")
    for i, item_path in enumerate(sys.argv[1:], 1):
        print(f"\n[{i}/{total_items}] ูุนุงูุฌุฉ: {item_path}")
        results = process_single_item(item_path)
        all_results.extend(results)

    print("\n" + "=" * 60)
    print("๐ ููุฎุต ุงููุนุงูุฌุฉ ุงูููุงุฆู:")
    print("=" * 60)
    total_files_processed = sum(r[1] for r in all_results if r)
    total_files_skipped = sum(r[2] for r in all_results if r)
    total_output_files = len(all_results)

    print(f"๐ ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงูููุดุฃุฉ: {total_output_files}")
    print(f"๐ ุฅุฌูุงูู ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {total_files_processed:,}")
    print(f"๐ซ ุฅุฌูุงูู ุงููููุงุช ุงููุชุฌุงููุฉ: {total_files_skipped:,}")

    if all_results:
        print(f"\n๐ ูุงุฆูุฉ ุงููููุงุช ุงููุงุชุฌุฉ:")
        for i, (output_file, processed, skipped) in enumerate(all_results, 1):
            if output_file and os.path.exists(output_file):
                try:
                    file_size = os.path.getsize(output_file)
                    size_str = f"{file_size:,} ุจุงูุช"
                    if file_size > 1024*1024*1024:
                        size_str = f"{file_size/(1024*1024*1024):.1f} GB"
                    elif file_size > 1024*1024:
                        size_str = f"{file_size/(1024*1024):.1f} MB"
                    elif file_size > 1024:
                        size_str = f"{file_size/1024:.1f} KB"
                    print(f"  {i:2d}. {os.path.basename(output_file)} ({size_str}) - {processed:,} ููููุง ูุนุงูุฌูุงุ {skipped:,} ูุชุฌุงูู")
                except:
                    print(f"  {i:2d}. {os.path.basename(output_file)} - {processed:,} ููููุง ูุนุงูุฌูุงุ {skipped:,} ูุชุฌุงูู")

    print("\nโ ุงูุชููุช ุงููุนุงูุฌุฉ!")
    print("๐ ุงูุชุงุฑูุฎ: " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

    if RAR_SUPPORT:
        rar_files = [r for r in all_results if r[0] and '_rar_contents' in r[0]]
        if rar_files:
            print("\n๐ก ูุตุงุฆุญ ูุชุญุณูู ูุชุงุฆุฌ ูููุงุช RAR:")
            print("- ูู ุจุชุซุจูุช WinRAR ูู: https://www.win-rar.com/")
            print("- ุฃู ูู ุจุชุซุจูุช 7-Zip ูู: https://www.7-zip.org/")
            print("- ุจุนุฏ ุงูุชุซุจูุชุ ุฃุนุฏ ุชุดุบูู ุงูุณูุฑูุจุช ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถู")

    print("\n๐ ููุงุญุธุฉ: ูููุงุช ูุฌูุฏุงุช models/")
    print("- ุชู ุชุณุฌูู ุฃุณูุงุก ุงููููุงุช ูู ูุฌูุฏุงุช models/ ููุท")
    print("- ูู ูุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ูุฐู ุงููููุงุช (ูุชุฌูุจ ุงููููุงุช ุงููุจูุฑุฉ)")

    print("\n๐ ุงููููุงุช ุงููุชุฌุงููุฉ:")
    print("- ุชู ุทุจุงุนุฉ ุฌููุน ุงููููุงุช ุงููุชุฌุงููุฉ ุนูู ุงูุดุงุดุฉ ุฃุซูุงุก ุงููุนุงูุฌุฉ.")
    print("- ููุง ููููู ุงูุงุทูุงุน ุนูู ุงููุงุฆูุฉ ุงููุงููุฉ ุฏุงุฎู ูู ููู ูุงุชุฌ.")

    print("\nุงุถุบุท Enter ููุฎุฑูุฌ...")
    input()

if __name__ == "__main__":
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/scripts/zip_rar_folder2txt_v2.py
----------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
(zip_rar_folder2txt.py) - ูุณุฎุฉ ูุทูุฑุฉ
ุงูุชุญุณููุงุช ุงูุฑุฆูุณูุฉ:
1. ูุนุงูุฌุฉ ูููุงุช Excel/Word ููุณุชูุฏุงุช ูููุณ ูุฃุฑุดููุงุช (ุชุฌูุจ ุงุณุชุฎุฑุงุฌ XML)
2. ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ูููุงุช HTML (ููุณ ุงูููุฏ ุงููุตุฏุฑ)
3. ุฏุนู ุชุญููู SQLite โ Excel โ ูุต ูุน ูุนุงูุฌุฉ ุงูุทูุงุจุน ุงูุฒูููุฉ
4. ุฃููููุฉ ุงููุนุงูุฌุฉ: ููุงุนุฏ ุงูุจูุงูุงุช > Excel > Word > HTML > ุฃุฑุดููุงุช > ูุต ุนุงุฏู
"""

import zipfile
import os
import sys
import pathlib
import datetime
import tempfile
import sqlite3
import re

# ============ ูุญุงููุฉ ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุงูุงุฎุชูุงุฑูุฉ ============
RAR_SUPPORT = False
rarfile = None
try:
    import rarfile
    RAR_SUPPORT = True
except ImportError:
    pass

DOCX_SUPPORT = False
try:
    from docx import Document
    DOCX_SUPPORT = True
except ImportError:
    pass

EXCEL_SUPPORT = False
try:
    import pandas as pd
    EXCEL_SUPPORT = True
except ImportError:
    pass

HTML_SUPPORT = False
try:
    from bs4 import BeautifulSoup
    HTML_SUPPORT = True
except ImportError:
    pass

# ============ ุงูุงูุชุฏุงุฏุงุช ุงููุฏุนููุฉ ============
TEXT_EXTENSIONS = {'.txt', '.py', '.js', '.json', '.xml', '.csv', '.md', '.yml', '.yaml', 
                   '.ini', '.cfg', '.conf', '.java', '.c', '.cpp', '.h', '.cs', '.php', 
                   '.rb', '.go', '.rs', '.swift', '.kt', '.sql', '.sh', '.bat', '.ps1',
                   '.r', '.m', '.f', '.for', '.f90', '.f95', '.properties', '.toml',
                   '.lock', '.log', '.tex', '.rst', '.adoc', '.asm', '.v', '.vhdl'}

DB_EXTENSIONS = {'.db', '.sqlite', '.sqlite3'}
WORD_EXTENSIONS = {'.docx'}
EXCEL_EXTENSIONS = {'.xls', '.xlsx'}
HTML_EXTENSIONS = {'.html', '.htm'}
ARCHIVE_EXTENSIONS = {'.zip', '.rar'}

# ุงูุชุฏุงุฏุงุช ุซูุงุฆูุฉ (ูุชู ุชุฌุงูููุง)
BINARY_EXTENSIONS = {'.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin', '.obj', 
                     '.o', '.a', '.lib', '.dylib', '.bundle', '.class', '.jar', 
                     '.war', '.ear', '.apk', '.ipa', '.app', '.dmg', '.iso', 
                     '.img', '.raw', '.dat', '.pkl', '.pickle', '.npy', '.npz',
                     '.pt', '.pth', '.h5', '.hdf', '.fits', '.parquet', '.feather'}

# ============ ูุธุงุฆู ูุนุงูุฌุฉ ููุงุนุฏ ุงูุจูุงูุงุช ============

def convert_timestamp(value):
    """ุชุญููู ุงูุทูุงุจุน ุงูุฒูููุฉ ุฅูู ุชูุณูู ุนุฑุจู ููุฑูุก (ุต/ู)"""
    try:
        if pd.isna(value) or value is None:
            return "NULL"
        
        # ุชุญููู ุฅูู datetime ุฅุฐุง ูุงู ุฑูููุง (timestamp)
        if isinstance(value, (int, float)):
            try:
                value = datetime.datetime.fromtimestamp(value)
            except:
                pass
        
        # ุฅุฐุง ูุงู ูุตูุงุ ุญุงูู ุชุญูููู
        if isinstance(value, str):
            formats = [
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d %H:%M:%S.%f',
                '%Y-%m-%dT%H:%M:%S',
                '%Y-%m-%dT%H:%M:%S.%f',
                '%Y-%m-%d',
                '%d/%m/%Y %H:%M:%S'
            ]
            for fmt in formats:
                try:
                    value = datetime.datetime.strptime(value, fmt)
                    break
                except:
                    continue
        
        # ุชุญููู ุฅูู ุชูุณูู 12 ุณุงุนุฉ ุจุงูุนุฑุจูุฉ (ุต/ู)
        if isinstance(value, datetime.datetime):
            hour = value.hour
            if hour == 0:
                hour_12 = 12
                meridiem = "ุต"
            elif 1 <= hour <= 11:
                hour_12 = hour
                meridiem = "ุต"
            elif hour == 12:
                hour_12 = 12
                meridiem = "ู"
            else:
                hour_12 = hour - 12
                meridiem = "ู"
            
            return f"{value.day:02d}/{value.month:02d}/{value.year} {hour_12}:{value.minute:02d}:{value.second:02d} {meridiem}"
        
        return str(value)
    except Exception:
        return str(value)

def export_db_to_excel(db_path, output_excel):
    """ุชุตุฏูุฑ ูุงุนุฏุฉ ุงูุจูุงูุงุช SQLite ุฅูู ููู Excel ูุน ูุนุงูุฌุฉ ุงูุทูุงุจุน ุงูุฒูููุฉ"""
    if not EXCEL_SUPPORT:
        print(" โ ููุชุจุฉ pandas ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ุงูุชุตุฏูุฑ ุฅูู Excel.")
        print("    pip install pandas openpyxl")
        return None, 0
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
        tables = [row[0] for row in cursor.fetchall()]
        cursor.close()
        
        if not tables:
            print(f" โ๏ธ ูุง ุชูุฌุฏ ุฌุฏุงูู ูุงุจูุฉ ููุชุตุฏูุฑ ูู {os.path.basename(db_path)}")
            conn.close()
            return None, 0
        
        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:
            total_rows = 0
            for table in tables:
                try:
                    # ูุฑุงุกุฉ ุงูุจูุงูุงุช
                    df = pd.read_sql_query(f"SELECT * FROM `{table}`", conn)
                    
                    # ูุนุงูุฌุฉ ุงูุฃุนูุฏุฉ ุงูุฒูููุฉ
                    for col in df.columns:
                        df[col] = df[col].apply(convert_timestamp)
                    
                    # ูุชุงุจุฉ ุงูุฌุฏูู ุฅูู ูุฑูุฉ Excel (ุงูุญุฏ ุงูุฃูุตู ูุงุณู ุงููุฑูุฉ 31 ุญุฑู)
                    sheet_name = table[:31]
                    df.to_excel(writer, sheet_name=sheet_name, index=False)
                    total_rows += len(df)
                    print(f" โ ุฌุฏูู '{table}': {len(df)} ุตู")
                except Exception as e:
                    print(f" โ๏ธ ุฎุทุฃ ูู ุชุตุฏูุฑ ุงูุฌุฏูู '{table}': {str(e)}")
        
        conn.close()
        return output_excel, total_rows
    
    except Exception as e:
        print(f" โ ุฎุทุฃ ูู ุงูุชุตุฏูุฑ ุฅูู Excel: {str(e)}")
        if 'conn' in locals():
            conn.close()
        return None, 0

def extract_db_via_excel_to_text(db_path, output_file):
    """ุชุญููู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุฅูู Excel ุซู ุฅูู ูุต ููุธู"""
    print(f"๐๏ธโ๐โ๐ ูุนุงูุฌุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุนุจุฑ Excel: {os.path.basename(db_path)}")
    
    # ุฅูุดุงุก ููู Excel ูุคูุช
    temp_excel = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx').name
    
    try:
        # ุงูุฎุทูุฉ 1: ุงูุชุตุฏูุฑ ุฅูู Excel
        excel_path, total_rows = export_db_to_excel(db_path, temp_excel)
        if not excel_path:
            return None, 0, 1
        
        # ุงูุฎุทูุฉ 2: ูุฑุงุกุฉ Excel ูุชุญูููู ุฅูู ูุต
        if not EXCEL_SUPPORT:
            print(" โ ููุชุจุฉ pandas ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุฑุงุกุฉ ููู Excel.")
            return None, 0, 1
        
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ูุงุนุฏุฉ ุงูุจูุงูุงุช (ุนุจุฑ Excel): {os.path.basename(db_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุชุญููู: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n\n")
            
            # ูุฑุงุกุฉ ููู Excel
            excel_file = pd.ExcelFile(excel_path)
            sheet_names = excel_file.sheet_names
            
            for sheet in sheet_names:
                df = pd.read_excel(excel_path, sheet_name=sheet)
                out_file.write(f"๐ ูุฑูุฉ ุงูุนูู: {sheet}\n")
                out_file.write("-" * 40 + "\n")
                
                # ูุชุงุจุฉ ุงูุนูุงููู
                out_file.write("\t".join(map(str, df.columns)) + "\n")
                
                # ูุชุงุจุฉ ุงูุตููู
                for _, row in df.iterrows():
                    row_str = [str(val) if not pd.isna(val) else "NULL" for val in row]
                    out_file.write("\t".join(row_str) + "\n")
                out_file.write("\n")
            
            out_file.write("=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงูุฃูุฑุงู: {len(sheet_names)}\n")
            out_file.write(f"- ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
            out_file.write("=" * 80 + "\n")
        
        print(f" โ ุชู ุงูุชุญููู ุนุจุฑ Excel ุฅูู: {output_file}")
        print(f" โ ุนุฏุฏ ุงูุฃูุฑุงู: {len(sheet_names)} ุ ุฅุฌูุงูู ุงูุตููู: {total_rows}")
        return output_file, total_rows, 0
    
    except Exception as e:
        print(f" โ ุฎุทุฃ ูู ูุนุงูุฌุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช: {str(e)}")
        return None, 0, 1
    finally:
        # ุชูุธูู ุงูููู ุงููุคูุช
        try:
            if os.path.exists(temp_excel):
                os.unlink(temp_excel)
        except:
            pass

def extract_db_direct_to_text(db_path, output_file):
    """ุงูุงุณุชุฎุฑุงุฌ ุงููุจุงุดุฑ ููุงุนุฏุฉ ุงูุจูุงูุงุช ูู TSV (ุจุฏูู Excel)"""
    if not os.path.exists(db_path):
        print(f"ุงูููู {db_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';")
        tables = cursor.fetchall()
        
        if not tables:
            print(f" โ๏ธ ูุง ุชูุฌุฏ ุฌุฏุงูู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช {os.path.basename(db_path)}")
            conn.close()
            return None, 0, 1
        
        total_rows = 0
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ูุงุนุฏุฉ ุงูุจูุงูุงุช (ูุจุงุดุฑ): {os.path.basename(db_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n\n")
            
            for table in tables:
                table_name = table[0]
                out_file.write(f"๐ ุฌุฏูู: {table_name}\n")
                out_file.write("-" * 40 + "\n")
                cursor.execute(f"SELECT * FROM \"{table_name}\"")
                rows = cursor.fetchall()
                col_names = [description[0] for description in cursor.description]
                out_file.write("\t".join(col_names) + "\n")
                
                for row in rows:
                    row_str = []
                    for value in row:
                        if value is None:
                            row_str.append("NULL")
                        elif isinstance(value, (bytes, bytearray)):
                            row_str.append("<binary data>")
                        else:
                            row_str.append(str(value))
                    out_file.write("\t".join(row_str) + "\n")
                out_file.write("\n")
                total_rows += len(rows)
                print(f" โ ุฌุฏูู {table_name}: {len(rows)} ุตู")
            
            out_file.write("=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงูุฌุฏุงูู: {len(tables)}\n")
            out_file.write(f"- ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
            out_file.write("=" * 80 + "\n")
        
        conn.close()
        print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุจุงุดุฑุฉ ุฅูู: {output_file}")
        return output_file, total_rows, 0
    
    except sqlite3.Error as e:
        print(f" โ ุฎุทุฃ ูู ูุฑุงุกุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช {db_path}: {str(e)}")
        return None, 0, 1
    except Exception as e:
        print(f" โ ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน: {str(e)}")
        return None, 0, 1

# ============ ูุธุงุฆู ูุนุงูุฌุฉ ุงููููุงุช ุงููุชุฎุตุตุฉ ============

def extract_excel_to_text(excel_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ููู Excel ููุต ููุธู (ููุณ ูุฃุฑุดูู)"""
    if not EXCEL_SUPPORT:
        print(f" โ ููุชุจุฉ pandas ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช Excel.")
        print(f"    pip install pandas openpyxl")
        return None, 0, 1
    
    if not os.path.exists(excel_path):
        print(f"ุงูููู {excel_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    try:
        # ูุฑุงุกุฉ ุฌููุน ุงูุฃูุฑุงู
        excel_file = pd.ExcelFile(excel_path)
        sheet_names = excel_file.sheet_names
        total_rows = 0
        
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ููู Excel: {os.path.basename(excel_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n\n")
            
            for sheet in sheet_names:
                df = pd.read_excel(excel_path, sheet_name=sheet)
                out_file.write(f"๐ ูุฑูุฉ ุงูุนูู: {sheet}\n")
                out_file.write("-" * 40 + "\n")
                
                # ูุชุงุจุฉ ุงูุนูุงููู
                out_file.write("\t".join(map(str, df.columns)) + "\n")
                
                # ูุชุงุจุฉ ุงูุตููู
                for _, row in df.iterrows():
                    row_str = [str(val) if not pd.isna(val) else "NULL" for val in row]
                    out_file.write("\t".join(row_str) + "\n")
                out_file.write("\n")
                
                total_rows += len(df)
                print(f" โ ูุฑูุฉ {sheet}: {len(df)} ุตู")
            
            out_file.write("=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงูุฃูุฑุงู: {len(sheet_names)}\n")
            out_file.write(f"- ุฅุฌูุงูู ุงูุตููู: {total_rows}\n")
            out_file.write("=" * 80 + "\n")
        
        print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชูู ููู Excel ุฅูู: {output_file}")
        print(f" โ ุนุฏุฏ ุงูุฃูุฑุงู: {len(sheet_names)} ุ ุฅุฌูุงูู ุงูุตููู: {total_rows}")
        return output_file, total_rows, 0
    
    except Exception as e:
        print(f" โ ุฎุทุฃ ูู ูุนุงูุฌุฉ ููู Excel {excel_path}: {str(e)}")
        return None, 0, 1

def extract_docx_to_text(docx_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู Word (.docx) ุจุงุณุชุฎุฏุงู python-docx (ููุณ ูุฃุฑุดูู)"""
    if not DOCX_SUPPORT:
        print(f" โ ููุชุจุฉ python-docx ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช Word.")
        print(f"    pip install python-docx")
        return None, 0, 1
    
    if not os.path.exists(docx_path):
        print(f"ุงูููู {docx_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    try:
        doc = Document(docx_path)
        paragraphs_count = 0
        tables_count = 0
        
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ูุณุชูุฏ Word: {os.path.basename(docx_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n\n")
            
            # ูุชุงุจุฉ ุฌููุน ุงูููุฑุงุช
            out_file.write("๐ ุงูููุฑุงุช:\n")
            out_file.write("-" * 40 + "\n")
            for para in doc.paragraphs:
                if para.text.strip():
                    out_file.write(para.text + "\n")
                    paragraphs_count += 1
            
            # ูุชุงุจุฉ ุงูุฌุฏุงูู
            if doc.tables:
                out_file.write("\n๐ ุงูุฌุฏุงูู:\n")
                out_file.write("-" * 40 + "\n")
                for table in doc.tables:
                    tables_count += 1
                    out_file.write(f"ุฌุฏูู {tables_count}:\n")
                    # ุงุณุชุฎุฑุงุฌ ุจูุงูุงุช ุงูุฌุฏูู ูู TSV
                    for row in table.rows:
                        row_cells = [cell.text.strip() for cell in row.cells]
                        out_file.write("\t".join(row_cells) + "\n")
                    out_file.write("\n")
            
            out_file.write("=" * 80 + "\n")
            out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
            out_file.write(f"- ุนุฏุฏ ุงูููุฑุงุช: {paragraphs_count}\n")
            out_file.write(f"- ุนุฏุฏ ุงูุฌุฏุงูู: {tables_count}\n")
            out_file.write("=" * 80 + "\n")
        
        print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชูู ูุณุชูุฏ Word ุฅูู: {output_file}")
        print(f" โ ุนุฏุฏ ุงูููุฑุงุช: {paragraphs_count} ุ ุนุฏุฏ ุงูุฌุฏุงูู: {tables_count}")
        return output_file, paragraphs_count + tables_count, 0
    
    except Exception as e:
        print(f" โ ุฎุทุฃ ูู ูุนุงูุฌุฉ ููู Word {docx_path}: {str(e)}")
        return None, 0, 1

def extract_html_to_text(html_path, output_file):
    """ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ููู HTML (ููุณ ุงูููุฏ ุงููุตุฏุฑ)"""
    if not HTML_SUPPORT:
        # ุฅุฐุง ูู ุชูู BeautifulSoup ูุซุจุชุฉุ ูุณุชุฎุฏู ุทุฑููุฉ ุจุฏููุฉ
        try:
            with open(html_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ุฅุฒุงูุฉ ุงูุนูุงูุงุช (tags) ูุงุณุชุฎุฑุงุฌ ุงููุตูุต
            text = re.sub(r'<[^>]+>', '', content)
            text = re.sub(r'\s+', ' ', text).strip()
            
            with open(output_file, 'w', encoding='utf-8') as out_file:
                out_file.write("=" * 80 + "\n")
                out_file.write(f"ูุญุชูู ุตูุญุฉ ุงูููุจ (ูุตู): {os.path.basename(html_path)}\n")
                out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
                out_file.write("=" * 80 + "\n\n")
                out_file.write(text)
                out_file.write("\n\n" + "=" * 80 + "\n")
            
            print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ุงููุต ูู ุตูุญุฉ ุงูููุจ ุฅูู: {output_file}")
            return output_file, 1, 0
        except Exception as e:
            print(f" โ๏ธ ูุง ูููู ูุนุงูุฌุฉ ููู HTML ุจุฏูู BeautifulSoup: {str(e)}")
            print(f"    pip install beautifulsoup4")
            return None, 0, 1
    
    if not os.path.exists(html_path):
        print(f"ุงูููู {html_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')
        
        # ุฅุฒุงูุฉ ุงูุนูุงุตุฑ ุบูุฑ ุงููุฑุบูุจ ูููุง
        for tag in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):
            tag.decompose()
        
        # ุงุณุชุฎุฑุงุฌ ุงููุตูุต
        text = soup.get_text(separator='\n', strip=True)
        
        with open(output_file, 'w', encoding='utf-8') as out_file:
            out_file.write("=" * 80 + "\n")
            out_file.write(f"ูุญุชูู ุตูุญุฉ ุงูููุจ (ูุตู): {os.path.basename(html_path)}\n")
            out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
            out_file.write("=" * 80 + "\n\n")
            out_file.write(text)
            out_file.write("\n\n" + "=" * 80 + "\n")
        
        lines_count = len(text.split('\n'))
        print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ุงููุต ูู ุตูุญุฉ ุงูููุจ ุฅูู: {output_file}")
        print(f" โ ุนุฏุฏ ุงูุฃุณุทุฑ ุงููุณุชุฎุฑุฌุฉ: {lines_count}")
        return output_file, lines_count, 0
    
    except Exception as e:
        print(f" โ ุฎุทุฃ ูู ูุนุงูุฌุฉ ููู HTML {html_path}: {str(e)}")
        return None, 0, 1

# ============ ูุธุงุฆู ุงููุนุงูุฌุฉ ุงูุนุงูุฉ ============

def is_split_archive_extension(extension):
    """ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงูุงูุชุฏุงุฏ ุฌุฒุกูุง ูู ุฃุฑุดูู ูุถุบูุท ูุชุนุฏุฏ ุงูุฃุฌุฒุงุก"""
    ext_lower = extension.lower()
    if ext_lower.startswith('.z') and ext_lower[2:].isdigit():
        return True
    if ext_lower in ['.001', '.002', '.003', '.004', '.005', '.006', '.007', '.008', '.009', '.010',
                     '.r00', '.r01', '.r02', '.r03', '.r04', '.part1.rar', '.part2.rar', '.part3.rar']:
        return True
    return False

def should_ignore_file(file_path):
    """ุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุชุฌุงูู ุงูููู/ุงููุฌูุฏ"""
    parts = file_path.split('/')
    for part in parts:
        if part.startswith('.') and part != '.' and part != '..':
            return True
    if '__pycache__' in parts:
        return True
    if file_path.endswith('.pyc'):
        return True
    for i, part in enumerate(parts):
        if part == 'venv' and i < len(parts) - 1:
            return True
    if '/venv/' in file_path or file_path.startswith('venv/'):
        return True
    return False

def get_unique_filename(base_name, extension):
    """ุฅูุดุงุก ุงุณู ููู ูุฑูุฏ ูุชุฌูุจ ุงููุชุงุจุฉ ููู ุงููููุงุช ุงูููุฌูุฏุฉ"""
    if not os.path.exists(base_name + extension):
        return base_name + extension
    
    counter = 1
    while True:
        new_name = f"{base_name}_{counter}{extension}"
        if not os.path.exists(new_name):
            return new_name
        counter += 1

def extract_archive_to_text(archive_path, output_file, archive_type="zip"):
    """ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช ุงูุฃุฑุดูู (ZIP ุฃู RAR) ุฅูู ููู ูุตู - ูุน ุชุฌุงูู ูููุงุช Excel/Word ุงูุฏุงุฎููุฉ"""
    if not os.path.exists(archive_path):
        print(f"ุงูููู {archive_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    try:
        if archive_type == "zip":
            if not zipfile.is_zipfile(archive_path):
                print(f" โ {os.path.basename(archive_path)} ููุณ ููู ZIP ุตุงูุญ!")
                return None, 0, 0
            archive = zipfile.ZipFile(archive_path, 'r')
        elif archive_type == "rar":
            if not RAR_SUPPORT:
                print(f" โ ููุชุจุฉ rarfile ุบูุฑ ูุซุจุชุฉ. ูุง ูููู ูุนุงูุฌุฉ ูููุงุช RAR.")
                return None, 0, 0
            try:
                archive = rarfile.RarFile(archive_path, 'r')
            except Exception as e:
                print(f" โ ุฎุทุฃ ูู ูุชุญ ููู RAR: {str(e)}")
                return None, 0, 0
        else:
            return None, 0, 0
        
        with archive:
            file_list = archive.namelist()
            with open(output_file, 'w', encoding='utf-8') as out_file:
                out_file.write("=" * 80 + "\n")
                out_file.write(f"ูุญุชูู ุงูุฃุฑุดูู ({archive_type.upper()}): {os.path.basename(archive_path)}\n")
                out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
                out_file.write("=" * 80 + "\n\n")
                
                files_processed = 0
                files_skipped = 0
                binary_files = []
                
                for file_name in sorted(file_list):
                    # ุชุฌุงูู ุงููุฌูุฏุงุช
                    if file_name.endswith('/'):
                        continue
                    
                    # ุชุฌุงูู ุงููููุงุช ุงููุฎููุฉ ูุงููุฌูุฏุงุช ุบูุฑ ุงููุฑุบูุจ ูููุง
                    if should_ignore_file(file_name):
                        files_skipped += 1
                        continue
                    
                    file_ext = pathlib.Path(file_name).suffix.lower()
                    
                    # ุชุฌุงูู ูููุงุช Excel/Word ุฏุงุฎู ุงูุฃุฑุดูู (ูุฃููุง ุณุชูุนุงูุฌ ููุณุชูุฏุงุช ุนูุฏ ุงุณุชุฎุฑุงุฌูุง)
                    if file_ext in EXCEL_EXTENSIONS or file_ext in WORD_EXTENSIONS:
                        files_skipped += 1
                        continue
                    
                    # ุชุฌุงูู ุงููููุงุช ุงูุซูุงุฆูุฉ
                    if file_ext in BINARY_EXTENSIONS:
                        binary_files.append(file_name)
                        files_skipped += 1
                        continue
                    
                    try:
                        if archive_type == "zip":
                            with archive.open(file_name, 'r') as file_in_archive:
                                content_bytes = file_in_archive.read()
                        elif archive_type == "rar":
                            with archive.open(file_name, 'r') as file_in_archive:
                                content_bytes = file_in_archive.read()
                        
                        if len(content_bytes) == 0:
                            continue
                        
                        # ูุญุงููุฉ ูู ุงูุชุดููุฑ
                        try:
                            content = content_bytes.decode('utf-8')
                        except UnicodeDecodeError:
                            try:
                                content = content_bytes.decode('latin-1')
                            except UnicodeDecodeError:
                                files_skipped += 1
                                continue
                        
                        out_file.write(f"ุงุณู ุงูููู: {file_name}\n")
                        out_file.write("-" * 40 + "\n")
                        out_file.write(content)
                        if not content.endswith('\n'):
                            out_file.write('\n')
                        out_file.write("\n" + "=" * 80 + "\n\n")
                        
                        files_processed += 1
                        
                    except Exception as e:
                        print(f" โ๏ธ ุฎุทุฃ ูู ูุนุงูุฌุฉ {file_name}: {str(e)}")
                        files_skipped += 1
                        continue
                
                out_file.write("=" * 80 + "\n")
                out_file.write("ููุฎุต ุงููุนุงูุฌุฉ:\n")
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: {files_processed}\n")
                out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: {files_skipped}\n")
                out_file.write(f"- ุฅุฌูุงูู ุงููููุงุช ูู ุงูุฃุฑุดูู: {len(file_list)}\n")
                if binary_files:
                    out_file.write(f"- ุนุฏุฏ ุงููููุงุช ุงูุซูุงุฆูุฉ ุงููุชุฌุงููุฉ: {len(binary_files)}\n")
                out_file.write("=" * 80 + "\n")
        
        print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชููุงุช {archive_type.upper()} ุฅูู: {output_file}")
        print(f" โ ุชูุช ูุนุงูุฌุฉ {files_processed} ููููุง ูุตููุง")
        print(f" โ ุชู ุชุฌุงูู {files_skipped} ููููุง")
        return output_file, files_processed, files_skipped
    
    except Exception as e:
        print(f" โ ุญุฏุซ ุฎุทุฃ ูู ูุนุงูุฌุฉ {archive_path}: {str(e)}")
        return None, 0, 0

def extract_single_file_to_text(file_path, output_file):
    """ูุนุงูุฌุฉ ููู ูุงุญุฏ (ูุตู) ูุงุณุชุฎุฑุงุฌ ูุญุชูุงู ุฅูู ููู ูุตู"""
    if not os.path.exists(file_path):
        print(f"ุงูููู {file_path} ุบูุฑ ููุฌูุฏ!")
        return None, 0, 0
    
    if not os.path.isfile(file_path):
        print(f"{file_path} ููุณ ููููุง!")
        return None, 0, 0
    
    ext = pathlib.Path(file_path).suffix.lower()
    
    # ุชุฌุงูู ุงููููุงุช ุงูุซูุงุฆูุฉ
    if ext in BINARY_EXTENSIONS and ext not in DB_EXTENSIONS and ext not in WORD_EXTENSIONS and ext not in EXCEL_EXTENSIONS and ext not in HTML_EXTENSIONS:
        print(f" โ๏ธ ุงูููู {os.path.basename(file_path)} ุซูุงุฆูุ ุณูุชู ุชุฌุงููู.")
        return None, 0, 1
    
    try:
        # ุชุญุฏูุฏ ููุน ุงููุนุงูุฌุฉ ุจูุงุกู ุนูู ุงูุงูุชุฏุงุฏ
        if ext in DB_EXTENSIONS:
            return extract_db_direct_to_text(file_path, output_file)
        elif ext in EXCEL_EXTENSIONS:
            return extract_excel_to_text(file_path, output_file)
        elif ext in WORD_EXTENSIONS:
            return extract_docx_to_text(file_path, output_file)
        elif ext in HTML_EXTENSIONS:
            return extract_html_to_text(file_path, output_file)
        else:
            # ูุนุงูุฌุฉ ูููู ูุตู ุนุงุฏู
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            if len(content.strip()) == 0:
                print(f" โ๏ธ ุงูููู {os.path.basename(file_path)} ูุงุฑุบ.")
                return None, 0, 1
            
            with open(output_file, 'w', encoding='utf-8') as out_file:
                out_file.write("=" * 80 + "\n")
                out_file.write(f"ูุญุชูู ุงูููู: {os.path.basename(file_path)}\n")
                out_file.write(f"ุชุงุฑูุฎ ุงูุฅูุดุงุก: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                out_file.write(f"ุงุณู ููู ุงูุฅุฎุฑุงุฌ: {os.path.basename(output_file)}\n")
                out_file.write("=" * 80 + "\n\n")
                out_file.write(content)
                if not content.endswith('\n'):
                    out_file.write('\n')
                out_file.write("\n" + "=" * 80 + "\n")
            
            print(f" โ ุชู ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงูููู ุฅูู: {output_file}")
            lines_count = len(content.split('\n'))
            return output_file, lines_count, 0
    
    except Exception as e:
        print(f" โ ุญุฏุซ ุฎุทุฃ ูู ูุนุงูุฌุฉ {file_path}: {str(e)}")
        return None, 0, 1

def process_single_item(item_path, via_excel=False):
    """ูุนุงูุฌุฉ ุนูุตุฑ ูุงุญุฏ ูุน ุฃููููุฉ ุงููุนุงูุฌุฉ ุงูุตุญูุญุฉ"""
    results = []
    
    if os.path.isfile(item_path):
        file_ext = pathlib.Path(item_path).suffix.lower()
        
        # ุชุฌุงูู ุฃุฌุฒุงุก ุงูุฃุฑุดูู ุงููุชุนุฏุฏุฉ
        if is_split_archive_extension(file_ext):
            print(f"โ๏ธ  ููู ุฌุฒุก ูู ุฃุฑุดูู ูุชุนุฏุฏ: {os.path.basename(item_path)} - ุณูุชู ุชุฌุงููู")
            return results
        
        # ุฃููููุฉ ุงููุนุงูุฌุฉ: ููุงุนุฏ ุงูุจูุงูุงุช > Excel > Word > HTML > ุฃุฑุดููุงุช > ูุต ุนุงุฏู
        base_name = os.path.splitext(os.path.basename(item_path))[0]
        
        if file_ext in DB_EXTENSIONS:
            if via_excel:
                print(f"๐๏ธโ๐โ๐ ูุนุงูุฌุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุนุจุฑ Excel: {os.path.basename(item_path)}")
                output_file = get_unique_filename(base_name + "_db_via_excel", ".txt")
                result = extract_db_via_excel_to_text(item_path, output_file)
            else:
                print(f"๐๏ธ ูุนุงูุฌุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุจุงุดุฑุฉ: {os.path.basename(item_path)}")
                output_file = get_unique_filename(base_name + "_db_direct", ".txt")
                result = extract_db_direct_to_text(item_path, output_file)
            
            if result[0]:
                results.append(result)
        
        elif file_ext in EXCEL_EXTENSIONS:
            print(f"๐ ูุนุงูุฌุฉ ููู Excel (ููุณ ูุฃุฑุดูู): {os.path.basename(item_path)}")
            output_file = get_unique_filename(base_name + "_excel_contents", ".txt")
            result = extract_excel_to_text(item_path, output_file)
            if result[0]:
                results.append(result)
        
        elif file_ext in WORD_EXTENSIONS:
            print(f"๐ ูุนุงูุฌุฉ ูุณุชูุฏ Word (ููุณ ูุฃุฑุดูู): {os.path.basename(item_path)}")
            output_file = get_unique_filename(base_name + "_doc_contents", ".txt")
            result = extract_docx_to_text(item_path, output_file)
            if result[0]:
                results.append(result)
        
        elif file_ext in HTML_EXTENSIONS:
            print(f"๐ ุงุณุชุฎุฑุงุฌ ุงููุต ูู ุตูุญุฉ ุงูููุจ (ููุณ ุงูููุฏ): {os.path.basename(item_path)}")
            output_file = get_unique_filename(base_name + "_html_text", ".txt")
            result = extract_html_to_text(item_path, output_file)
            if result[0]:
                results.append(result)
        
        elif file_ext == '.zip':
            print(f"๐ฆ ูุนุงูุฌุฉ ููู ZIP: {os.path.basename(item_path)}")
            output_file = get_unique_filename(base_name + "_zip_contents", ".txt")
            result = extract_archive_to_text(item_path, output_file, "zip")
            if result[0]:
                results.append(result)
        
        elif file_ext == '.rar' and RAR_SUPPORT:
            print(f"๐ฆ ูุนุงูุฌุฉ ููู RAR: {os.path.basename(item_path)}")
            output_file = get_unique_filename(base_name + "_rar_contents", ".txt")
            result = extract_archive_to_text(item_path, output_file, "rar")
            if result[0]:
                results.append(result)
        
        else:
            # ูุนุงูุฌุฉ ูููู ูุตู ุนุงุฏู
            if file_ext in TEXT_EXTENSIONS or file_ext not in BINARY_EXTENSIONS:
                print(f"๐ ูุนุงูุฌุฉ ููู ูุตู: {os.path.basename(item_path)}")
                output_file = get_unique_filename(base_name + "_file_contents", ".txt")
                result = extract_single_file_to_text(item_path, output_file)
                if result[0]:
                    results.append(result)
            else:
                print(f"โ ููุน ุงูููู ุบูุฑ ูุฏุนูู ุฃู ุซูุงุฆู: {os.path.basename(item_path)}")
    
    elif os.path.isdir(item_path):
        print(f"๐ ูุนุงูุฌุฉ ุงููุฌูุฏ: {os.path.basename(item_path)}")
        # (ูููู ุฅุถุงูุฉ ุฏุนู ุงููุฌูุฏุงุช ูุงุญููุง ุจููุณ ุงูููุทู)
        pass
    
    else:
        print(f"โ ุงููุณุงุฑ ุบูุฑ ููุฌูุฏ: {item_path}")
    
    return results

def main():
    print("๐ ูุญุต ุงูููุชุจุงุช ุงููุซุจุชุฉ:")
    print(f" โ zipfile: ูุซุจุช (ุฏุนู ูููุงุช ZIP)")
    print(f" โ sqlite3: ูุซุจุช (ุฏุนู ููุงุนุฏ ุงูุจูุงูุงุช SQLite)")
    
    if DOCX_SUPPORT:
        print(f" โ python-docx: ูุซุจุช (ุฏุนู ูุณุชูุฏุงุช Word .docx)")
    else:
        print(f" โ๏ธ python-docx: ุบูุฑ ูุซุจุช (ุณูุชู ุชุฌุงูู ูููุงุช Word ุฃู ูุนุงูุฌุชูุง ูุฃุฑุดูู)")
        print(f"    pip install python-docx")
    
    if EXCEL_SUPPORT:
        print(f" โ pandas: ูุซุจุช (ุฏุนู ูููุงุช Excel .xls/.xlsx)")
    else:
        print(f" โ๏ธ pandas: ุบูุฑ ูุซุจุช (ุณูุชู ุชุฌุงูู ูููุงุช Excel ุฃู ูุนุงูุฌุชูุง ูุฃุฑุดูู)")
        print(f"    pip install pandas openpyxl")
    
    if HTML_SUPPORT:
        print(f" โ beautifulsoup4: ูุซุจุช (ุงุณุชุฎุฑุงุฌ ูุตูุต ูู ุตูุญุงุช ุงูููุจ)")
    else:
        print(f" โน๏ธ beautifulsoup4: ุบูุฑ ูุซุจุช (ุณูุชู ูุนุงูุฌุฉ HTML ููุต ุนุงุฏู)")
        print(f"    pip install beautifulsoup4")
    
    if RAR_SUPPORT:
        print(f" โ rarfile: ูุซุจุช (ุฏุนู ูููุงุช RAR)")
    else:
        print(f" โน๏ธ rarfile: ุบูุฑ ูุซุจุช (ูุง ุฏุนู ููููุงุช RAR)")
        print(f"    pip install rarfile")
    
    print()
    
    if len(sys.argv) < 2:
        print("=" * 60)
        print("ุงูุงุณุชุฎุฏุงู:")
        print("1. ุงูุณุญุจ ูุงูุฅููุงุช: ุงุณุญุจ ุงููููุงุช/ุงููุฌูุฏุงุช ููู ุงูุณูุฑูุจุช")
        print("2. ุณุทุฑ ุงูุฃูุงูุฑ:")
        print("   python script.py ููู1.db ููู2.xlsx ููู3.docx ููู4.html ููู5.zip")
        print("3. ูุชุญููู ููุงุนุฏ ุงูุจูุงูุงุช ุนุจุฑ Excel (ููุตู ุจู):")
        print("   python script.py --via-excel database.db")
        print("=" * 60)
        input("ุงุถุบุท Enter ููุฎุฑูุฌ...")
        return
    
    # ุงูุชุดุงู ูุถุน ุงูุชุญููู ุนุจุฑ Excel
    via_excel = "--via-excel" in sys.argv
    args_to_process = [arg for arg in sys.argv[1:] if arg != "--via-excel"]
    
    if via_excel:
        print("๐ก ุณูุชู ุงุณุชุฎุฏุงู ุชุญููู SQLite โ Excel โ ูุต (ูููุนุงูุฌุฉ ุงููุญุณููุฉ ููุทูุงุจุน ุงูุฒูููุฉ)")
    
    total_items = len(args_to_process)
    all_results = []
    
    print(f"\n๐ฏ ุชู ุงูุนุซูุฑ ุนูู {total_items} ุนูุตุฑ ูููุนุงูุฌุฉ:")
    for i, item_path in enumerate(args_to_process, 1):
        print(f"\n[{i}/{total_items}] {'='*50}")
        results = process_single_item(item_path, via_excel=via_excel)
        all_results.extend(results)
    
    # ุนุฑุถ ููุฎุต ุงููุนุงูุฌุฉ
    print("\n" + "=" * 60)
    print("๐ ููุฎุต ุงููุนุงูุฌุฉ ุงูููุงุฆู:")
    print("=" * 60)
    
    total_files_processed = sum(r[1] for r in all_results if r)
    total_files_skipped = sum(r[2] for r in all_results if r)
    total_output_files = len(all_results)
    
    print(f"๐ ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงูููุดุฃุฉ: {total_output_files}")
    print(f"๐ ุฅุฌูุงูู ุงูุนูุงุตุฑ ุงููุนุงูุฌุฉ: {total_files_processed:,}")
    print(f"๐ซ ุฅุฌูุงูู ุงูุนูุงุตุฑ ุงููุชุฌุงููุฉ: {total_files_skipped:,}")
    
    if all_results:
        print(f"\n๐ ูุงุฆูุฉ ุงููููุงุช ุงููุงุชุฌุฉ:")
        for i, (output_file, processed, skipped) in enumerate(all_results, 1):
            if output_file and os.path.exists(output_file):
                try:
                    file_size = os.path.getsize(output_file)
                    size_str = f"{file_size:,} ุจุงูุช"
                    if file_size > 1024*1024*1024:
                        size_str = f"{file_size/(1024*1024*1024):.1f} GB"
                    elif file_size > 1024*1024:
                        size_str = f"{file_size/(1024*1024):.1f} MB"
                    elif file_size > 1024:
                        size_str = f"{file_size/1024:.1f} KB"
                    print(f" {i:2d}. {os.path.basename(output_file)} ({size_str}) - {processed:,} ุนูุตุฑ")
                except:
                    print(f" {i:2d}. {os.path.basename(output_file)} - {processed:,} ุนูุตุฑ")
    
    print("\nโ ุงูุชููุช ุงููุนุงูุฌุฉ!")
    print("๐ ุงูุชุงุฑูุฎ: " + datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    input("\nุงุถุบุท Enter ููุฎุฑูุฌ...")

if __name__ == "__main__":
    main()

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/garuda-alternatives.md
----------------------------------------
# ุจุฏุงุฆู ุจุฑุงูุฌ Windows ูู Garuda Linux

## ุฃุฏูุงุช ุงููุธุงู ูุงูุฅูุชุงุฌูุฉ

| ูููุฏows | ุจุฏูู Linux (Garuda) | ููุงุญุธุงุช |
|---------|---------------------|---------|
| Microsoft Office | LibreOffice / OnlyOffice | ูุชูุงูู ูุน .docx, .xlsx |
| Notepad++ | Kate / Mousepad / Geany | ูุญุฑุฑุงุช ุฎูููุฉ ูุน ุฏุนู ุงูุนุฑุจูุฉ |
| EmEditor | VS Code / Sublime Text | VS Code ูุชููุฑ ูุณุจูุงู |
| 7-Zip | Ark / p7zip | ูุฏูุฌ ูู ุงููุธุงู |
| WinRAR | unrar / p7zip | `sudo pacman -S unrar` |
| TeraCopy | Ultracopier / rsync | ูุชููุฑ ูู ูุณุชูุฏุนุงุช Arch |

## ุงููุชุตูุญุงุช ูุงูุงุชุตุงู

| ูููุฏows | ุจุฏูู Linux | ููุงุญุธุงุช |
|---------|------------|---------|
| Google Chrome | Chromium / Firefox | Firefox ูุซุจุช ูุณุจูุงู |
| Microsoft Edge | Firefox / Chromium | ุบูุฑ ููุตู ุจู |
| Microsoft Teams | Teams (AUR) / Web App | `yay -S teams` |
| YourPhone | KDE Connect | ูุฏูุฌ ูู Garuda (KDE) |

## ุฃุฏูุงุช ุงููุทูุฑูู

| ูููุฏows | ุจุฏูู Linux | ููุงุญุธุงุช |
|---------|------------|---------|
| Visual Studio | VS Code / Qt Creator | VS Code ูุชููุฑ ูุณุจูุงู |
| PyCharm | PyCharm Community | `yay -S pycharm-community-edition` |
| Git / GitHub CLI | Git + GitHub CLI | ูุซุจุชุงู ูุณุจูุงู |
| Docker Desktop | Docker Engine + Portainer | `sudo pacman -S docker` |
| Node.js | Node.js + npm | `sudo pacman -S nodejs npm` |
| Python 3.x | Python 3.12+ | ูุซุจุช ูุณุจูุงู |
| Miniconda | Miniforge / Mambaforge | ุจุฏูู ููุชูุญ ุงููุตุฏุฑ |

## ุงููุณุงุฆุท ูุงูุฑุณูููุงุช

| ูููุฏows | ุจุฏูู Linux | ููุงุญุธุงุช |
|---------|------------|---------|
| Photoshop | GIMP / Krita | GIMP ููุชุนุฏููุ Krita ููุฑุณู |
| ACDSee | Darktable | ูุนุงูุฌุฉ ุงูุตูุฑ ุงูุฎุงู |
| KMPlayer | VLC / MPV | VLC ูุชููุฑ ูุณุจูุงู |
| Foxit PDF | Okular / Foliate | Okular ูุฏูุฌ ูู KDE |
| ShareX | Flameshot / Spectacle | ุงูุชูุงุท ุงูุดุงุดุฉ |

## ุงูุฃูุงู ูุงูุตูุงูุฉ

| ูููุฏows | ุจุฏูู Linux | ููุงุญุธุงุช |
|---------|------------|---------|
| ESET / AVG | ClamAV + rkhunter | Linux ูุง ูุญุชุงุฌ ูุถุงุฏ ููุฑูุณุงุช ุนุงุฏุฉ |
| IObit Uninstaller | Pamac / Octopi | ูุฏูุฑ ุงูุญุฒู ุงูุฑุณููู |
| Driver Booster | - | ุงูุณูุงูุงุช ุชูุญุฏุซ ุชููุงุฆูุงู |

## ุงูุชุฑุฌูุฉ ูุงููุบุงุช

| ูููุฏows | ุจุฏูู Linux | ููุงุญุธุงุช |
|---------|------------|---------|
| Trados Studio | OmegaT / Poedit | OmegaT ูุฌุงูู ูููุชูุญ ุงููุตุฏุฑ |
| memoQ | OmegaT | ูุฏุนู ุซูุงุฆู ุงููุบุฉ |
| DeepL | DeepL Web / Argos Translate | ูููู ุงุณุชุฎุฏุงู ุงููุจ |
| ABBYY FineReader | OCRmyPDF + Tesseract | ูุฏุนู ุงูุนุฑุจูุฉ |

## ุชุซุจูุช ุงูุจุฑุงูุฌ ุนูู Garuda

```bash
# ูู ุงููุณุชูุฏุนุงุช ุงูุฑุณููุฉ
sudo pacman -S ุงุณู_ุงูุญุฒูุฉ

# ูู AUR (ุจุฑุงูุฌ ุฅุถุงููุฉ)
yay -S ุงุณู_ุงูุญุฒูุฉ

# ูู Flatpak
flatpak install flathub ุงุณู_ุงูุญุฒูุฉ
```

## ุฃูุงูุฑ ุงูุชุญุฏูุซ ุงูุฃุณุงุณูุฉ

```bash
# ุชุญุฏูุซ ุงููุธุงู ุงููุงูู
sudo pacman -Syu

# ุฃู ุงุณุชุฎุฏุงู ุฃุฏุงุฉ Garuda
garuda-update

# ุชุซุจูุช ุญุฒู ูุชุนุฏุฏุฉ
sudo pacman -S --needed ุญุฒูุฉ1 ุญุฒูุฉ2 ุญุฒูุฉ3

# ุงูุจุญุซ ุนู ุญุฒูุฉ
pacman -Ss ุงุณู_ุงูุญุฒูุฉ
```

## ููุงุญุธุงุช ูููุฉ

1. **Garuda Linux** ูุจูู ุนูู Arch ููุฃุชู ูุน `garuda-update` ููุชุญุฏูุซ ุงูุชููุงุฆู
2. **ูุง ุชุญุชุงุฌ ุจุฑุงูุฌ ุตูุงูุฉ** ูุซู ูููุฏูุฒ - Linux ูุง ูุชุทูุจ ุชูุธูู ุงูุณุฌู
3. ููุจุฑุงูุฌ ุงููุบููุฉ ุงููุตุฏุฑุ ุบุงูุจุงู ุชูุฌุฏ ุจุฏุงุฆู ููุชูุญุฉ ุฃูุถู

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/garuda-packages-list.md
----------------------------------------
# ูุงุฆูุฉ ุญุฒู Garuda Linux

## ูุธุฑุฉ ุนุงูุฉ

| ุงูุฅุญุตุงุฆูุฉ | ุงููููุฉ |
|-----------|--------|
| ุฅุฌูุงูู ุงูุญุฒู | 336 ุญุฒูุฉ |
| ุงููุตุฏุฑ | Garuda-Pkg-Library-20260203 |
| ุงูุชุงุฑูุฎ | ูุจุฑุงูุฑ 2026 |

---

## ุงูุญุฒู ุญุณุจ ุงูุชุตููู

### ๐ฅ๏ธ ุฃุฏูุงุช ุงููุธุงู ุงูุฃุณุงุณูุฉ

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| pacman | ูุฏูุฑ ุงูุญุฒู |
| pacman-mirrorlist | ูุงุฆูุฉ ุงููุฑุงูุง |
| systemd | ูุธุงู init |
| pam | ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู |
| procps-ng | ุฃุฏูุงุช ุงูุนูููุงุช |
| reflector | ุชุญุฏูุซ ุงููุฑุงูุง |

### ๐ ุงูุดุจูุงุช ูุงูุงุชุตุงู

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| openvpn | VPN client |
| pipewire | ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู |
| pipewire-audio | ุตูุช Pipewire |
| pipewire-pulse | ุชูุงูู PulseAudio |
| pipewire-jack | ุชูุงูู JACK |
| networkmanager | ูุฏูุฑ ุงูุดุจูุฉ |
| wget | ุชูุฒูู ุงููููุงุช |
| rclone | ูุฒุงููุฉ ุงูุณุญุงุจุฉ |
| tcpdump | ุชุญููู ุงูุดุจูุฉ |
| ufw | ุฌุฏุงุฑ ุญูุงูุฉ |
| proxychains-ng | ุจุฑููุณู |

### ๐ฆ ุฃุฏูุงุช ุงูุชุทููุฑ

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| python | ูุบุฉ Python |
| python-pip | ูุฏูุฑ ุญุฒู Python |
| python-numpy | ุญุณุงุจ ุนููู |
| python-pydantic | ุงูุชุญูู ูู ุงูุจูุงูุงุช |
| rust | ูุบุฉ Rust |
| nodejs | Node.js |
| npm | ูุฏูุฑ ุญุฒู Node |
| cmake | ุจูุงุก ุงููุดุงุฑูุน |
| meson | ูุธุงู ุจูุงุก |
| ninja | ุจูุงุก ุณุฑูุน |
| vala | ูุบุฉ Vala |
| doxygen | ุชูุซูู ุงูููุฏ |
| pandoc-cli | ุชุญููู ุงููุณุชูุฏุงุช |

### ๐จ ูุงุฌูุฉ ุงููุณุชุฎุฏู (Qt/KDE)

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| qt6 | Qt 6 |
| qt6-base | ููุชุจุฉ Qt ุงูุฃุณุงุณูุฉ |
| qt6-tools | ุฃุฏูุงุช Qt |
| qt6-webengine | ูุชุตูุญ Qt |
| pyside6 | Qt for Python |
| plasma-integration | ุชูุงูู KDE |
| plasma-wayland-protocols | ุจุฑูุชููููุงุช Wayland |

### ๐ต ุงููุณุงุฆุท ุงููุชุนุฏุฏุฉ

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| telegram-desktop | ุชูููุฌุฑุงู |
| thunderbird | ุจุฑูุฏ ุฅููุชุฑููู |
| vlc | ูุดุบู ููุฏูู |
| simple-scan | ูุงุณุญ ุถูุฆู |
| xournalpp | ููุงุญุธุงุช ุฑูููุฉ |

### ๐ง ุฃุฏูุงุช ุงููุธุงู ุงููุชูุฏูุฉ

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| virtualbox | ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ |
| virtualbox-host-dkms | ูุญุฏุงุช kernel |
| qemu-desktop | ูุญุงูุงุฉ QEMU |
| waydroid | ุชุดุบูู Android |
| docker | ุญุงููุงุช |
| wine-staging | ุชุดุบูู Windows |
| winetricks | ุฃุฏูุงุช Wine |
| yay | ูุฏูุฑ AUR |

### ๐ Office ูุงูุฅูุชุงุฌูุฉ

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| wps-office | WPS Office |
| wps-office-cn | WPS Office CN |
| libreoffice-fresh | LibreOffice |
| trilium-next | ููุงุญุธุงุช |

### ๐ค ุงูุฎุทูุท

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| ttf-dejavu | ุฎุท DejaVu |
| ttf-fira-code | ุฎุท Fira Code |
| ttf-jetbrains-mono | ุฎุท JetBrains |
| ttf-liberation | ุฎุทูุท Liberation |
| ttf-ms-fonts | ุฎุทูุท Microsoft |
| ttf-ubuntu-font-family | ุฎุทูุท Ubuntu |
| noto-fonts | ุฎุทูุท Noto |

### ๐ก๏ธ ุงูุฃูุงู

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| yara | ุชุญููู ุงูุจุฑูุฌูุงุช |
| rhash | ุญุณุงุจ ุงููุงุด |
| p11-kit | ุดูุงุฏุงุช PKCS |

### ๐ผ๏ธ ุงูุณูุงุช ูุงูุฃููููุงุช

| ุงูุญุฒูุฉ | ุงููุตู |
|--------|-------|
| papirus-icon-theme | ุฃููููุงุช Papirus |
| tela-icon-theme | ุฃููููุงุช Tela |
| orchis-theme | ุณูุฉ Orchis |
| qogir-gtk-theme | ุณูุฉ Qogir |

---

## ุฃูุงูุฑ ูููุฏุฉ

```bash
# ุชุซุจูุช ุฌููุน ุงูุญุฒู ูู ุงููุงุฆูุฉ
sudo pacman -S $(cat packages_list.txt)

# ุงูุจุญุซ ุนู ุญุฒูุฉ
pacman -Ss package_name

# ูุนูููุงุช ุนู ุญุฒูุฉ
pacman -Qi package_name
```

---

## ูุตุงุฏุฑ ุฅุถุงููุฉ

- [Arch Linux Packages](https://archlinux.org/packages/)
- [Garuda Linux](https://garudalinux.org/)
- [AUR](https://aur.archlinux.org/)

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/linux/package-lists.md
----------------------------------------
# ููุงุฆู ุญุฒู Arch/Garuda - ูุฑุฌุน ุงูุชุซุจูุช

## ููุฎุต
ูุงุฆูุฉ ุจุงูุญุฒู ุงููุซุจุชุฉ ุนูู ูุธุงู Garuda Linux ููุฑุฌูุน ุฅูููุง ุนูุฏ ุฅุนุงุฏุฉ ุงูุชุซุจูุช.

---

## ๐ฆ ุญุฒู ุงูุฃุฏูุงุช ุงูุฃุณุงุณูุฉ

### ุงูุฃุฏูุงุช ุงููุตูุฉ
```
fastfetch-2.58.0
7zip-25.01
unrar-7.2.3
unzip-6.0
dos2unix-7.5.3
cdrtools-3.02a09
debtap-3.6.3
```

### ุงูุฎุทูุท
```
noto-fonts-2026.02.01
ttf-jetbrains-mono-2.304
ttf-fira-code-6.2
ttf-firacode-nerd-3.4.0
ttf-carlito-20230509
ttf-liberation-2.1.5
ttf-ms-fonts-2.0
ttf-ubuntu-font-family-0.83
powerline-fonts-2.8.4
```

### ุชุทุจููุงุช GNOME
```
gnome-calculator-49.2
gnome-calendar-49.1
gnome-maps-49.4
gnome-sound-recorder-42.0
gnome-system-monitor-49.1
gnome-weather-49.0
simple-scan-49.1
snapshot-49.1
tecla-49.0
d-spy-49.2
sysprof-49.0
```

---

## ๐ง ุญุฒู ุงูุชุทููุฑ

### ูุบุงุช ุงูุจุฑูุฌุฉ
```
python-3.14.2
python-numpy-2.4.2
python-pip-25.3
python-pipx-1.8.0
python-virtualenv-20.36.1
nodejs-25.4.0
npm-11.8.0
rust-1.93.0
dotnet-sdk-10.0.0
```

### ุฃุฏูุงุช ุงูุชุทููุฑ
```
cmake-4.2.3
meson-1.10.1
ninja-1.13.2
doxygen-1.16.1
ctags-6.2.1
pandoc-cli-3.5
```

### IDEs
```
code-1.107.1          # VS Code
code-marketplace-1.106.0
```

---

## ๐ ุญุฒู ุงูุดุจูุงุช

```
openvpn-2.6.17
wget-1.25.0
ufw-0.36.2            # Firewall
proxychains-ng-4.17
```

---

## ๐ฅ๏ธ ุญุฒู ุณุทุญ ุงูููุชุจ

### Plasma/KDE
```
plasma-integration-6.5.5
plasma-wayland-protocols-1.20.0
plasma6-applets-panel-colorizer-6.6.0
plasma6-themes-layan-git-r99
plasma5-themes-sweet-full-git-r442
```

### ุซููุงุช
```
orchis-theme-2025.04.25
papirus-icon-theme-20250501
papirus-folders-1.14.0
tela-icon-theme-2025.02.10
```

---

## ๐ฑ ุญุฒู ุงูุชูุงุตู

```
telegram-desktop-6.4.4
thunderbird-147.0.1
```

---

## ๐ฎ ุญุฒู ุงูุฃูุนุงุจ ูุงููุณุงุฆุท

```
wine-staging-11.1
wine-mono-10.4.1
winetricks-20260125
waydroid-1.6.1
virtualbox-7.2.6
virt-manager-5.1.0
```

---

## ๐ ุญุฒู ุงูููุชุจ

```
libreoffice-fresh-25.8.4
libreoffice-fresh-ar-25.8.4
wps-office-cn-12.1.2.24722
wps-office-fonts-1.0
```

---

## ๐ ุญุฒู ูููุฏุฉ ููุจุญุซ

### ุงูุจุญุซ ุนู ุญุฒูุฉ
```bash
# ุงูุจุญุซ ูู ุงููููุงุช ุงููุญููุฉ
grep -i "package-name" pkglist.txt

# ุงูุจุญุซ ูู pacman
pacman -Ss keyword

# ูุนูููุงุช ุญุฒูุฉ
pacman -Qi package-name
```

### ุชุซุจูุช ูู ููู
```bash
sudo pacman -U package-name.pkg.tar.zst
```

---

## ๐ ุฅุญุตุงุฆูุงุช

| ุงููุฆุฉ | ุนุฏุฏ ุงูุญุฒู |
|-------|-----------|
| Python | ~50 |
| Haskell (pandoc) | ~200 |
| System | ~30 |
| Desktop | ~20 |
| Development | ~15 |

---

## ๐ก ูุตุงุฆุญ

1. **ููุชุซุจูุช ุงูุฌูุงุนู:**
```bash
sudo pacman -U *.pkg.tar.zst
```

2. **ูุฅุตูุงุญ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงูููููุฉ:**
```bash
sudo rm /var/lib/pacman/db.lck
```

3. **ููุชุซุจูุช ูู AUR:**
```bash
yay -S package-name
```

---

> ููุงุญุธุฉ: ูุฐู ุงููุงุฆูุฉ ูู ูุณุฎุฉ ุงุญุชูุงุทูุฉ ุจุชุงุฑูุฎ 2026-02-03. ุชุฃูุฏ ูู ุงูุชุญูู ูู ูุฌูุฏ ุฅุตุฏุงุฑุงุช ุฃุญุฏุซ ูุจู ุงูุชุซุจูุช.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/linux/vpn-tools.md
----------------------------------------
# ุฃุฏูุงุช VPN ุนูู ููููุณ | VPN Tools on Linux

## ููุฎุต
ุฏููู ุดุงูู ูุชุดุบูู ุฃุฏูุงุช ุชุฌุงูุฒ ุงูุญุฌุจ ุนูู ููููุณุ ูุน ุงูุชุฑููุฒ ุนูู Psiphon ูุจุฏุงุฆูู.

---

## ๐ซ ูุดููุฉ Psiphon ุนูู Linux

### ููุงุฐุง ูุง ูุนูู Psiphon ุนุจุฑ Wineุ

| ุงููุดููุฉ | ุงูุชูุงุตูู |
|---------|----------|
| **.NET Framework** | Wine ูุง ูุฏุนู .NET 4.8 ุจุดูู ูุงูู |
| **ุฎุฏูุงุช Windows** | Psiphon ูุนุชูุฏ ุนูู ุฎุฏูุงุช ุฎูููุฉ ูุง ูุฌูุฏ ููุง ูู Wine |
| **ุดุจูุฉ ูุชูุฏูุฉ** | ุชูููุงุช TAP/TUN ูVPN ูุง ุชุนูู ุนุจุฑ ุทุจูุฉ ุงูุชูุงูู |
| **ุจุฑุงูุฌ ุชุดุบูู ุงูุดุจูุฉ** | ูุญุงููุฉ ุชุซุจูุช ูุญูู ุดุจูุฉ ุงูุชุฑุงุถู ุนูู ูุณุชูู ููุงุฉ ุงููุธุงู |

---

## โ ุงูุญููู ุงูุนูููุฉ

### 1. ูุณุฎุฉ Linux ุงูุฑุณููุฉ (ููุตู ุจูุง)

Psiphon ูููุฑ ูุณุฎุฉ ุณุทุฑ ุฃูุงูุฑ ูููููุณ ุฌุงูุฒุฉ:

```bash
cd ~/Downloads
wget https://github.com/Psiphon-Labs/psiphon-tunnel-core-binaries/raw/master/psiphon-tunnel-core-x86_64 -O psiphon-linux
chmod +x psiphon-linux
./psiphon-linux
```

ุนูุฏ ุงูุชุดุบูู ุงููุงุฌุญ:
```
Started SOCKS proxy on 127.0.0.1:1080
```

### 2. Tor Browser (ุงูุฃูุซุฑ ุงุณุชูุฑุงุฑุงู)

```bash
sudo pacman -S tor tor-browser
tor-browser &
```

**ุงููููุฒุงุช:**
- ูุณุชูุฑ ุฌุฏุงู ุนูู Linux
- ูุง ูุญุชุงุฌ ุฅุนุฏุงุฏุงุช ูุนูุฏุฉ
- ุฏุนู ุฑุณูู ูู ูุดุฑูุน Tor

### 3. Outline Client

```bash
wget https://github.com/Jigsaw-Code/outline-client/releases/download/client-v1.19.0/Outline-Client.AppImage -O ~/Downloads/Outline.AppImage
chmod +x ~/Downloads/Outline.AppImage
~/Downloads/Outline.AppImage
```

**ุงููููุฒุงุช:**
- ุณุฑูุน ูุณูู
- ูู ุฌูุฌู (Jigsaw)
- ูุงุฌูุฉ ุฑุณูููุฉ ุจุณูุทุฉ

### 4. Proton VPN

```bash
yay -S protonvpn-cli
protonvpn-cli login
protonvpn-cli connect
```

**ุงููููุฒุงุช:**
- ูุฌุงูู ูุน 3 ุฎูุงุฏู
- ุฎูุงุฏู ูู ุนุฏุฉ ุฏูู
- ูุง ูุญุชุงุฌ ุชุณุฌูู ูุนูุฏ

---

## ๐ง ุชุซุจูุช Outline ุนูู Arch Linux

### ุงูุทุฑููุฉ 1: AppImage

```bash
cd ~/Downloads
wget https://github.com/Jigsaw-Code/outline-client/releases/download/v1.15.0/Outline-Client.AppImage
chmod +x Outline-Client.AppImage
./Outline-Client.AppImage
```

### ุงูุทุฑููุฉ 2: ูู AUR

```bash
yay -S outline-client-appimage
```

### ุงูุทุฑููุฉ 3: Flatpak

```bash
flatpak install flathub org.jigsaw.Outline
flatpak run org.jigsaw.Outline
```

---

## ๐ ุงุณุชูุดุงู ุงูุฃุฎุทุงุก ูุฅุตูุงุญูุง

### ูุดููุฉ: Outline ูุง ูุบูุฑ ุงููููุน

**ุงูุญููู:**

1. ุชุญูู ูู ุงูุงุชุตุงู:
```bash
curl --socks5 127.0.0.1:1080 ifconfig.me
```

2. ุฅุนุงุฏุฉ ุชุดุบูู ุงูุฎุฏูุฉ:
```bash
systemctl --user restart outline-client
```

3. ุชุญูู ูู ุฅุนุฏุงุฏุงุช ุงููุธุงู:
```bash
gsettings get org.gnome.system.proxy mode
```

### ูุดููุฉ: ุงูุงุชุตุงู ุจุทูุก

**ุงูุญููู:**

1. ุชุบููุฑ ุงูุฎุงุฏู
2. ุงุณุชุฎุฏุงู WireGuard ุจุฏูุงู ูู Shadowsocks
3. ุงูุชุญูู ูู ุณุฑุนุฉ ุงูุฅูุชุฑูุช

---

## ๐ ููุงุฑูุฉ ุงูุญููู

| ุงูุญู | ุงูุณุฑุนุฉ | ุงูุงุณุชูุฑุงุฑ | ุงูุณูููุฉ | ุงูุชูุตูุฉ |
|------|--------|-----------|---------|---------|
| Tor Browser | โญโญโญ | โญโญโญโญโญ | โญโญโญโญโญ | ุงูุฃูุถู ูููุจุชุฏุฆูู |
| Outline Client | โญโญโญโญ | โญโญโญโญ | โญโญโญโญโญ | ููุชุงุฒ ููุณุฑุนุฉ |
| Psiphon Linux | โญโญโญ | โญโญโญโญ | โญโญโญ | ุฎูุงุฑ ุฌูุฏ |
| Proton VPN | โญโญโญโญ | โญโญโญโญ | โญโญโญโญ | ููุชุงุฒ ูุฌุงูุงู |
| Wine + Psiphon | โญ | โญ | โญ | โ ุบูุฑ ููุตู ุจู |

---

## โ๏ธ ุชุญุฐูุฑุงุช ูููุฉ

1. **ูุงููููุฉ**: ุชุฃูุฏ ูู ุงูุงูุชุฒุงู ุจููุงููู ุจูุฏู
2. **ุงูุฃูุงู**: ูุง ุชุฑุณู ุจูุงูุงุช ุญุณุงุณุฉ ุนุจุฑ ุงุชุตุงูุงุช ุบูุฑ ููุซููุฉ
3. **ุงูุงุณุชูุฑุงุฑ**: ุงุณุชุฎุฏู ุงูุญููู ุงูุฃูุซุฑ ุงุณุชูุฑุงุฑุงู ููุนูู ุงูููู

---

## ๐ก ูุตุงุฆุญ ุฅุถุงููุฉ

### ููุชุตูุญ ุงูุนุงู:
- Tor Browser ูุงูู

### ููุนูู ูุงูุณุฑุนุฉ:
- Outline ุฃู Proton VPN

### ููุฎุตูุตูุฉ ุงููุตูู:
- Tor + VPN (ูุชูุฏู)

---

> ๐ **ููุงุญุธุฉ**: ูุฐู ุงููุนูููุงุช ููุฃุบุฑุงุถ ุงูุชุนููููุฉ. ุงููุณุชุฎุฏู ูุณุคูู ุนู ุงูุงูุชุฒุงู ุจููุงููู ุจูุฏู.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/linux/wine-comparison.md
----------------------------------------
# ููุงุฑูุฉ Wine ู Bottles ู CrossOver

## ุงูููุฎุต

ุซูุงุซ ุฃุฏูุงุช ุฑุฆูุณูุฉ ูุชุดุบูู ุชุทุจููุงุช Windows ุนูู Linuxุ ูู ูููุง ูู ููุฌ ูุฎุชูู.

---

## ๐ ุงูููุงุฑูุฉ ุงูุดุงููุฉ

| ุงููุนูุงุฑ | Wine | Bottles | CrossOver |
|--------|------|---------|-----------|
| **ุงูููุน** | ูุดุฑูุน ููุชูุญ ุงููุตุฏุฑ (ุฃุณุงุณู) | ูุงุฌูุฉ ุฑุณูููุฉ ูุฌุงููุฉ ูุจููุฉ ุนูู Wine | ูุณุฎุฉ ุชุฌุงุฑูุฉ ูุฏููุนุฉ ูุจููุฉ ุนูู Wine |
| **ุงูุณุนุฑ** | ูุฌุงูู ุจุงููุงูู | ูุฌุงูู | ูุฏููุน (~74โฌุ ูุน ุชุฌุฑุจุฉ 14 ููููุง) |
| **ุงูุณูููุฉ** | ูุชุทูุจ ูุนุฑูุฉ ุชูููุฉ (ุณุทุฑ ุฃูุงูุฑุ ุถุจุท ูุฏูู) | ุณูู ูุณุจููุง (ูุงุฌูุฉ ุฑุณูููุฉุ "ุฒุฌุงุฌุงุช" ูุนุฒููุฉ) | ุณูู ุฌุฏูุง (ุชุซุจูุช ุจููุฑุฉ ูุงุญุฏุฉุ ุฅุนุฏุงุฏุงุช ุชููุงุฆูุฉ) |
| **ุงูุฏุนู** | ูุฌุชูุนู (ููุชุฏูุงุชุ ูููู) | ูุฌุชูุนู | ุฏุนู ููู ูุจุงุดุฑ ูู ุดุฑูุฉ CodeWeavers |
| **ุงูุชูุงูู** | ูุนุชูุฏ ุนูู ุงููุณุชุฎุฏู (ูุญุชุงุฌ ุถุจุท) | ุฃูุถู ูู Wine ุงูุฎุงูุ ููู ุบูุฑ ูุถููู | ูุถููู ูุชุทุจููุงุช ูุญุฏุฏุฉ (ูุฎุชุจุฑุฉ ูุณุจููุง) |
| **ุงูุชุญุฏูุซุงุช** | ุฏูุฑูุฉ ูู ุงููุฌุชูุน | ุฏูุฑูุฉ ูู ูุทูุฑู Bottles | ุณุฑูุนุฉ ูุน ุฅุตูุงุญุงุช ุฎุงุตุฉ ุบูุฑ ููุฌูุฏุฉ ูู Wine |

---

## ๐ง Wine (ุงูุฃุณุงุณ)

**ุงููููุฒุงุช:**
- ูุดุฑูุน ููุชูุญ ุงููุตุฏุฑ ููุฐ 30+ ุณูุฉ
- ูุญูู ุงุณุชุฏุนุงุกุงุช Windows API ุฅูู ุงุณุชุฏุนุงุกุงุช Linux/macOS
- ูุฌุงูู ุจุงููุงูู

**ุงูุนููุจ:**
- ูุชุทูุจ ูุนุฑูุฉ ุชูููุฉ ุฌูุฏุฉ ููุชููุฆุฉ
- ุงูุญุฏ ุงูุฃุฏูู ูู ุงูุฃุฏูุงุช ุงููุณุงุนุฏุฉ ููุชููุฆุฉ
- ูุง ููุฌุฏ ุฏุนู ููู ุฑุณูู

---

## ๐พ Bottles (ูุงุฌูุฉ ูุฌุงููุฉ)

**ุงููููุฒุงุช:**
- ูุงุฌูุฉ ูุณุชุฎุฏู ูุฌุงููุฉ ูุจููุฉ ุนูู Wine
- ุณูููุฉ ุฅูุดุงุก ูุฅุฏุงุฑุฉ "ุฒุฌุงุฌุงุช" (ุจูุฆุงุช ูุนุฒููุฉ ููู ุชุทุจูู)
- ุชุซุจูุช ุงูุงุนุชูุงุฏูุงุช ูุงูุชุดุบูู ุจุถุบุทุฉ ุฒุฑ
- ุฃูุซุฑ ุณูููุฉ ูู Wine ุงูุฎุงู

**ุงูุนููุจ:**
- ุฃูู ุงูุชูุงูุงู ูู CrossOver
- ุฏุนู ูุฌุชูุนู ููุท
- ุงูุชูุงูู ุบูุฑ ูุถููู 100%

---

## ๐ผ CrossOver (ุงูุชุฌุงุฑู)

**ุงููููุฒุงุช:**
- ูุณุฎุฉ ุชุฌุงุฑูุฉ ูุฏููุนุฉ ูู ุดุฑูุฉ CodeWeavers
- ูุงุฌูุฉ ูุณุชุฎุฏู ุฑุณูููุฉ ุณููุฉ ุฌุฏุงู
- ุชุซุจูุช ุชููุงุฆู ููุฅุนุฏุงุฏุงุช ุงููุซูู ููู ุชุทุจูู
- ุฏุนู ููู ูุจุงุดุฑ
- ุชุฑููุงุช ูุถูููุฉ ูุงุฎุชุจุงุฑ ููุซู
- ุฅุตูุงุญุงุช ูููุญูุงุช ุฎุงุตุฉ ุบูุฑ ูุชููุฑุฉ ูู Wine ุงูุฃุณุงุณู
- ุถูุงู ุนูู ุชุทุจููุงุช ูุญุฏุฏุฉ (ูุซู Microsoft Office)

**ุงูุนููุจ:**
- ูุฏููุน (~74โฌ)
- ุจุนุถ ุงูุชุญุณููุงุช ูุบููุฉ ุงููุตุฏุฑ

---

## โ ูู ูููู ุชุทููุฑ Wine/Bottles ููุตุจุญุง ูุซู CrossOverุ

### ูุง ูููู ุชุทููุฑู:
- ูุงุฌูุฉ ุงููุณุชุฎุฏู - ูููู ุฌุนููุง ุฃูุซุฑ ุณูููุฉ
- ุฃูุธูุฉ ุงูุชุซุจูุช ุงูุชููุงุฆู - ูููู ุชุทููุฑูุง
- ุฅุฏุงุฑุฉ ุงูููุชุจุงุช - ูููู ุชุญุณูููุง
- ุงูุนุฒู ูุงูุชูุณูู - ูููู ุชุนุฒูุฒู

### ุงูุชุญุฏูุงุช ูุงูุนูุจุงุช:
- **ุงูุชูููู**: CrossOver ุชุฏุนููุง ุดุฑูุฉ ุจุฑุจุญูุฉ ุชุฏูุน ููุทูุฑูู ุจุฏูุงู ูุงูู
- **ุงูุงุฎุชุจุงุฑ ุงูููุซู**: ูุชุทูุจ ููุงุฑุฏ ุจุดุฑูุฉ ููุงุฏูุฉ ูุจูุฑุฉ
- **ุงูุฏุนู ุงูููู**: ูุญุชุงุฌ ูุฑููุงู ูุฎุตุตุงู
- **ุงูุชูุงูู ุงููุถููู**: ุดุฑูุงุช ุชุญุชุงุฌ ุถูุงูุงุช ูุนูู ุจุฑุงูุฌูุง ุงูุชุฌุงุฑูุฉ
- **ุงูุชุฑุฎูุต**: ุจุนุถ ุงูุชูููุงุช ูุฏ ุชุชุทูุจ ุชุฑุงุฎูุต ุฎุงุตุฉ

---

## ๐ฏ ุงูุชูุตูุงุช ุงูุนูููุฉ

### ุงุฎุชุฑ CrossOver ุฅุฐุง:
- ุชุญุชุงุฌ ุชุดุบูู ุชุทุจููุงุช ุญุณุงุณุฉ (ูุซู Officeุ Photoshop) ุจุฏูู ูุชุงุนุจ
- ูุณุชุนุฏ ูุฏูุน ููุงุจู ูุชุฌุฑุจุฉ "ุฌุงูุฒุฉ ููุนูู"
- ุชุญุชุงุฌ ุฏุนูุงู ูููุงู ูุถูููุงู

### ุงุฎุชุฑ Bottles ุฅุฐุง:
- ุชุฑูุฏ ุญูุงู ูุฌุงููุงู ูุณูู ุงูุงุณุชุฎุฏุงู ูุณุจูุงู
- ุชุชุญูู ุจุนุถ ุงูุถุจุท ุงููุฏูู ุนูุฏ ููุงุฌูุฉ ูุดุงูู
- ุชุญุชุงุฌ ูุฑููุฉ ูู ุงูุชุฎุตูุต

### ุงุฎุชุฑ Wine ุงููุจุงุดุฑ ุฅุฐุง:
- ูุฏูู ุฎุจุฑุฉ ุชูููุฉ ูุชุฑูุฏ ุชุญููุงู ูุงููุงู ุจุงูุฅุนุฏุงุฏุงุช
- ุชุญุชุงุฌ ุฃูุตู ุฃุฏุงุก ูููู
- ุชุณุชูุชุน ุจุงูุชุนูู ูุงูุชุฌุฑุจุฉ

---

## ๐ก ูุตุงุฆุญ ูุชุญุณูู Wine/Bottles

### 1. ุชุทููุฑุงุช ููุงุฌูุฉ ุงููุณุชุฎุฏู:
- ูุนุงูุฌ ุชุซุจูุช ุฐูู ููุญุต ููู ุงูุชุซุจูุช ูููุชุฑุญ ุงูุฅุนุฏุงุฏุงุช ุงููุซูู
- ูุงุนุฏุฉ ุจูุงูุงุช ุงูุชุทุจููุงุช ูุน ุชููููุงุช ุงููุณุชุฎุฏููู
- ูุธุงู ุงูุงุณุชุฑุฌุงุน (Restore Points)

### 2. ุชุญุณููุงุช ุงูุชูุงูู:
- ูุฏูุฑ ุงุนุชูุงุฏูุงุช ุฐูู ููุชุดู ุงูุชุทุจููุงุช ุงูููููุฏุฉ
- ููุชุจุฉ DLLs ูุฑูุฒูุฉ ูุน ุฎูุงุฑุงุช ุงููุณุฎ ุงููุธููุฉ
- ุชุญุณููุงุช ุงูุฑุณููุงุช (DXVK/VKD3D-Proton)

### 3. ูุธุงู ุงูุฏุนู:
- ุชูุซูู ุชูุงุนูู ูุน ุฃูุซูุฉ ุนูููุฉ
- ูุธุงู ุงูุฏุนู ุงูุฌูุงุนู ุจูุธุงู ููุงุท
- ุดูุงุฏุฉ ุงูุชูุงูู ููุชุทุจููุงุช ุงููุฌุฑุจุฉ

---

## ๐ ุฎูุงุตุฉ

| ุงูุญุงุฌุฉ | ุงูุฃุฏุงุฉ ุงูููุตู ุจูุง |
|--------|-------------------|
| ุณูููุฉ ูุทููุฉ | CrossOver |
| ูุฌุงูู + ุณูู | Bottles |
| ุชุญูู ูุงูู | Wine |
| ุฃูุนุงุจ | Proton/Bottles |

**ุงูุฎูุงุตุฉ**: CrossOver ููุฏู ุชุฌุฑุจุฉ "ุฌุงูุฒุฉ ููุนูู" ูุฏุนููุฉ ุชุฌุงุฑูุงูุ ุจูููุง Wine/Bottles ููุฏูุงู ูุฑููุฉ ุฃูุจุฑ ูุฌุงูุงู ูููู ูุน ุฌูุฏ ุฃูุจุฑ ูู ุงููุณุชุฎุฏู.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/packages_raw_list.txt
----------------------------------------
opensubdiv
opentimelineio
openucx
openvdb
openvpn
opus
orc
orca
orchis-theme
ostree
p11-kit
pacman
pacman-mirrorlist
pam
pandoc-cli
pangomm
papers
papirus-folders
papirus-icon-theme
patchelf
patool
pipewire
pipewire-alsa
pipewire-audio
pipewire-jack
pipewire-pulse
pipewire-v4l2
pipewire-x11-bell
pipewire-zeroconf
plank
plasma-integration
plasma-wayland-protocols
plasma5-themes-sweet-full-git-r441.8772f6d
plasma5-themes-sweet-full-git-r442.9450c5c
plasma6-applets-panel-colorizer
plasma6-themes-layan-git-r99.a0b6a49
polkit-qt5
poppler-glib
potrace
powerline
powerline-fonts
procps-ng
protontricks-git
proxychains-ng
prrte
ptex
pulse-native-provider
pyalpm
pybind11
pyside6
pystring
python
python-aaf2
python-accessible-pygments
python-agate
python-agate-dbf
python-agate-excel
python-agate-sql
python-annotated-types
python-appdirs
python-babel
python-boolean.py
python-cachecontrol
python-cffi
python-cryptography
python-cssselect
python-dasbus
python-dbfread
python-defusedxml
python-distlib
python-docutils
python-et-xmlfile
python-filelock
python-gbinder
python-greenlet
python-html5lib
python-imagesize
python-iniconfig
python-isodate
python-jinja
python-leather
python-license-expression
python-lockfile
python-markdown
python-markupsafe
python-msgpack
python-numpy
python-olefile
python-opengl
python-openpyxl
python-packaging
python-parsedatetime
python-pip
python-pipx
python-pivy
python-pluggy
python-ply
python-psutil
python-pycparser
python-pydantic
python-pydantic-core
python-pyelftools
python-pygments
python-pyparsing
python-pyqt6
python-pyserial
python-pytest
python-pytimeparse
python-pytz
python-pyxdg
python-roman-numerals-py
python-setproctitle
python-shtab
python-slugify
python-snowballstemmer
python-soupsieve
python-sphinx
python-sphinx-alabaster-theme
python-sphinx-basic-ng
python-sphinx-furo
python-sphinx_rtd_theme
python-sphinxcontrib-applehelp
python-sphinxcontrib-devhelp
python-sphinxcontrib-htmlhelp
python-sphinxcontrib-jquery
python-sphinxcontrib-jsmath
python-sphinxcontrib-qthelp
python-sphinxcontrib-serializinghtml
python-sqlalchemy
python-steamgriddb
python-systemd
python-text-unidecode
python-tinycss2
python-tqdm
python-typing-inspection
python-userpath
python-virtualenv
python-webencodings
python-wheel
python-xlrd
python-yara
python-zstandard
qemu-audio-alsa
qemu-audio-dbus
qemu-audio-jack
qemu-audio-oss
qemu-audio-pa
qemu-audio-pipewire
qemu-audio-sdl
qemu-audio-spice
qemu-base
qemu-block-curl
qemu-block-dmg
qemu-block-nfs
qemu-block-ssh
qemu-chardev-spice
qemu-common
qemu-desktop
qemu-hw-display-qxl
qemu-hw-display-virtio-gpu
qemu-hw-display-virtio-gpu-gl
qemu-hw-display-virtio-gpu-pci
qemu-hw-display-virtio-gpu-pci-gl
qemu-hw-display-virtio-gpu-pci-rutabaga
qemu-hw-display-virtio-gpu-rutabaga
qemu-hw-display-virtio-vga
qemu-hw-display-virtio-vga-gl
qemu-hw-display-virtio-vga-rutabaga
qemu-hw-uefi-vars
qemu-hw-usb-host
qemu-hw-usb-redirect
qemu-hw-usb-smartcard
qemu-img
qemu-system-x86
qemu-system-x86-firmware
qemu-ui-curses
qemu-ui-dbus
qemu-ui-egl-headless
qemu-ui-gtk
qemu-ui-opengl
qemu-ui-sdl
qemu-ui-spice-app
qemu-ui-spice-core
qemu-vhost-user-gpu
qogir-gtk-theme-git
qpdf
qqwing
qt5-graphicaleffects
qt5-location
qt5-multimedia
qt5-quickcontrols
qt5-quickcontrols2
qt5-speech
qt5-tools
qt6
qt6-base
qt6-charts
qt6-connectivity
qt6-declarative
qt6-imageformats
qt6-location
qt6-multimedia
qt6-multimedia-ffmpeg
qt6-networkauth
qt6-positioning
qt6-quick3d
qt6-quicktimeline
qt6-scxml
qt6-sensors
qt6-shadertools
qt6-speech
qt6-svg
qt6-tools
qt6-translations
qt6-virtualkeyboard
qt6-wayland
qt6-webchannel
qt6-webengine
qt6-websockets
qt6-webview
quadrapassel
ragel
rasqal
rate-mirrors
rclone
rdma-core
redland
reflector
remmina
reshade-shaders-git-r152.5fe4a90
rhash
ruby
run-parts
runc
rust
rygel
sane
sassc
scour
sdl3
sdl3_ttf
semver
shiboken6
showtime
simdjson
simple-scan
smbclient
snapshot
sof-firmware
sofia-sip
solid5
sonnet5
soqt
speech-dispatcher
squashfs-tools
startup-notification
suitesparse
sushi
swell-foop
syndication5
sysprof
tcpdump
tdb
tecla
tela-icon-theme
telegram-desktop
template-glib
thunderbird
tk
totem-pl-parser
tree-sitter
tree-sitter-c
tree-sitter-lua
tree-sitter-markdown
tree-sitter-query
tree-sitter-vim
tree-sitter-vimdoc
trilium-next
ttf-carlito
ttf-dejavu
ttf-fira-code
ttf-firacode-nerd
ttf-jetbrains-mono
ttf-liberation
ttf-ms-fonts
ttf-ubuntu-font-family
ufw
unibilium
unifetch
unoconv
unrar
unzip
usbmuxd
usd
vala
virt-install
virt-manager
virtualbox
virtualbox-host-dkms
vte4
vtk
vulkan-intel
vulkan-mesa-implicit-layers
vulkan-mesa-layers
vulkan-radeon
vulkan-swrast
wavpack
waydroid
webkitgtk
wget
wine-mono
wine-staging
winetricks
wireless_tools
wps-office
wps-office-cn
wps-office-fonts
wsdd
xdg-desktop-portal
xdg-desktop-portal-gnome
xdg-desktop-portal-kde
xdg-user-dirs-gtk
xerces-c
xfsprogs
xorg-mkfontscale
xournalpp
yaml-cpp
yara
yay
yelp
yelp-xsl
zsh
zsh-autosuggestions
zsh-completions
zsh-syntax-highlighting
zsync

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/vpn-tools.md
----------------------------------------
# ุฃุฏูุงุช VPN ูุชุฌุงูุฒ ุงูุญุฌุจ ุนูู Linux

## ูุธุฑุฉ ุนุงูุฉ

| ุงูุฃุฏุงุฉ | ุงูููุน | ุงูุงุณุชูุฑุงุฑ | ุงูุณุฑุนุฉ | ุงูุณูููุฉ |
|--------|------|-----------|--------|---------|
| **Tor Browser** | ุดุจูุฉ ุชูุฌูู | โญโญโญโญโญ | โญโญ | โญโญโญโญโญ |
| **Outline Client** | Shadowsocks | โญโญโญโญ | โญโญโญโญ | โญโญโญโญโญ |
| **Proton VPN** | VPN ุชุฌุงุฑู | โญโญโญโญ | โญโญโญ | โญโญโญโญ |
| **Psiphon** | ูุชุนุฏุฏ ุงูุจุฑูุชููููุงุช | โญโญ | โญโญ | โญโญโญ |

## Psiphon ุนูู Linux

### ุงููุดููุฉ
Psiphon ูุตูู ูู Windows ููุง ูุนูู ุจุดูู ูุงูู ุนุจุฑ Wine ุจุณุจุจ:
- ูุญุชุงุฌ ูุญูู ุดุจูุฉ TAP/TUN ุนูู ูุณุชูู ุงูููุงุฉ
- Wine ูุง ูุญุงูู ุจุฑุงูุฌ ุชุดุบูู ุงูุดุจูุฉ

### ุงูุญููู ุงููุชุงุญุฉ

#### 1. ูุณุฎุฉ Linux ุงูุฑุณููุฉ (ุณุทุฑ ุฃูุงูุฑ)
```bash
# ุชูุฒูู ุงููุณุฎุฉ ุงูุฌุงูุฒุฉ
cd ~/Downloads
wget https://github.com/Psiphon-Labs/psiphon-tunnel-core-binaries/raw/master/psiphon-tunnel-core-x86_64 -O psiphon-linux
chmod +x psiphon-linux

# ุงูุชุดุบูู
./psiphon-linux
```

#### 2. ุขูุฉ ุงูุชุฑุงุถูุฉ ูุน Windows
```bash
# ุชุซุจูุช VirtualBox
sudo pacman -S virtualbox virtualbox-host-dkms virtualbox-guest-iso
sudo modprobe vboxdrv
sudo usermod -aG vboxusers $USER

# ุฅูุดุงุก ุขูุฉ ุงูุชุฑุงุถูุฉ
VBoxManage createvm --name "Windows-Psiphon" --ostype "Windows10_64" --register
VBoxManage modifyvm "Windows-Psiphon" --memory 2048 --cpus 2
```

## Tor Browser (ุงูุฃูุซุฑ ุงุณุชูุฑุงุฑุงู)

### ุงูุชุซุจูุช ุนูู Garuda
```bash
sudo pacman -S tor tor-browser

# ุงูุชุดุบูู
tor-browser &
```

### ุงููููุฒุงุช
- ุงุณุชูุฑุงุฑ ุนุงูู ุฌุฏุงู
- ุฎุตูุตูุฉ ูุตูู
- ุณูู ุงูุงุณุชุฎุฏุงู
- ูุฏูุฌ ูู ูุนุธู ุชูุฒูุนุงุช Linux

### ุงูุนููุจ
- ุจุทูุก ูุณุจูุงู
- ุจุนุถ ุงูููุงูุน ุชุญุฌุจู

## Outline Client (ูู Google)

### ุงูุชุซุจูุช
```bash
# ุชูุฒูู AppImage
wget https://github.com/Jigsaw-Code/outline-client/releases/download/client-v1.19.0/Outline-Client.AppImage -O ~/Downloads/Outline.AppImage
chmod +x ~/Downloads/Outline.AppImage

# ุงูุชุดุบูู
~/Downloads/Outline.AppImage
```

### ุงููููุฒุงุช
- ุณุฑูุน
- ุณูู ุงูุฅุนุฏุงุฏ
- ูุงุฌูุฉ ุฑุณูููุฉ

## Proton VPN

### ุงูุชุซุจูุช
```bash
# ุนุจุฑ AUR
yay -S protonvpn-cli

# ุงูุฅุนุฏุงุฏ
protonvpn-cli login
protonvpn-cli connect
```

### ุงููููุฒุงุช
- ุงูุฎุทุฉ ุงููุฌุงููุฉ: 3 ุฎูุงุฏู
- ุชุดููุฑ ููู
- ุณูุงุณุฉ ุนุฏู ุงูุงุญุชูุงุธ ุจุงูุณุฌูุงุช

## ุงุณุชุฎุฏุงู proxychains ูุน Tor

```bash
# ุงูุชุซุจูุช
sudo pacman -S proxychains-ng

# ุงูุฅุนุฏุงุฏ
echo "socks5 127.0.0.1 9050" | sudo tee /etc/proxychains.conf

# ุงูุงุณุชุฎุฏุงู
proxychains firefox
proxychains curl https://example.com
```

## ููุงุฑูุฉ ุงูุญููู

| ุงูุญู | ุงูุณุฑุนุฉ | ุงูุงุณุชูุฑุงุฑ | ุงูุณูููุฉ | ุงูุชูุตูุฉ |
|------|--------|-----------|---------|---------|
| Tor Browser | โญโญ | โญโญโญโญโญ | โญโญโญโญโญ | โ ุงูุฃูุถู ูููุจุชุฏุฆูู |
| Outline Client | โญโญโญโญ | โญโญโญโญ | โญโญโญโญโญ | โ ููุชุงุฒ ูุณุฑูุน |
| ุขูุฉ ุงูุชุฑุงุถูุฉ + Psiphon | โญโญ | โญโญโญโญ | โญโญ | โ๏ธ ุฅุฐุง ููุช ูุตุฑุงู ุนูู Psiphon |
| Wine/CrossOver + Psiphon | โญ | โญ | โญ | โ ุบูุฑ ููุตู ุจู |

## ุชุญุฐูุฑ ูุงูููู

> โ๏ธ ุงุณุชุฎุฏุงู ุฃุฏูุงุช ุชุฌุงูุฒ ุงูุฑูุงุจุฉ ูุฏ ูุฎุงูู ุงูููุงููู ุงููุญููุฉ ูู ุจุนุถ ุงูุฏูู. ุชุฃูุฏ ูู ุงูุงูุชุฒุงู ุจููุงููู ุจูุฏู ูุจู ุงุณุชุฎุฏุงู ุฃู ุฃุฏุงุฉ ูู ูุฐุง ุงูููุน.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/technical/wine-comparison.md
----------------------------------------
# ููุงุฑูุฉ ุฃุฏูุงุช ุชุดุบูู ุชุทุจููุงุช Windows ุนูู Linux

## ูุธุฑุฉ ุนุงูุฉ

| ุงูุฃุฏุงุฉ | ุงูููุน | ุงูุณุนุฑ | ุงูุณูููุฉ | ุงูุฏุนู |
|--------|------|-------|---------|-------|
| **Wine** | ูุดุฑูุน ููุชูุญ ุงููุตุฏุฑ (ุฃุณุงุณู) | ูุฌุงูู | โญโญ | ูุฌุชูุนู |
| **Bottles** | ูุงุฌูุฉ ุฑุณูููุฉ ูุจููุฉ ุนูู Wine | ูุฌุงูู | โญโญโญโญ | ูุฌุชูุนู |
| **CrossOver** | ูุณุฎุฉ ุชุฌุงุฑูุฉ ูุจููุฉ ุนูู Wine | ูุฏููุน (~74โฌ) | โญโญโญโญโญ | ุฏุนู ููู ูุจุงุดุฑ |

## Wine (ุงูุฃุณุงุณ)

### ุงููุตู
ูุดุฑูุน ููุชูุญ ุงููุตุฏุฑ ูุนูู ูุทุจูุฉ ุชูุงูู ูุญูู ุงุณุชุฏุนุงุกุงุช Windows API ุฅูู ุงุณุชุฏุนุงุกุงุช Linux/macOS.

### ุงููููุฒุงุช
- ูุฌุงูู ุจุงููุงูู
- ุชุญุฏูุซุงุช ูุณุชูุฑุฉ ูู ุงููุฌุชูุน
- ูุฑููุฉ ุนุงููุฉ ูู ุงูุชุฎุตูุต
- ูุฏุนู ุขูุงู ุงูุชุทุจููุงุช

### ุงูุนููุจ
- ูุชุทูุจ ูุนุฑูุฉ ุชูููุฉ ุฌูุฏุฉ
- ุถุจุท ูุฏูู ููุฅุนุฏุงุฏุงุช
- ูุง ููุฌุฏ ุฏุนู ููู ูุจุงุดุฑ

## Bottles (ุงููุงุฌูุฉ ุงููุฌุงููุฉ)

### ุงููุตู
ูุงุฌูุฉ ุฑุณูููุฉ ูุฌุงููุฉ ูุจููุฉ ุนูู Wine ุชุณูู ุฅูุดุงุก ูุฅุฏุงุฑุฉ "ุฒุฌุงุฌุงุช" (ุจูุฆุงุช ูุนุฒููุฉ ููู ุชุทุจูู).

### ุงููููุฒุงุช
- ุณูููุฉ ุงูุงุณุชุฎุฏุงู ุงููุณุจูุฉ
- ุนุฒู ุงูุชุทุจููุงุช ูู ุจูุฆุงุช ูููุตูุฉ
- ุชุซุจูุช ุงูุงุนุชูุงุฏูุงุช ุจุถุบุทุฉ ุฒุฑ
- ุฏุนู DXVK ู VKD3D ูุฏูุฌ

### ุงูุนููุจ
- ุฃูู ุงูุชูุงูุงู ูู CrossOver
- ุฏุนู ูุฌุชูุนู ููุท
- ุงูุชูุงูู ุบูุฑ ูุถููู 100%

## CrossOver (ุงูุญู ุงูุชุฌุงุฑู)

### ุงููุตู
ูุณุฎุฉ ุชุฌุงุฑูุฉ ูุฏููุนุฉ ูู Wine ุทูุฑุชูุง ุดุฑูุฉ CodeWeavers.

### ุงููููุฒุงุช
- ูุงุฌูุฉ ูุณุชุฎุฏู ุณููุฉ ุฌุฏุงู
- ุชุซุจูุช ุชููุงุฆู ููุฅุนุฏุงุฏุงุช ุงููุซูู ููู ุชุทุจูู
- ุฏุนู ููู ูุจุงุดุฑ
- ุชุฑููุงุช ูุถูููุฉ ูุงุฎุชุจุงุฑ ููุซู
- ุฅุตูุงุญุงุช ูููุญูุงุช ุฎุงุตุฉ ุบูุฑ ูุชููุฑุฉ ูู Wine
- ุถูุงู ุนูู ุชุทุจููุงุช ูุญุฏุฏุฉ (Microsoft Office, Adobe)

### ุงูุนููุจ
- ูุฏููุน (~74โฌ)
- ุจุนุถ ุงูุชุญุณููุงุช ูุบููุฉ ุงููุตุฏุฑ

## ูู ูููู ุชุทููุฑ Wine/Bottles ููุตุจุญุง ูุซู CrossOverุ

### ูุง ูููู ุชุทููุฑู
- โ ูุงุฌูุฉ ุงููุณุชุฎุฏู
- โ ุฃูุธูุฉ ุงูุชุซุจูุช ุงูุชููุงุฆู
- โ ุฅุฏุงุฑุฉ ุงูููุชุจุงุช
- โ ุงูุนุฒู ูุงูุชูุณูู

### ุงูุชุญุฏูุงุช
- โ ุงูุชูุงูู ุงููุถููู ูุชุทูุจ ููุงุฑุฏ ุจุดุฑูุฉ ููุงุฏูุฉ ุถุฎูุฉ
- โ ุงูุฏุนู ุงููุจุงุดุฑ ูุญุชุงุฌ ูุฑูู ูุฏููุน ุงูุฃุฌุฑ
- โ ุจุนุถ ุงูุฅุตูุงุญุงุช ูุบููุฉ ุงููุตุฏุฑ

## ุชูุตูุฉ ุนูููุฉ

| ุงูุญุงูุฉ | ุงูุชูุตูุฉ |
|--------|---------|
| ุชุญุชุงุฌ ุชุดุบูู ุชุทุจููุงุช ุญุณุงุณุฉ ุจุฏูู ูุชุงุนุจ | โ CrossOver |
| ุชุฑูุฏ ุญูุงู ูุฌุงููุงู ูุณูู ูุณุจูุงู | โ Bottles |
| ูุฏูู ุฎุจุฑุฉ ุชูููุฉ ูุชุฑูุฏ ุชุญูู ูุงูู | โ Wine ุงููุจุงุดุฑ |

## ุชุซุจูุช ุนูู Garuda Linux

```bash
# ุชุซุจูุช Wine
sudo pacman -S wine wine-gecko wine-mono

# ุชุซุจูุช Bottles
sudo pacman -S bottles

# ุฃู ูู Flatpak
flatpak install flathub com.usebottles.bottles

# ุชุซุจูุช CrossOver (ูุฏููุน)
yay -S crossover
```

## ุงูุจุฏุงุฆู ุงููุฌุงููุฉ ุงููุญุณูุฉ

```bash
# Wine-GE (ูุญุณู ููุฃูุนุงุจ)
yay -S wine-ge-custom

# Proton (ูู Valve ููุฃูุนุงุจ)
# ูุชููุฑ ุนุจุฑ Steam

# Lutris (ูุฏูุฑ ุฃูุนุงุจ ูุชุทุจููุงุช)
sudo pacman -S lutris
```

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation/extracted/PROCESSING_REPORT.md
----------------------------------------
# ๐ ุชูุฑูุฑ ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช

**ุงูุชุงุฑูุฎ:** 2026-02-16 22:05:31

## ๐ ุงูุฅุญุตุงุฆูุงุช

| ุงูุจูุฏ | ุงูุนุฏุฏ |
|-------|-------|
| ุงููููุงุช ุงููุนุงูุฌุฉ | 20 |
| ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ ุงููุณุชุฎุฑุฌุฉ | 69 |
| ุงูุชูุฑุงุฑุงุช ุงููุฒุงูุฉ | 230 |
| ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ | 0 |

## ๐ ุงูุชุตููู

| ุงููุฆุฉ | ุงูุนุฏุฏ | ุงููุณุจุฉ |
|-------|-------|--------|
| ๐ misc | 31 | 44.9% |
| ๐ป technical | 16 | 23.2% |
| ๐ฅ medical | 11 | 15.9% |
| ๐ translation | 7 | 10.1% |
| ๐ reference | 4 | 5.8% |

## ๐ ุงููููุงุช ุงููุงุชุฌุฉ

| ุงูููู | ุงููุตู |
|-------|-------|
| translations_unified.csv | ุฌููุน ุงูุชุฑุฌูุงุช ูู ููู CSV |
| translations_training.tsv | ุชูุณูู TSV ููุชุฏุฑูุจ |
| translations_huggingface.jsonl | ุชูุณูู JSONL ูู HuggingFace |

## โ ุฃูุซูุฉ ุนูู ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ

| English | Arabic | Category |
|---------|--------|----------|
| English... | ุงููุต ุงูุฅูุฌููุฒู... | translation |
| Needs Correction... | ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ... | misc |
| Source... | ูุตุฏุฑ ุงูุชุฑุฌูุฉ... | reference |
| MIT License... | ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.... | reference |
| subcategory... | ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ... | misc |
| summary... | ููุฎุต ููุฌุฒ ูููุต... | reference |
| Book... | ูุชุงุจ... | misc |
| Computer... | ุญุงุณูุจ... | misc |
| Programming... | ุจุฑูุฌุฉ... | technical |
| Medical... | ุทุจู... | medical |

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation/extracted/translations_huggingface.jsonl
----------------------------------------
{"translation": {"en": "English", "ar": "ุงููุต ุงูุฅูุฌููุฒู"}, "category": "translation", "confidence": 0.1}
{"translation": {"en": "Needs Correction", "ar": "ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Source", "ar": "ูุตุฏุฑ ุงูุชุฑุฌูุฉ"}, "category": "reference", "confidence": 0.07}
{"translation": {"en": "MIT License", "ar": "ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ."}, "category": "reference", "confidence": 0.04}
{"translation": {"en": "subcategory", "ar": "ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "summary", "ar": "ููุฎุต ููุฌุฒ ูููุต"}, "category": "reference", "confidence": 0.07}
{"translation": {"en": "Book", "ar": "ูุชุงุจ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Computer", "ar": "ุญุงุณูุจ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Programming", "ar": "ุจุฑูุฌุฉ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "Medical", "ar": "ุทุจู"}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "Treatment", "ar": "ุนูุงุฌ"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "Disease", "ar": "ูุฑุถ"}, "category": "medical", "confidence": 0.02}
{"translation": {"en": "Hospital", "ar": "ูุณุชุดูู"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "Doctor", "ar": "ุทุจูุจ"}, "category": "medical", "confidence": 0.08}
{"translation": {"en": "Software", "ar": "ุจุฑูุงูุฌ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "Development", "ar": "ุชุทููุฑ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "summary", "ar": "ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู..."}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "API", "ar": "ุงูุชูููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "summary", "ar": "ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู..."}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "category", "ar": "ุทุจู"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "language", "ar": "ุงูุนุฑุจูุฉ"}, "category": "translation", "confidence": 0.07}
{"translation": {"en": "quality", "ar": "ุนุงููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Combined Text Splitter", "ar": "ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ"}, "category": "translation", "confidence": 0.07}
{"translation": {"en": "technical", "ar": "ุชููู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "translation", "ar": "ุชุฑุฌูุฉ"}, "category": "translation", "confidence": 0.07}
{"translation": {"en": "misc", "ar": "ูุชูุฑูุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Knowledge Processor", "ar": "ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "pacman", "ar": "ูุฏูุฑ ุงูุญุฒู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "pacman-mirrorlist", "ar": "ูุงุฆูุฉ ุงููุฑุงูุง"}, "category": "reference", "confidence": 0.07}
{"translation": {"en": "pam", "ar": "ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "procps-ng", "ar": "ุฃุฏูุงุช ุงูุนูููุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "reflector", "ar": "ุชุญุฏูุซ ุงููุฑุงูุง"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "pipewire", "ar": "ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "networkmanager", "ar": "ูุฏูุฑ ุงูุดุจูุฉ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "wget", "ar": "ุชูุฒูู ุงููููุงุช"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "rclone", "ar": "ูุฒุงููุฉ ุงูุณุญุงุจุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "tcpdump", "ar": "ุชุญููู ุงูุดุจูุฉ"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "ufw", "ar": "ุฌุฏุงุฑ ุญูุงูุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "proxychains-ng", "ar": "ุจุฑููุณู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "python-numpy", "ar": "ุญุณุงุจ ุนููู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "python-pydantic", "ar": "ุงูุชุญูู ูู ุงูุจูุงูุงุช"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "cmake", "ar": "ุจูุงุก ุงููุดุงุฑูุน"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "meson", "ar": "ูุธุงู ุจูุงุก"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "ninja", "ar": "ุจูุงุก ุณุฑูุน"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "doxygen", "ar": "ุชูุซูู ุงูููุฏ"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "pandoc-cli", "ar": "ุชุญููู ุงููุณุชูุฏุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "telegram-desktop", "ar": "ุชูููุฌุฑุงู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "thunderbird", "ar": "ุจุฑูุฏ ุฅููุชุฑููู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "vlc", "ar": "ูุดุบู ููุฏูู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "simple-scan", "ar": "ูุงุณุญ ุถูุฆู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "xournalpp", "ar": "ููุงุญุธุงุช ุฑูููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "virtualbox", "ar": "ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "docker", "ar": "ุญุงููุงุช"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "trilium-next", "ar": "ููุงุญุธุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "yara", "ar": "ุชุญููู ุงูุจุฑูุฌูุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "rhash", "ar": "ุญุณุงุจ ุงููุงุด"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "LibreOffice", "ar": "ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Google Chrome", "ar": "ูุชููุฑ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "PyCharm Community", "ar": "ูุฌุงูู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Orthopedic Supplies", "ar": "ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ"}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "instruction", "ar": "ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:"}, "category": "translation", "confidence": 0.03}
{"translation": {"en": "instruction", "ar": "ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:"}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "osteomyelitis", "ar": "ุงูุชูุงุจ ุงูุนุธู ูุงูููู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Mode 1", "ar": "ูุชุฑุฌู"}, "category": "translation", "confidence": 0.03}
{"translation": {"en": "Mode 2", "ar": "ุงุณุชุดุงุฑุฉ ุนูููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Mode 3", "ar": "ุชุญุฑูุฑ ููุดูุฑุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "fracture", "ar": "ูุณุฑ"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "internal fixation", "ar": "ุชุซุจูุช ุฏุงุฎูู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "instruction", "ar": "ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:"}, "category": "translation", "confidence": 0.03}

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation/extracted/translations_training.tsv
----------------------------------------
English	ุงููุต ุงูุฅูุฌููุฒู
Needs Correction	ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ
Source	ูุตุฏุฑ ุงูุชุฑุฌูุฉ
MIT License	ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.
subcategory	ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ
summary	ููุฎุต ููุฌุฒ ูููุต
Book	ูุชุงุจ
Computer	ุญุงุณูุจ
Programming	ุจุฑูุฌุฉ
Medical	ุทุจู
Treatment	ุนูุงุฌ
Disease	ูุฑุถ
Hospital	ูุณุชุดูู
Doctor	ุทุจูุจ
Software	ุจุฑูุงูุฌ
Development	ุชุทููุฑ
summary	ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู...
API	ุงูุชูููุฉ
summary	ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...
category	ุทุจู
language	ุงูุนุฑุจูุฉ
quality	ุนุงููุฉ
Combined Text Splitter	ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ
technical	ุชููู
translation	ุชุฑุฌูุฉ
misc	ูุชูุฑูุงุช
Knowledge Processor	ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ
pacman	ูุฏูุฑ ุงูุญุฒู
pacman-mirrorlist	ูุงุฆูุฉ ุงููุฑุงูุง
pam	ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู
procps-ng	ุฃุฏูุงุช ุงูุนูููุงุช
reflector	ุชุญุฏูุซ ุงููุฑุงูุง
pipewire	ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู
networkmanager	ูุฏูุฑ ุงูุดุจูุฉ
wget	ุชูุฒูู ุงููููุงุช
rclone	ูุฒุงููุฉ ุงูุณุญุงุจุฉ
tcpdump	ุชุญููู ุงูุดุจูุฉ
ufw	ุฌุฏุงุฑ ุญูุงูุฉ
proxychains-ng	ุจุฑููุณู
python-numpy	ุญุณุงุจ ุนููู
python-pydantic	ุงูุชุญูู ูู ุงูุจูุงูุงุช
cmake	ุจูุงุก ุงููุดุงุฑูุน
meson	ูุธุงู ุจูุงุก
ninja	ุจูุงุก ุณุฑูุน
doxygen	ุชูุซูู ุงูููุฏ
pandoc-cli	ุชุญููู ุงููุณุชูุฏุงุช
telegram-desktop	ุชูููุฌุฑุงู
thunderbird	ุจุฑูุฏ ุฅููุชุฑููู
vlc	ูุดุบู ููุฏูู
simple-scan	ูุงุณุญ ุถูุฆู
xournalpp	ููุงุญุธุงุช ุฑูููุฉ
virtualbox	ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ
docker	ุญุงููุงุช
trilium-next	ููุงุญุธุงุช
yara	ุชุญููู ุงูุจุฑูุฌูุงุช
rhash	ุญุณุงุจ ุงููุงุด
LibreOffice	ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ
Google Chrome	ูุชููุฑ
PyCharm Community	ูุฌุงูู
Orthopedic Supplies	ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ
instruction	ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:
instruction	ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:
osteomyelitis	ุงูุชูุงุจ ุงูุนุธู ูุงูููู
Mode 1	ูุชุฑุฌู
Mode 2	ุงุณุชุดุงุฑุฉ ุนูููุฉ
Mode 3	ุชุญุฑูุฑ ููุดูุฑุงุช
fracture	ูุณุฑ
internal fixation	ุชุซุจูุช ุฏุงุฎูู
instruction	ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation/extracted/translations_unified.csv
----------------------------------------
#,English,Arabic (Original),Arabic (Corrected),Category,Confidence,Corrections,Source File,Hash
1,English,ุงููุต ุงูุฅูุฌููุฒู,ุงููุต ุงูุฅูุฌููุฒู,translation,0.1,,KNOWLEDGE_PROCESSOR_GUIDE.md,989d10f0c911506b
2,Needs Correction,ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ,ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ,misc,0.0,,KNOWLEDGE_PROCESSOR_GUIDE.md,8125f942dbd41db4
3,Source,ูุตุฏุฑ ุงูุชุฑุฌูุฉ,ูุตุฏุฑ ุงูุชุฑุฌูุฉ,reference,0.07,,KNOWLEDGE_PROCESSOR_GUIDE.md,d05ec784fcb98aa6
4,MIT License,ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.,ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.,reference,0.04,,README.md,1e475393e1fff14a
5,subcategory,ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ,ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,c36007f7c10bf3ce
6,summary,ููุฎุต ููุฌุฒ ูููุต,ููุฎุต ููุฌุฒ ูููุต,reference,0.07,,Text_classifier_22026-main(2)_zip_contents.txt,4116b3ec2888bf79
7,Book,ูุชุงุจ,ูุชุงุจ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,8cc3c4c74f428695
8,Computer,ุญุงุณูุจ,ุญุงุณูุจ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,57e48a0b37fe6e2a
9,Programming,ุจุฑูุฌุฉ,ุจุฑูุฌุฉ,technical,0.03,,Text_classifier_22026-main(2)_zip_contents.txt,528031549148fe6b
10,Medical,ุทุจู,ุทุจู,medical,0.06,,Text_classifier_22026-main(2)_zip_contents.txt,fd6acf0608fd754a
11,Treatment,ุนูุงุฌ,ุนูุงุฌ,medical,0.04,,Text_classifier_22026-main(2)_zip_contents.txt,4ded5b90b271f804
12,Disease,ูุฑุถ,ูุฑุถ,medical,0.02,,Text_classifier_22026-main(2)_zip_contents.txt,ba64b600365665d1
13,Hospital,ูุณุชุดูู,ูุณุชุดูู,medical,0.04,,Text_classifier_22026-main(2)_zip_contents.txt,5ebb3d241e93d920
14,Doctor,ุทุจูุจ,ุทุจูุจ,medical,0.08,,Text_classifier_22026-main(2)_zip_contents.txt,16f9ee1c09c52306
15,Software,ุจุฑูุงูุฌ,ุจุฑูุงูุฌ,technical,0.03,,Text_classifier_22026-main(2)_zip_contents.txt,ef57a5f0a994a43a
16,Development,ุชุทููุฑ,ุชุทููุฑ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,313ac8f68672e632
17,summary,ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู...,ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู...,medical,0.04,,Text_classifier_22026-main(2)_zip_contents.txt,ba64aa20adc6b9a5
18,API,ุงูุชูููุฉ,ุงูุชูููุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,604c259c9eafcb1d
19,summary,ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...,ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...,medical,0.06,,Text_snippets-main(1)_zip_contents.txt,82b910142230e0a3
20,category,ุทุจู,ุทุจู,medical,0.04,,Text_snippets-main(1)_zip_contents.txt,77999fe05785dc8b
21,language,ุงูุนุฑุจูุฉ,ุงูุนุฑุจูุฉ,translation,0.07,,Text_snippets-main(1)_zip_contents.txt,55768a7eff752182
22,quality,ุนุงููุฉ,ุนุงููุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,cca21de38c6c41a6
23,Combined Text Splitter,ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ,ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ,translation,0.07,,Text_snippets-main(1)_zip_contents.txt,aee09839ac47d978
24,technical,ุชููู,ุชููู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,0e66ef2a65561934
25,translation,ุชุฑุฌูุฉ,ุชุฑุฌูุฉ,translation,0.07,,Text_snippets-main(1)_zip_contents.txt,db4de4a5a9cad8bc
26,misc,ูุชูุฑูุงุช,ูุชูุฑูุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,49a302f56404d6e7
27,Knowledge Processor,ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ,ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ,technical,0.03,,Text_snippets-main(1)_zip_contents.txt,b5185636270d8f8d
28,pacman,ูุฏูุฑ ุงูุญุฒู,ูุฏูุฑ ุงูุญุฒู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,fe187e92ca60553c
29,pacman-mirrorlist,ูุงุฆูุฉ ุงููุฑุงูุง,ูุงุฆูุฉ ุงููุฑุงูุง,reference,0.07,,Text_snippets-main(1)_zip_contents.txt,33055669f11768d7
30,pam,ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู,ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,39971cd2c35e7cfb
31,procps-ng,ุฃุฏูุงุช ุงูุนูููุงุช,ุฃุฏูุงุช ุงูุนูููุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,2990edf7a7da2bc8
32,reflector,ุชุญุฏูุซ ุงููุฑุงูุง,ุชุญุฏูุซ ุงููุฑุงูุง,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,565eb79468e842dc
33,pipewire,ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู,ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,21aa077c2a54bdfb
34,networkmanager,ูุฏูุฑ ุงูุดุจูุฉ,ูุฏูุฑ ุงูุดุจูุฉ,technical,0.03,,Text_snippets-main(1)_zip_contents.txt,8cc0693d1333dd7b
35,wget,ุชูุฒูู ุงููููุงุช,ุชูุฒูู ุงููููุงุช,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,99c668e96f3499ad
36,rclone,ูุฒุงููุฉ ุงูุณุญุงุจุฉ,ูุฒุงููุฉ ุงูุณุญุงุจุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,15ad107bbe914649
37,tcpdump,ุชุญููู ุงูุดุจูุฉ,ุชุญููู ุงูุดุจูุฉ,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,7872c2f9db238e7d
38,ufw,ุฌุฏุงุฑ ุญูุงูุฉ,ุฌุฏุงุฑ ุญูุงูุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,e2cf2fc591f73fd8
39,proxychains-ng,ุจุฑููุณู,ุจุฑููุณู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,227b3c25f93a0b8d
40,python-numpy,ุญุณุงุจ ุนููู,ุญุณุงุจ ุนููู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,0b5d8eb1e7bcd1a1
41,python-pydantic,ุงูุชุญูู ูู ุงูุจูุงูุงุช,ุงูุชุญูู ูู ุงูุจูุงูุงุช,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,4ddba992adf74ba4
42,cmake,ุจูุงุก ุงููุดุงุฑูุน,ุจูุงุก ุงููุดุงุฑูุน,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,03916e8835bd8fbe
43,meson,ูุธุงู ุจูุงุก,ูุธุงู ุจูุงุก,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,9fd105171d947a88
44,ninja,ุจูุงุก ุณุฑูุน,ุจูุงุก ุณุฑูุน,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,f5725dbbcdc44f67
45,doxygen,ุชูุซูู ุงูููุฏ,ุชูุซูู ุงูููุฏ,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,3e74b9db6bbdb42d
46,pandoc-cli,ุชุญููู ุงููุณุชูุฏุงุช,ุชุญููู ุงููุณุชูุฏุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,ed423e58dc8211a9
47,telegram-desktop,ุชูููุฌุฑุงู,ุชูููุฌุฑุงู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,e5d3fc2686985125
48,thunderbird,ุจุฑูุฏ ุฅููุชุฑููู,ุจุฑูุฏ ุฅููุชุฑููู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,fda0901867a507da
49,vlc,ูุดุบู ููุฏูู,ูุดุบู ููุฏูู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,22dce213525b8d8d
50,simple-scan,ูุงุณุญ ุถูุฆู,ูุงุณุญ ุถูุฆู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,b5d9337ef9929abb
51,xournalpp,ููุงุญุธุงุช ุฑูููุฉ,ููุงุญุธุงุช ุฑูููุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,c451b14711570670
52,virtualbox,ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ,ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,db837f094483ae84
53,docker,ุญุงููุงุช,ุญุงููุงุช,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,4262aaa9d92c74f7
54,trilium-next,ููุงุญุธุงุช,ููุงุญุธุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,e70e5c3513eaab51
55,yara,ุชุญููู ุงูุจุฑูุฌูุงุช,ุชุญููู ุงูุจุฑูุฌูุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,e62fc6b5c6c230f2
56,rhash,ุญุณุงุจ ุงููุงุด,ุญุณุงุจ ุงููุงุด,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,ae3817be5223947a
57,LibreOffice,ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ,ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ,misc,0.0,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,1a15fa27514821de
58,Google Chrome,ูุชููุฑ,ูุชููุฑ,misc,0.0,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,24e04f4c1d59c487
59,PyCharm Community,ูุฌุงูู,ูุฌุงูู,misc,0.0,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,f05f0d8c2a163f97
60,Orthopedic Supplies,ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ,ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ,medical,0.06,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,f7783966018643c1
61,instruction,ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:,ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:,translation,0.03,,ุชูููู ูุดุฑูุน ุชุตููู.txt,907a42fa9a93556d
62,instruction,ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:,ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:,medical,0.06,,ุชูููู ูุดุฑูุน ุชุตููู.txt,ad7739db9319b389
63,osteomyelitis,ุงูุชูุงุจ ุงูุนุธู ูุงูููู,ุงูุชูุงุจ ุงูุนุธู ูุงูููู,misc,0.0,,ุชูููู ูุดุฑูุน ุชุตููู.txt,3ef956d29f8ab9b2
64,Mode 1,ูุชุฑุฌู,ูุชุฑุฌู,translation,0.03,,ุชูููู ูุดุฑูุน ุชุตููู.txt,7e2d1e8b3dbf49da
65,Mode 2,ุงุณุชุดุงุฑุฉ ุนูููุฉ,ุงุณุชุดุงุฑุฉ ุนูููุฉ,misc,0.0,,ุชูููู ูุดุฑูุน ุชุตููู.txt,945cf089ede7e434
66,Mode 3,ุชุญุฑูุฑ ููุดูุฑุงุช,ุชุญุฑูุฑ ููุดูุฑุงุช,misc,0.0,,ุชูููู ูุดุฑูุน ุชุตููู.txt,75ce04055de77f8b
67,fracture,ูุณุฑ,ูุณุฑ,medical,0.04,,ุชูููู ูุดุฑูุน ุชุตููู.txt,d407a7a688b22df3
68,internal fixation,ุชุซุจูุช ุฏุงุฎูู,ุชุซุจูุช ุฏุงุฎูู,technical,0.02,,ุชูููู ูุดุฑูุน ุชุตููู.txt,c4f3013d544fec5b
69,instruction,ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:,ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:,translation,0.03,,ุชูููู ูุดุฑูุน ุชุตููู.txt,6306f5db30cd34ca

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation/ocr-bilingual.md
----------------------------------------
# ุฅุนุฏุงุฏ ุจูุฆุฉ OCR ุซูุงุฆูุฉ ุงููุบุฉ | Bilingual OCR Setup

## ููุฎุต
ุฏููู ูุชูุงูู ูุฅุนุฏุงุฏ ุจูุฆุฉ ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู (OCR) ุชุฏุนู ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ ุนูู Linux.

---

## ๐งฑ ุงูุชุซุจูุช ุงูุฃุณุงุณู

### 1. Tesseract OCR ูุน ุฏุนู ุงููุบุงุช

```bash
# ุชุซุจูุช ุงููุญุฑู ุงูุฃุณุงุณู
sudo pacman -S tesseract tesseract-data-eng tesseract-data-ara

# ุชุญุณูู ุฏูุฉ ุงูุชุนุฑู ุนูู ุงูุนุฑุจูุฉ (ุญุฒูุฉ ูุชุทูุฑุฉ)
yay -S tesseract-ocr-ara-best
```

> โ **ููุงุญุธุฉ**: ุญุฒูุฉ `tesseract-ocr-ara-best` ูู AUR ุชุนุทู ูุชุงุฆุฌ ุฃูุถู ููุนุฑุจูุฉ ูู ุงูุญุฒูุฉ ุงูุงูุชุฑุงุถูุฉ.

### 2. ูุนุงูุฌุฉ PDF (OCRmyPDF)

```bash
sudo pacman -S ocrmypdf ghostscript unpaper
```

### 3. ูุงุฌูุฉ ุฑุณูููุฉ

```bash
# ูุงุฌูุฉ ุฑุณูููุฉ ูู Tesseract ูุน ุฏุนู ุซูุงุฆู ุงููุบุฉ
sudo pacman -S gimagereader gimagereader-qt

# ุจุฏูู ุฎููู
yay -S ocrfeeder
```

---

## ๐ ุณูุฑุจุช ูุนุงูุฌุฉ PDF ุซูุงุฆู ุงููุบุฉ

```python
#!/usr/bin/env python3
"""
ูุนุงูุฌุฉ ูููุงุช PDF ุจุงููุบุชูู ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
"""
import subprocess
import sys
from pathlib import Path

def process_pdf(input_path: str, output_path: str):
    """ูุนุงูุฌุฉ ููู PDF ุจุงููุบุชูู ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ"""
    cmd = [
        "ocrmypdf",
        "--language", "ara+eng",
        "--force-ocr",              # ุฅุนุงุฏุฉ ุงูุชุนุฑู ุญุชู ูู ููุฌูุฏ ูุต
        "--deskew",                 # ุชุตุญูุญ ุงูููู
        "--clean",                  # ุชูุธูู ุงูุฎูููุฉ
        "--optimize", "3",          # ุถุบุท ูุชูุฏู
        "--output-type", "pdfa",    # ูุนูุงุฑ ุฃุฑุดูุฉ
        input_path,
        output_path
    ]
    subprocess.run(cmd, check=True)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: bilingual_ocr.py input.pdf output.pdf")
        sys.exit(1)
    process_pdf(sys.argv[1], sys.argv[2])
```

---

## ๐ฌ ูุนุงูุฌุฉ ุงูุตูุฑ ุงูุทุจูุฉ (DICOM)

```python
from pydicom import dcmread
import pytesseract
from PIL import Image
import numpy as np

def extract_text_from_dicom(dicom_path: str) -> str:
    """ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ุตูุฑุฉ ุทุจูุฉ (DICOM)"""
    ds = dcmread(dicom_path)

    # ุชุญููู ุงูุจูุณูุงุช ุฅูู ุตูุฑุฉ
    image = Image.fromarray(ds.pixel_array.astype(np.uint8))

    # OCR ุซูุงุฆู ุงููุบุฉ
    text = pytesseract.image_to_string(
        image,
        lang='ara+eng',
        config='--psm 6'  # ูุถุนูุฉ ููุงุณุจุฉ ูููุตูุต ุงูููุธูุฉ
    )
    return text
```

### ุชุซุจูุช ุงููุชุทูุจุงุช:
```bash
pip install pydicom Pillow pytesseract
```

---

## ๐ ููููุฉ ุงูุจูุงูุงุช ููุชุฏุฑูุจ

```python
import re

def align_sentences(english_text: str, arabic_text: str) -> list:
    """
    ูุญุงุฐุงุฉ ุงูุฌูู ุงูุฅูุฌููุฒูุฉ ูุงูุนุฑุจูุฉ
    ุงูุฅุฎุฑุงุฌ: ูุงุฆูุฉ ุจู [ุฅูุฌููุฒู, ุนุฑุจู]
    """
    # ุชูุณูู ุงููุตูุต ุฅูู ุฌูู
    en_sentences = re.split(r'[.!?]+', english_text)
    ar_sentences = re.split(r'[.ุ!ุ]+', arabic_text)

    aligned = []
    for i in range(min(len(en_sentences), len(ar_sentences))):
        en = en_sentences[i].strip()
        ar = ar_sentences[i].strip()
        if en and ar:
            aligned.append([en, ar])

    return aligned

def export_to_tsv(aligned_pairs: list, output_file: str):
    """ุชุตุฏูุฑ ุฅูู TSV"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("english\tarabic\n")
        for en, ar in aligned_pairs:
            f.write(f"{en}\t{ar}\n")
```

---

## ๐งช ุงุฎุชุจุงุฑ ุงูุฌูุฏุฉ

```python
import pytesseract
from PIL import Image

def ocr_with_confidence(image_path: str, min_conf: float = 70.0):
    """ุงุณุชุฎุฑุงุฌ ุงููุต ูุน ูุณุชูู ุซูุฉ ููู ูููุฉ"""
    data = pytesseract.image_to_data(
        Image.open(image_path),
        lang='ara+eng',
        output_type=pytesseract.Output.DICT
    )

    high_conf_text = []
    for i in range(len(data['text'])):
        if int(data['conf'][i]) > min_conf:
            high_conf_text.append(data['text'][i])

    return ' '.join(high_conf_text)
```

---

## ๐ค ุฑุจุท ุงูุจูุฆุฉ ุจู Hugging Face

### ุฅูุดุงุก Dataset

```python
from datasets import Dataset, DatasetDict
import pandas as pd

def create_hf_dataset(tsv_path: str, test_size: float = 0.1):
    """ุชุญููู TSV ุฅูู Dataset ูุชูุงูู ูุน Hugging Face"""
    df = pd.read_csv(tsv_path, sep='\t')

    # ุชูุณูู ุงูุจูุงูุงุช
    test_len = int(len(df) * test_size)
    train_df = df.iloc[:-test_len]
    test_df = df.iloc[-test_len:]

    dataset = DatasetDict({
        'train': Dataset.from_pandas(train_df),
        'test': Dataset.from_pandas(test_df)
    })

    dataset.save_to_disk('datasets/hf_dataset/')
    return dataset
```

### ุชุฏุฑูุจ ูููุฐุฌ ุชุฑุฌูุฉ

```python
from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments

# ุชุญููู ูููุฐุฌ ุฃุณุงุณู ุซูุงุฆู ุงููุบุฉ
model_name = 'Helsinki-NLP/opus-mt-en-ar'
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# ุฅุนุฏุงุฏ ุงูุชุฏุฑูุจ
training_args = TrainingArguments(
    output_dir='./models/medical-en-ar',
    num_train_epochs=15,
    per_device_train_batch_size=8,
    learning_rate=3e-5,
    evaluation_strategy='epoch',
    save_strategy='epoch',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['test'],
)

trainer.train()
trainer.save_model('./models/medical-en-ar-final')
```

---

## ๐ฆ ุญุฒูุฉ ุงูุชุซุจูุช ุงูุณุฑูุน

```bash
# ุชูููุฐ ูุฑุฉ ูุงุญุฏุฉ ูุชุซุจูุช ุงูุจูุฆุฉ ุงููุงููุฉ
sudo pacman -Syu --needed tesseract tesseract-data-eng tesseract-data-ara \
    ghostscript unpaper gimagereader ocrmypdf python-pip

yay -S tesseract-ocr-ara-best ocrfeeder

pip install pytesseract Pillow pydicom transformers datasets
```

---

## ๐๏ธ ููููุฉ ุงููุฌูุฏุงุช ุงูููุชุฑุญุฉ

```
~/ocr-workspace/
โโโ raw/               # ุงูุตูุฑ ุงูุฃุตููุฉ/PDF
โโโ processed/         # ุงููุชุงุฆุฌ ุจุนุฏ ุงููุนุงูุฌุฉ
โโโ bilingual-corpus/  # ูููุงุช TSV ููุชุฏุฑูุจ
โ   โโโ medical-en-ar.tsv
โ   โโโ tools-en-ar.tsv
โโโ scripts/
โ   โโโ bilingual_ocr.py
โ   โโโ align_bilingual.py
โ   โโโ quality_check.py
โโโ models/            # ููุงุฐุฌ ูุฎุตุตุฉ
```

---

## ๐ ููุงุฑูุฉ ุงูููุงุฐุฌ

| ุงููููุฐุฌ | ุงููุฒุงูุง | ุงูุนููุจ | ููุงุณุจ ูู |
|---------|----------|---------|-----------|
| **MarianMT** | ุฎูููุ ุชุฏุฑูุจ ุณุฑูุน | ุฏูุฉ ูุชูุณุทุฉ ูููุตุทูุญุงุช ุงููุงุฏุฑุฉ | ุชุฏุฑูุจ ูุญูู ุณุฑูุน |
| **NLLB-200** | ุฏูุฉ ุนุงููุฉุ 200 ูุบุฉ | ูุญุชุงุฌ ุฐุงูุฑุฉ GPU ูุจูุฑุฉ | ูุดุงุฑูุน ุงุญุชุฑุงููุฉ |
| **mBART-50** | ููุชุงุฒ ููุชุฑุฌูุฉ ุงูุซูุงุฆูุฉ | ุจุทูุก ูู ุงูุชุฏุฑูุจ | ูุดุงุฑูุน ุทูููุฉ ุงููุฏู |

---

## โ ุชูุตูุงุช

1. **ุงุจุฏุฃ ุจู MarianMT** ููุชุฏุฑูุจ ุงููุญูู ุงูุณุฑูุน
2. **ุงุณุชุฎุฏู gImageReader** ููุชุญูู ุงููุฏูู ูู ุงููุชุงุฆุฌ
3. **ุตุฏุฑ ุจูุงูุงุช TSV** ูุชุญุณูู ุงููููุฐุฌ ูุงุญูุงู
4. **ุงุฎุชุจุฑ ุงูุฌูุฏุฉ** ูุจู ุฑูุน ุงูุจูุงูุงุช ููุชุฏุฑูุจ

---

> ๐ **ููุงุญุธุฉ**: ููุจูุงูุงุช ุงูุทุจูุฉ ุงูุญุณุงุณุฉุ ุฏุฑูุจ ุงููููุฐุฌ ูุญููุงู ููุท ููุง ุชุฑูุน ุงูุจูุงูุงุช ุฅูู Hugging Face Hub.

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation/translations.csv
----------------------------------------
#,English,Arabic (Original),Arabic (Corrected),Needs Correction,Confidence
1,"chrome.deb, discord.deb, onlyoffice.deb, zoom.deb",ุฅูุฎ,ุฅูุฎ,No,medium
2,LibreOffice,ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ ูุงูุฃุบุฑุงุถ ุงูุนุงูุฉ,ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ ูุงูุฃุบุฑุงุถ ุงูุนุงูุฉ,No,medium
3,OnlyOffice,ุงูุฃูุถู ูููุฑู ูุงูุชุนุงูู ุนุจุฑ ุงูุณุญุงุจุฉ,ุงูุฃูุถู ูููุฑู ูุงูุชุนุงูู ุนุจุฑ ุงูุณุญุงุจุฉ,No,medium
4,AUR,ุฅุตุฏุงุฑุงุช ูุญุฏุซุฉ,ุฅุตุฏุงุฑุงุช ูุญุฏุซุฉ,No,medium
5,OnlyOffice,ูุฏุนู ุงูุนุฑุจูุฉ ููู ูุงุฌูุฉ ุงููุณุชุฎุฏู ุฃุณุงุณุงู ุจุงูุฅูุฌููุฒูุฉ,ูุฏุนู ุงูุนุฑุจูุฉ ููู ูุงุฌูุฉ ุงููุณุชุฎุฏู ุฃุณุงุณุงู ุจุงูุฅูุฌููุฒูุฉ,No,medium
6,Papirus Icons,ุฃููููุงุช ุฌูููุฉ ูุญุฏูุซุฉ,ุฃููููุงุช ุฌูููุฉ ูุญุฏูุซุฉ,No,high
7,Materia KDE,ุณูุฉ ูุงุฏูุฉ ูุธููุฉ,ุณูุฉ ูุงุฏูุฉ ูุธููุฉ,No,high
8,Arc KDE Theme,ูุซุจุช ุฌุฒุฆูุงู (ููู ูุดุงูู ูู ุงูุชูุฒูู),ูุซุจุช ุฌุฒุฆูุงู (ููู ูุดุงูู ูู ุงูุชูุฒูู),No,high
9,Arc GTK Theme,ูุดู ุงูุชุซุจูุช ุจุณุจุจ ูุดุงูู ุงูุดุจูุฉ,ูุดู ุงูุชุซุจูุช ุจุณุจุจ ูุดุงูู ุงูุดุจูุฉ,No,medium
10,Garuda Assistant,ููุญุฉ ุชุญูู ูุฑูุฒูุฉ ูููุธุงู,ููุญุฉ ุชุญูู ูุฑูุฒูุฉ ูููุธุงู,No,high
11,Garuda Boot Manager,ุฃุฏุงุฉ ูุชูุฏูุฉ ูุฅุฏุงุฑุฉ ุงูุฅููุงุน,ุฃุฏุงุฉ ูุชูุฏูุฉ ูุฅุฏุงุฑุฉ ุงูุฅููุงุน,No,high
12,Garuda Gamer,ูุฌููุนุฉ ุฃุฏูุงุช ุชุญุณูู ููุฃูุนุงุจ,ูุฌููุนุฉ ุฃุฏูุงุช ุชุญุณูู ููุฃูุนุงุจ,No,high
13,System Monitoring Center,ูุฑุงูุจ ููุงุฑุฏ ูุชุทูุฑ,ูุฑุงูุจ ููุงุฑุฏ ูุชุทูุฑ,No,high
14,Zorin Grid,ุฅุฏุงุฑุฉ ุงูููุงูุฐ ุงููุชูุฏูุฉ,ุฅุฏุงุฑุฉ ุงูููุงูุฐ ุงููุชูุฏูุฉ,No,high
15,Zorin Taskbar,ุดุฑูุท ููุงู ูุชูุฏู,ุดุฑูุท ููุงู ูุชูุฏู,No,high
16,Zorin Upgrader,ุฃุฏุงุฉ ุชุฑููุฉ ูุฎุตุตุฉ,ุฃุฏุงุฉ ุชุฑููุฉ ูุฎุตุตุฉ,No,high
17,JetBrains Account,ุฅุฐุง ูุฏูู ุญุณุงุจ ููุนู,ุฅุฐุง ูุฏูู ุญุณุงุจ ููุนู,No,medium
18,License server,ุฅุฐุง ูุคุณุณุชู ุชููุฑ ุณูุฑูุฑ,ุฅุฐุง ูุคุณุณุชู ุชููุฑ ุณูุฑูุฑ,No,medium
19,Activation code,ุฅุฐุง ูุฏูู ููุฏ ุชูุนูู,ุฅุฐุง ูุฏูู ููุฏ ุชูุนูู,No,medium
20,Name,ุงุณุชุฎุฑุงุฌ ุฅูู ูุต,ุงุณุชุฎุฑุงุฌ ุฅูู ูุต,No,medium
21,Auto-Detection Hardware,ูุดู ุชููุงุฆู ููููุงุฑุฏ ูุชุนุฏูู ุงูุฅุนุฏุงุฏุงุช,ูุดู ุชููุงุฆู ููููุงุฑุฏ ูุชุนุฏูู ุงูุฅุนุฏุงุฏุงุช,No,medium
22,User Profiles,ูููุงุช ุฅุนุฏุงุฏ ูุณุจูุฉ (ูุทูุฑุ ุทุงูุจุ ููุชุจ),ูููุงุช ุฅุนุฏุงุฏ ูุณุจูุฉ (ูุทูุฑุ ุทุงูุจุ ููุชุจ),No,medium
23,Offline Mode,ุชุซุจูุช ุจุฏูู ุฅูุชุฑูุช,ุชุซุจูุช ุจุฏูู ุฅูุชุฑูุช,No,medium
24,DISCLAIMER.md,ุชูุถูุญ ุงููุณุคูููุงุชุ ุงูุชุญุฐูุฑุงุชุ ุดุฑูุท ุงูุงุณุชุฎุฏุงูุ ูุญููู ุงููุณุงูููู,ุชูุถูุญ ุงููุณุคูููุงุชุ ุงูุชุญุฐูุฑุงุชุ ุดุฑูุท ุงูุงุณุชุฎุฏุงูุ ูุญููู ุงููุณุงูููู,No,medium
25,cleanup.sh,ุชูุธูู ุงููุธุงู ูุฅุฒุงูุฉ ุงููููุงุช ุงููุคูุชุฉ,ุชูุธูู ุงููุธุงู ูุฅุฒุงูุฉ ุงููููุงุช ุงููุคูุชุฉ,No,high
26,Medium,ุจูุบุฉ ุนุฑุจูุฉ ูุน ูุณู ุชููู,ุจูุบุฉ ุนุฑุจูุฉ ูุน ูุณู ุชููู,No,high
27,Dev.to,ูููุทูุฑูู ุงูุนุฑุจ,ูููุทูุฑูู ุงูุนุฑุจ,No,high
28,Hashnode,ูููุญุชูู ุงูุชููู,ูููุญุชูู ุงูุชููู,No,high
29,README.md,ุฏููู ุงูุจุฏุก ุงูุณุฑูุน,ุฏููู ุงูุจุฏุก ุงูุณุฑูุน,No,medium
30,GNOME Shell,ุงูุจูุฆุฉ ููุณูุง,ุงูุจูุฆุฉ ููุณูุง,No,medium
31,GDM (GNOME Display Manager),ุดุงุดุฉ ุชุณุฌูู ุงูุฏุฎูู,ุดุงุดุฉ ุชุณุฌูู ุงูุฏุฎูู,No,medium
32,KDE Plasma,ุงูุจูุฆุฉ ุงูุฃุตููุฉ,ุงูุจูุฆุฉ ุงูุฃุตููุฉ,No,medium
33,GNOME Shell,ุงููุงุฌูุฉ ุงูุฑุฆูุณูุฉ,ุงููุงุฌูุฉ ุงูุฑุฆูุณูุฉ,No,high
34,GNOME Settings,ุฅุนุฏุงุฏุงุช ุงููุธุงู,ุฅุนุฏุงุฏุงุช ุงููุธุงู,No,high
35,GNOME Boxes,ุขูุงุช ุงูุชุฑุงุถูุฉ,ุขูุงุช ุงูุชุฑุงุถูุฉ,No,high
36,GNOME Builder,ุจูุฆุฉ ุชุทููุฑ,ุจูุฆุฉ ุชุทููุฑ,No,high
37,GNOME Chess,ุดุทุฑูุฌ,ุดุทุฑูุฌ,No,high
38,GNOME Maps,ุฎุฑุงุฆุท,ุฎุฑุงุฆุท,No,high
39,GNOME Music,ูุดุบู ููุณููู,ูุดุบู ููุณููู,No,high
40,GNOME Weather,ุงูุทูุณ,ุงูุทูุณ,No,high
41,X-KDE-Submenu,ุงุณุชุฎุฑุงุฌ ุงููุญุชููุงุช ุฅูู ูุต,ุงุณุชุฎุฑุงุฌ ุงููุญุชููุงุช ุฅูู ูุต,No,medium
42,Name,ุญูุธ ุงููุญุชููุงุช ูููู ูุตู,ุญูุธ ุงููุญุชููุงุช ูููู ูุตู,No,medium
43,Name,ูุณุฎ ุงููุณุงุฑ,ูุณุฎ ุงููุณุงุฑ,No,medium
44,Name,ูุณุฎ ุงููุณุงุฑ ุงููุงูู,ูุณุฎ ุงููุณุงุฑ ุงููุงูู,No,medium
45,Name,ูุณุฎ ุงููุณุงุฑ ุงููุณุจู,ูุณุฎ ุงููุณุงุฑ ุงููุณุจู,No,medium
46,Name,ูุณุฎ ุงุณู ุงูููู ููุท,ูุณุฎ ุงุณู ุงูููู ููุท,No,medium
47,Comment,ูุณุฎ ุงููุณุงุฑ ุงููุงูู ููููู,ูุณุฎ ุงููุณุงุฑ ุงููุงูู ููููู,No,medium
48,Google Chrome,ูุชููุฑ,ูุชููุฑ,No,high
49,Mozilla Firefox,ูุชููุฑ,ูุชููุฑ,No,high
50,Git,ูุชููุฑ,ูุชููุฑ,No,high
51,Node.js,ูุชููุฑ,ูุชููุฑ,No,high
52,Visual Studio Code,ูุชููุฑ,ูุชููุฑ,No,high
53,PyCharm,ูุชููุฑ,ูุชููุฑ,No,high
54,CMake,ูุชููุฑ,ูุชููุฑ,No,high
55,DB Browser for SQLite,ูุชููุฑ,ูุชููุฑ,No,high
56,Size,ุงูุจุงูู (ุฃู ุญุณุจ ุงูุญุงุฌุฉ),ุงูุจุงูู (ุฃู ุญุณุจ ุงูุญุงุฌุฉ),No,medium
57,Partition,ุงุณุชุฎุฏู ุงููุฑุต ูุงููุงู,ุงุณุชุฎุฏู ุงููุฑุต ูุงููุงู,No,medium
58,Region,ุญุณุจ ูููุนู,ุญุณุจ ูููุนู,No,medium
59,Password,ุงุฎุชูุงุฑู ููู ููุตู ุจู,ุงุฎุชูุงุฑู ููู ููุตู ุจู,No,medium
60,outline-client-appimage,ุงูุฃูุซุฑ ุงุณุชูุฑุงุฑุงู,ุงูุฃูุซุฑ ุงุณุชูุฑุงุฑุงู,No,high
61,outline-client-appimage-git,ุงูุฅุตุฏุงุฑ ุงูุชุทููุฑู (ูุฏ ูููู ุฃุญุฏุซ),ุงูุฅุตุฏุงุฑ ุงูุชุทููุฑู (ูุฏ ูููู ุฃุญุฏุซ),No,high
62,outline-client-appimage-git,ุงูุฅุตุฏุงุฑ ุงูุชุทููุฑู,ุงูุฅุตุฏุงุฑ ุงูุชุทููุฑู,No,high
63,programs.csv,ูุงุฆูุฉ ุจุจุนุถ ุงูุจุฑุงูุฌ ูุน ุงูุชูุงุตูู (ุฑุจูุง ุฌุฒุก ูู ุงููุงุฆูุฉ ุงูุณุงุจูุฉ),ูุงุฆูุฉ ุจุจุนุถ ุงูุจุฑุงูุฌ ูุน ุงูุชูุงุตูู (ุฑุจูุง ุฌุฒุก ูู ุงููุงุฆูุฉ ุงูุณุงุจูุฉ),No,high
64,PlayOnLinux,ุณูู ุงูุงุณุชุฎุฏุงู ูุน ุฏุนู ุงูุนุฏูุฏ ูู ุงูุชุทุจููุงุช,ุณูู ุงูุงุณุชุฎุฏุงู ูุน ุฏุนู ุงูุนุฏูุฏ ูู ุงูุชุทุจููุงุช,No,medium
65,Lutris,ููุชุงุฒ ููุฃูุนุงุจ,ููุชุงุฒ ููุฃูุนุงุจ,No,medium
66,VLC,ูุชููุฑ,ูุชููุฑ,No,high
67,VirtualBox,ุฃุณูู ููุฌุงูู,ุฃุณูู ููุฌุงูู,No,medium
68,Boot,ูุจู ูู ุชุญุฏูุซ,ูุจู ูู ุชุญุฏูุซ,No,medium
69,Linux-zen kernel,ูุญุณู ููุฃุฏุงุก ูุงูุงุณุชุฌุงุจุฉ,ูุญุณู ููุฃุฏุงุก ูุงูุงุณุชุฌุงุจุฉ,No,medium
70,Preload,ุชุญููู ุงูุจุฑุงูุฌ ุงูุดุงุฆุนุฉ ูู ุงูุฐุงูุฑุฉ ูุณุจูุงู,ุชุญููู ุงูุจุฑุงูุฌ ุงูุดุงุฆุนุฉ ูู ุงูุฐุงูุฑุฉ ูุณุจูุงู,No,medium
71,PlayOnLinux,ูุน ุฅุนุฏุงุฏุงุช ูุฎุตุตุฉ,ูุน ุฅุนุฏุงุฏุงุช ูุฎุตุตุฉ,No,medium
72,Zorin Appearance,ุฃุฏุงุฉ ูุชุบููุฑ ุงููุธูุฑ ุจุณูููุฉ,ุฃุฏุงุฉ ูุชุบููุฑ ุงููุธูุฑ ุจุณูููุฉ,No,high
73,Manjaro,ุญุฑูุงุช ุฎุทูุฉ (ุฃูู ุณูุงุณุฉ),ุญุฑูุงุช ุฎุทูุฉ (ุฃูู ุณูุงุณุฉ),No,medium
74,Manjaro,ุฅุนุฏุงุฏุงุช ุงูุชุฑุงุถูุฉ,ุฅุนุฏุงุฏุงุช ุงูุชุฑุงุถูุฉ,No,medium
75,Compression,ูุง ููุฌุฏ,ูุง ููุฌุฏ,No,medium
76,LibreOffice,ุบูุฑ ูุซุจุช,ุบูุฑ ูุซุจุช,No,medium
77,Material Theme UI,ูุงุฌูุฉ ุฃุฌูู,ูุงุฌูุฉ ุฃุฌูู,No,medium
78,Rainbow Brackets,ุฃููุงุณ ููููุฉ,ุฃููุงุณ ููููุฉ,No,medium
79,CodeGlance,ูุนุงููุฉ ูุตุบุฑุฉ ููููุฏ,ูุนุงููุฉ ูุตุบุฑุฉ ููููุฏ,No,medium
80,PyCharm Community,ูุฌุงูู,ูุฌุงูู,No,high
81,MIT License,ููุชูุญ ุงููุตุฏุฑ ููุฌุงูู ููุฌููุน,ููุชูุญ ุงููุตุฏุฑ ููุฌุงูู ููุฌููุน,No,high

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation_extracted/PROCESSING_REPORT.md
----------------------------------------
# ๐ ุชูุฑูุฑ ุงุณุชุฎุฑุงุฌ ุงูุชุฑุฌูุงุช

**ุงูุชุงุฑูุฎ:** 2026-02-16 22:05:31

## ๐ ุงูุฅุญุตุงุฆูุงุช

| ุงูุจูุฏ | ุงูุนุฏุฏ |
|-------|-------|
| ุงููููุงุช ุงููุนุงูุฌุฉ | 20 |
| ุฃุฒูุงุฌ ุงูุชุฑุฌูุฉ ุงููุณุชุฎุฑุฌุฉ | 69 |
| ุงูุชูุฑุงุฑุงุช ุงููุฒุงูุฉ | 230 |
| ุงูุชุตุญูุญุงุช ุงูุฅููุงุฆูุฉ | 0 |

## ๐ ุงูุชุตููู

| ุงููุฆุฉ | ุงูุนุฏุฏ | ุงููุณุจุฉ |
|-------|-------|--------|
| ๐ misc | 31 | 44.9% |
| ๐ป technical | 16 | 23.2% |
| ๐ฅ medical | 11 | 15.9% |
| ๐ translation | 7 | 10.1% |
| ๐ reference | 4 | 5.8% |

## ๐ ุงููููุงุช ุงููุงุชุฌุฉ

| ุงูููู | ุงููุตู |
|-------|-------|
| translations_unified.csv | ุฌููุน ุงูุชุฑุฌูุงุช ูู ููู CSV |
| translations_training.tsv | ุชูุณูู TSV ููุชุฏุฑูุจ |
| translations_huggingface.jsonl | ุชูุณูู JSONL ูู HuggingFace |

## โ ุฃูุซูุฉ ุนูู ุงูุชุฑุฌูุงุช ุงููุณุชุฎุฑุฌุฉ

| English | Arabic | Category |
|---------|--------|----------|
| English... | ุงููุต ุงูุฅูุฌููุฒู... | translation |
| Needs Correction... | ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ... | misc |
| Source... | ูุตุฏุฑ ุงูุชุฑุฌูุฉ... | reference |
| MIT License... | ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.... | reference |
| subcategory... | ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ... | misc |
| summary... | ููุฎุต ููุฌุฒ ูููุต... | reference |
| Book... | ูุชุงุจ... | misc |
| Computer... | ุญุงุณูุจ... | misc |
| Programming... | ุจุฑูุฌุฉ... | technical |
| Medical... | ุทุจู... | medical |

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation_extracted/translations_huggingface.jsonl
----------------------------------------
{"translation": {"en": "English", "ar": "ุงููุต ุงูุฅูุฌููุฒู"}, "category": "translation", "confidence": 0.1}
{"translation": {"en": "Needs Correction", "ar": "ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Source", "ar": "ูุตุฏุฑ ุงูุชุฑุฌูุฉ"}, "category": "reference", "confidence": 0.07}
{"translation": {"en": "MIT License", "ar": "ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ."}, "category": "reference", "confidence": 0.04}
{"translation": {"en": "subcategory", "ar": "ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "summary", "ar": "ููุฎุต ููุฌุฒ ูููุต"}, "category": "reference", "confidence": 0.07}
{"translation": {"en": "Book", "ar": "ูุชุงุจ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Computer", "ar": "ุญุงุณูุจ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Programming", "ar": "ุจุฑูุฌุฉ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "Medical", "ar": "ุทุจู"}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "Treatment", "ar": "ุนูุงุฌ"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "Disease", "ar": "ูุฑุถ"}, "category": "medical", "confidence": 0.02}
{"translation": {"en": "Hospital", "ar": "ูุณุชุดูู"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "Doctor", "ar": "ุทุจูุจ"}, "category": "medical", "confidence": 0.08}
{"translation": {"en": "Software", "ar": "ุจุฑูุงูุฌ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "Development", "ar": "ุชุทููุฑ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "summary", "ar": "ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู..."}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "API", "ar": "ุงูุชูููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "summary", "ar": "ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู..."}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "category", "ar": "ุทุจู"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "language", "ar": "ุงูุนุฑุจูุฉ"}, "category": "translation", "confidence": 0.07}
{"translation": {"en": "quality", "ar": "ุนุงููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Combined Text Splitter", "ar": "ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ"}, "category": "translation", "confidence": 0.07}
{"translation": {"en": "technical", "ar": "ุชููู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "translation", "ar": "ุชุฑุฌูุฉ"}, "category": "translation", "confidence": 0.07}
{"translation": {"en": "misc", "ar": "ูุชูุฑูุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Knowledge Processor", "ar": "ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "pacman", "ar": "ูุฏูุฑ ุงูุญุฒู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "pacman-mirrorlist", "ar": "ูุงุฆูุฉ ุงููุฑุงูุง"}, "category": "reference", "confidence": 0.07}
{"translation": {"en": "pam", "ar": "ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "procps-ng", "ar": "ุฃุฏูุงุช ุงูุนูููุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "reflector", "ar": "ุชุญุฏูุซ ุงููุฑุงูุง"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "pipewire", "ar": "ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "networkmanager", "ar": "ูุฏูุฑ ุงูุดุจูุฉ"}, "category": "technical", "confidence": 0.03}
{"translation": {"en": "wget", "ar": "ุชูุฒูู ุงููููุงุช"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "rclone", "ar": "ูุฒุงููุฉ ุงูุณุญุงุจุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "tcpdump", "ar": "ุชุญููู ุงูุดุจูุฉ"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "ufw", "ar": "ุฌุฏุงุฑ ุญูุงูุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "proxychains-ng", "ar": "ุจุฑููุณู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "python-numpy", "ar": "ุญุณุงุจ ุนููู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "python-pydantic", "ar": "ุงูุชุญูู ูู ุงูุจูุงูุงุช"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "cmake", "ar": "ุจูุงุก ุงููุดุงุฑูุน"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "meson", "ar": "ูุธุงู ุจูุงุก"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "ninja", "ar": "ุจูุงุก ุณุฑูุน"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "doxygen", "ar": "ุชูุซูู ุงูููุฏ"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "pandoc-cli", "ar": "ุชุญููู ุงููุณุชูุฏุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "telegram-desktop", "ar": "ุชูููุฌุฑุงู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "thunderbird", "ar": "ุจุฑูุฏ ุฅููุชุฑููู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "vlc", "ar": "ูุดุบู ููุฏูู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "simple-scan", "ar": "ูุงุณุญ ุถูุฆู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "xournalpp", "ar": "ููุงุญุธุงุช ุฑูููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "virtualbox", "ar": "ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "docker", "ar": "ุญุงููุงุช"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "trilium-next", "ar": "ููุงุญุธุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "yara", "ar": "ุชุญููู ุงูุจุฑูุฌูุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "rhash", "ar": "ุญุณุงุจ ุงููุงุด"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "LibreOffice", "ar": "ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Google Chrome", "ar": "ูุชููุฑ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "PyCharm Community", "ar": "ูุฌุงูู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Orthopedic Supplies", "ar": "ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ"}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "instruction", "ar": "ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:"}, "category": "translation", "confidence": 0.03}
{"translation": {"en": "instruction", "ar": "ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:"}, "category": "medical", "confidence": 0.06}
{"translation": {"en": "osteomyelitis", "ar": "ุงูุชูุงุจ ุงูุนุธู ูุงูููู"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Mode 1", "ar": "ูุชุฑุฌู"}, "category": "translation", "confidence": 0.03}
{"translation": {"en": "Mode 2", "ar": "ุงุณุชุดุงุฑุฉ ุนูููุฉ"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "Mode 3", "ar": "ุชุญุฑูุฑ ููุดูุฑุงุช"}, "category": "misc", "confidence": 0.0}
{"translation": {"en": "fracture", "ar": "ูุณุฑ"}, "category": "medical", "confidence": 0.04}
{"translation": {"en": "internal fixation", "ar": "ุชุซุจูุช ุฏุงุฎูู"}, "category": "technical", "confidence": 0.02}
{"translation": {"en": "instruction", "ar": "ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:"}, "category": "translation", "confidence": 0.03}

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation_extracted/translations_training.tsv
----------------------------------------
English	ุงููุต ุงูุฅูุฌููุฒู
Needs Correction	ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ
Source	ูุตุฏุฑ ุงูุชุฑุฌูุฉ
MIT License	ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.
subcategory	ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ
summary	ููุฎุต ููุฌุฒ ูููุต
Book	ูุชุงุจ
Computer	ุญุงุณูุจ
Programming	ุจุฑูุฌุฉ
Medical	ุทุจู
Treatment	ุนูุงุฌ
Disease	ูุฑุถ
Hospital	ูุณุชุดูู
Doctor	ุทุจูุจ
Software	ุจุฑูุงูุฌ
Development	ุชุทููุฑ
summary	ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู...
API	ุงูุชูููุฉ
summary	ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...
category	ุทุจู
language	ุงูุนุฑุจูุฉ
quality	ุนุงููุฉ
Combined Text Splitter	ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ
technical	ุชููู
translation	ุชุฑุฌูุฉ
misc	ูุชูุฑูุงุช
Knowledge Processor	ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ
pacman	ูุฏูุฑ ุงูุญุฒู
pacman-mirrorlist	ูุงุฆูุฉ ุงููุฑุงูุง
pam	ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู
procps-ng	ุฃุฏูุงุช ุงูุนูููุงุช
reflector	ุชุญุฏูุซ ุงููุฑุงูุง
pipewire	ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู
networkmanager	ูุฏูุฑ ุงูุดุจูุฉ
wget	ุชูุฒูู ุงููููุงุช
rclone	ูุฒุงููุฉ ุงูุณุญุงุจุฉ
tcpdump	ุชุญููู ุงูุดุจูุฉ
ufw	ุฌุฏุงุฑ ุญูุงูุฉ
proxychains-ng	ุจุฑููุณู
python-numpy	ุญุณุงุจ ุนููู
python-pydantic	ุงูุชุญูู ูู ุงูุจูุงูุงุช
cmake	ุจูุงุก ุงููุดุงุฑูุน
meson	ูุธุงู ุจูุงุก
ninja	ุจูุงุก ุณุฑูุน
doxygen	ุชูุซูู ุงูููุฏ
pandoc-cli	ุชุญููู ุงููุณุชูุฏุงุช
telegram-desktop	ุชูููุฌุฑุงู
thunderbird	ุจุฑูุฏ ุฅููุชุฑููู
vlc	ูุดุบู ููุฏูู
simple-scan	ูุงุณุญ ุถูุฆู
xournalpp	ููุงุญุธุงุช ุฑูููุฉ
virtualbox	ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ
docker	ุญุงููุงุช
trilium-next	ููุงุญุธุงุช
yara	ุชุญููู ุงูุจุฑูุฌูุงุช
rhash	ุญุณุงุจ ุงููุงุด
LibreOffice	ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ
Google Chrome	ูุชููุฑ
PyCharm Community	ูุฌุงูู
Orthopedic Supplies	ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ
instruction	ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:
instruction	ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:
osteomyelitis	ุงูุชูุงุจ ุงูุนุธู ูุงูููู
Mode 1	ูุชุฑุฌู
Mode 2	ุงุณุชุดุงุฑุฉ ุนูููุฉ
Mode 3	ุชุญุฑูุฑ ููุดูุฑุงุช
fracture	ูุณุฑ
internal fixation	ุชุซุจูุช ุฏุงุฎูู
instruction	ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:

================================================================================

ุงุณู ุงูููู: Text_snippets-main/translation_extracted/translations_unified.csv
----------------------------------------
#,English,Arabic (Original),Arabic (Corrected),Category,Confidence,Corrections,Source File,Hash
1,English,ุงููุต ุงูุฅูุฌููุฒู,ุงููุต ุงูุฅูุฌููุฒู,translation,0.1,,KNOWLEDGE_PROCESSOR_GUIDE.md,989d10f0c911506b
2,Needs Correction,ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ,ูู ูุงู ูุญุชุงุฌ ุชุตุญูุญุ,misc,0.0,,KNOWLEDGE_PROCESSOR_GUIDE.md,8125f942dbd41db4
3,Source,ูุตุฏุฑ ุงูุชุฑุฌูุฉ,ูุตุฏุฑ ุงูุชุฑุฌูุฉ,reference,0.07,,KNOWLEDGE_PROCESSOR_GUIDE.md,d05ec784fcb98aa6
4,MIT License,ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.,ุงุณุชุฎุฏู ุจุญุฑูุฉ ูุน ุงูุฅุดุงุฑุฉ ูููุตุฏุฑ.,reference,0.04,,README.md,1e475393e1fff14a
5,subcategory,ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ,ุงููุฆุฉ ุงููุฑุนูุฉ ุงููุญุฏุฏุฉ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,c36007f7c10bf3ce
6,summary,ููุฎุต ููุฌุฒ ูููุต,ููุฎุต ููุฌุฒ ูููุต,reference,0.07,,Text_classifier_22026-main(2)_zip_contents.txt,4116b3ec2888bf79
7,Book,ูุชุงุจ,ูุชุงุจ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,8cc3c4c74f428695
8,Computer,ุญุงุณูุจ,ุญุงุณูุจ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,57e48a0b37fe6e2a
9,Programming,ุจุฑูุฌุฉ,ุจุฑูุฌุฉ,technical,0.03,,Text_classifier_22026-main(2)_zip_contents.txt,528031549148fe6b
10,Medical,ุทุจู,ุทุจู,medical,0.06,,Text_classifier_22026-main(2)_zip_contents.txt,fd6acf0608fd754a
11,Treatment,ุนูุงุฌ,ุนูุงุฌ,medical,0.04,,Text_classifier_22026-main(2)_zip_contents.txt,4ded5b90b271f804
12,Disease,ูุฑุถ,ูุฑุถ,medical,0.02,,Text_classifier_22026-main(2)_zip_contents.txt,ba64b600365665d1
13,Hospital,ูุณุชุดูู,ูุณุชุดูู,medical,0.04,,Text_classifier_22026-main(2)_zip_contents.txt,5ebb3d241e93d920
14,Doctor,ุทุจูุจ,ุทุจูุจ,medical,0.08,,Text_classifier_22026-main(2)_zip_contents.txt,16f9ee1c09c52306
15,Software,ุจุฑูุงูุฌ,ุจุฑูุงูุฌ,technical,0.03,,Text_classifier_22026-main(2)_zip_contents.txt,ef57a5f0a994a43a
16,Development,ุชุทููุฑ,ุชุทููุฑ,misc,0.0,,Text_classifier_22026-main(2)_zip_contents.txt,313ac8f68672e632
17,summary,ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู...,ูุต ูุชูุงูู ุทุฑู ุงูุชุดุฎูุต ุงูุทุจู...,medical,0.04,,Text_classifier_22026-main(2)_zip_contents.txt,ba64aa20adc6b9a5
18,API,ุงูุชูููุฉ,ุงูุชูููุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,604c259c9eafcb1d
19,summary,ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...,ุชูุฑูุฑ ุทุจู ุนู ุญุงูุงุช ุงูุนุธุงู...,medical,0.06,,Text_snippets-main(1)_zip_contents.txt,82b910142230e0a3
20,category,ุทุจู,ุทุจู,medical,0.04,,Text_snippets-main(1)_zip_contents.txt,77999fe05785dc8b
21,language,ุงูุนุฑุจูุฉ,ุงูุนุฑุจูุฉ,translation,0.07,,Text_snippets-main(1)_zip_contents.txt,55768a7eff752182
22,quality,ุนุงููุฉ,ุนุงููุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,cca21de38c6c41a6
23,Combined Text Splitter,ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ,ููุณู ุงููููุงุช ุงููุตูุฉ ุงููุฌูุนุฉ,translation,0.07,,Text_snippets-main(1)_zip_contents.txt,aee09839ac47d978
24,technical,ุชููู,ุชููู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,0e66ef2a65561934
25,translation,ุชุฑุฌูุฉ,ุชุฑุฌูุฉ,translation,0.07,,Text_snippets-main(1)_zip_contents.txt,db4de4a5a9cad8bc
26,misc,ูุชูุฑูุงุช,ูุชูุฑูุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,49a302f56404d6e7
27,Knowledge Processor,ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ,ุณูุฑูุจุช ูุนุงูุฌุฉ ุงููููุงุช ุงููุนุฑููุฉ,technical,0.03,,Text_snippets-main(1)_zip_contents.txt,b5185636270d8f8d
28,pacman,ูุฏูุฑ ุงูุญุฒู,ูุฏูุฑ ุงูุญุฒู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,fe187e92ca60553c
29,pacman-mirrorlist,ูุงุฆูุฉ ุงููุฑุงูุง,ูุงุฆูุฉ ุงููุฑุงูุง,reference,0.07,,Text_snippets-main(1)_zip_contents.txt,33055669f11768d7
30,pam,ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู,ูุตุงุฏูุฉ ุงููุณุชุฎุฏููู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,39971cd2c35e7cfb
31,procps-ng,ุฃุฏูุงุช ุงูุนูููุงุช,ุฃุฏูุงุช ุงูุนูููุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,2990edf7a7da2bc8
32,reflector,ุชุญุฏูุซ ุงููุฑุงูุง,ุชุญุฏูุซ ุงููุฑุงูุง,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,565eb79468e842dc
33,pipewire,ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู,ุฎุงุฏู ุงูุตูุช ูุงูููุฏูู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,21aa077c2a54bdfb
34,networkmanager,ูุฏูุฑ ุงูุดุจูุฉ,ูุฏูุฑ ุงูุดุจูุฉ,technical,0.03,,Text_snippets-main(1)_zip_contents.txt,8cc0693d1333dd7b
35,wget,ุชูุฒูู ุงููููุงุช,ุชูุฒูู ุงููููุงุช,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,99c668e96f3499ad
36,rclone,ูุฒุงููุฉ ุงูุณุญุงุจุฉ,ูุฒุงููุฉ ุงูุณุญุงุจุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,15ad107bbe914649
37,tcpdump,ุชุญููู ุงูุดุจูุฉ,ุชุญููู ุงูุดุจูุฉ,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,7872c2f9db238e7d
38,ufw,ุฌุฏุงุฑ ุญูุงูุฉ,ุฌุฏุงุฑ ุญูุงูุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,e2cf2fc591f73fd8
39,proxychains-ng,ุจุฑููุณู,ุจุฑููุณู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,227b3c25f93a0b8d
40,python-numpy,ุญุณุงุจ ุนููู,ุญุณุงุจ ุนููู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,0b5d8eb1e7bcd1a1
41,python-pydantic,ุงูุชุญูู ูู ุงูุจูุงูุงุช,ุงูุชุญูู ูู ุงูุจูุงูุงุช,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,4ddba992adf74ba4
42,cmake,ุจูุงุก ุงููุดุงุฑูุน,ุจูุงุก ุงููุดุงุฑูุน,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,03916e8835bd8fbe
43,meson,ูุธุงู ุจูุงุก,ูุธุงู ุจูุงุก,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,9fd105171d947a88
44,ninja,ุจูุงุก ุณุฑูุน,ุจูุงุก ุณุฑูุน,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,f5725dbbcdc44f67
45,doxygen,ุชูุซูู ุงูููุฏ,ุชูุซูู ุงูููุฏ,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,3e74b9db6bbdb42d
46,pandoc-cli,ุชุญููู ุงููุณุชูุฏุงุช,ุชุญููู ุงููุณุชูุฏุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,ed423e58dc8211a9
47,telegram-desktop,ุชูููุฌุฑุงู,ุชูููุฌุฑุงู,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,e5d3fc2686985125
48,thunderbird,ุจุฑูุฏ ุฅููุชุฑููู,ุจุฑูุฏ ุฅููุชุฑููู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,fda0901867a507da
49,vlc,ูุดุบู ููุฏูู,ูุดุบู ููุฏูู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,22dce213525b8d8d
50,simple-scan,ูุงุณุญ ุถูุฆู,ูุงุณุญ ุถูุฆู,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,b5d9337ef9929abb
51,xournalpp,ููุงุญุธุงุช ุฑูููุฉ,ููุงุญุธุงุช ุฑูููุฉ,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,c451b14711570670
52,virtualbox,ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ,ูุญุงูุงุฉ ุงูุชุฑุงุถูุฉ,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,db837f094483ae84
53,docker,ุญุงููุงุช,ุญุงููุงุช,technical,0.02,,Text_snippets-main(1)_zip_contents.txt,4262aaa9d92c74f7
54,trilium-next,ููุงุญุธุงุช,ููุงุญุธุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,e70e5c3513eaab51
55,yara,ุชุญููู ุงูุจุฑูุฌูุงุช,ุชุญููู ุงูุจุฑูุฌูุงุช,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,e62fc6b5c6c230f2
56,rhash,ุญุณุงุจ ุงููุงุด,ุญุณุงุจ ุงููุงุด,misc,0.0,,Text_snippets-main(1)_zip_contents.txt,ae3817be5223947a
57,LibreOffice,ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ,ุงูุฃูุถู ูููุตุงุฏุฑ ุงูููุชูุญุฉ,misc,0.0,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,1a15fa27514821de
58,Google Chrome,ูุชููุฑ,ูุชููุฑ,misc,0.0,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,24e04f4c1d59c487
59,PyCharm Community,ูุฌุงูู,ูุฌุงูู,misc,0.0,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,f05f0d8c2a163f97
60,Orthopedic Supplies,ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ,ุงููุณุชูุฒูุงุช ุงูุนุธููุฉ,medical,0.06,,chat-ูุนุงูุฌุฉ ูุชุตููู ุงููุญุชูู.txt,f7783966018643c1
61,instruction,ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:,ุงุดุฑุญ ุงููุต ุงูุชุงูู ุจุงุฎุชุตุงุฑ:,translation,0.03,,ุชูููู ูุดุฑูุน ุชุตููู.txt,907a42fa9a93556d
62,instruction,ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:,ุชุฑุฌู ุชุฑุฌูุฉ ุทุจูุฉ ุงุญุชุฑุงููุฉ:,medical,0.06,,ุชูููู ูุดุฑูุน ุชุตููู.txt,ad7739db9319b389
63,osteomyelitis,ุงูุชูุงุจ ุงูุนุธู ูุงูููู,ุงูุชูุงุจ ุงูุนุธู ูุงูููู,misc,0.0,,ุชูููู ูุดุฑูุน ุชุตููู.txt,3ef956d29f8ab9b2
64,Mode 1,ูุชุฑุฌู,ูุชุฑุฌู,translation,0.03,,ุชูููู ูุดุฑูุน ุชุตููู.txt,7e2d1e8b3dbf49da
65,Mode 2,ุงุณุชุดุงุฑุฉ ุนูููุฉ,ุงุณุชุดุงุฑุฉ ุนูููุฉ,misc,0.0,,ุชูููู ูุดุฑูุน ุชุตููู.txt,945cf089ede7e434
66,Mode 3,ุชุญุฑูุฑ ููุดูุฑุงุช,ุชุญุฑูุฑ ููุดูุฑุงุช,misc,0.0,,ุชูููู ูุดุฑูุน ุชุตููู.txt,75ce04055de77f8b
67,fracture,ูุณุฑ,ูุณุฑ,medical,0.04,,ุชูููู ูุดุฑูุน ุชุตููู.txt,d407a7a688b22df3
68,internal fixation,ุชุซุจูุช ุฏุงุฎูู,ุชุซุจูุช ุฏุงุฎูู,technical,0.02,,ุชูููู ูุดุฑูุน ุชุตููู.txt,c4f3013d544fec5b
69,instruction,ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:,ุงูุชุจ ุจุฃุณููุจ ุนููู ุนุฑุจู ูุงุถุญ ููุจุงุดุฑ:,translation,0.03,,ุชูููู ูุดุฑูุน ุชุตููู.txt,6306f5db30cd34ca

================================================================================

ุงุณู ุงูููู: Text_snippets-main/upload.sh
----------------------------------------
#!/bin/bash
# ุณูุฑูุจุช ุฑูุน ุงููุณุชูุฏุน ุฅูู GitHub

echo "=================================="
echo "ุฑูุน ูุงุนุฏุฉ ุงููุนุฑูุฉ ุฅูู GitHub"
echo "=================================="

# ูุนูููุงุช ุงููุณุชูุฏุน
REPO_URL="https://github.com/DrAbdulmalek/Text_snippets.git"
USERNAME="DrAbdulmalek"

echo ""
echo "ููุฑูุนุ ุชุญุชุงุฌ ุฅูู GitHub Token ุตุงูุญ."
echo "ุงุญุตู ุนูู ุชูููู ูู: https://github.com/settings/tokens"
echo ""
echo "ุซู ููุฐ ุงูุฃูุฑ ุงูุชุงูู:"
echo ""
echo "git push https://YOUR_TOKEN@github.com/DrAbdulmalek/Text_snippets.git main"
echo ""
echo "ุฃู ุฃุถู ุงูุชูููู ูู ููู .git-credentials:"
echo ""
echo "echo 'https://DrAbdulmalek:YOUR_TOKEN@github.com' > ~/.git-credentials"
echo "git config --global credential.helper store"
echo "git push -u origin main"
echo ""

# ูุญุงููุฉ ุงูุฑูุน ุฅุฐุง ูุงู ุงูุชูููู ูุชููุฑุงู
if [ -f ~/.git-credentials ]; then
    echo "ุชู ุงูุนุซูุฑ ุนูู ุจูุงูุงุช ุงูุงุนุชูุงุฏ..."
    git push -u origin main
else
    echo "โ๏ธ ูู ูุชู ุงูุนุซูุฑ ุนูู ุจูุงูุงุช ุงูุงุนุชูุงุฏ"
    echo "ูุฑุฌู ุฅุนุฏุงุฏ ุงูุชูููู ุฃููุงู"
fi

================================================================================

================================================================================
ููุฎุต ุงููุนุงูุฌุฉ:
- ุนุฏุฏ ุงููููุงุช ุงููุตูุฉ ุงููุนุงูุฌุฉ: 48
- ุนุฏุฏ ุงููููุงุช ุงููุชุฌุงููุฉ: 2
- ุฅุฌูุงูู ุงููููุงุช ูู ุงูุฃุฑุดูู: 61
================================================================================
